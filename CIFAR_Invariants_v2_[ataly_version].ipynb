{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR Invariants v2 [ataly version].ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "rW16gvv7ifik",
        "QhKV-FYLv-Eb"
      ]
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "sWtW7m9i8Q1e",
        "colab_type": "code",
        "outputId": "d69a0839-84ce-474b-bce7-0e2e5daa664d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/02/6717354e5f6eeec5face466cc5a2b94f745716bea270c39301d44134abc7/tensorflow_datasets-0.0.1-py2.py3-none-any.whl (44kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python2.7/dist-packages (from tensorflow-datasets) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from tensorflow-datasets) (1.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python2.7/dist-packages (from tensorflow-datasets) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python2.7/dist-packages (from tensorflow-datasets) (4.28.1)\n",
            "Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (from tensorflow-datasets) (1.1.6)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests->tensorflow-datasets) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests->tensorflow-datasets) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests->tensorflow-datasets) (2018.10.15)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests->tensorflow-datasets) (3.0.4)\n",
            "Installing collected packages: tensorflow-datasets\n",
            "Successfully installed tensorflow-datasets-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t-JL_MKop5Qh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import io\n",
        "import os\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn import tree\n",
        "from tqdm import tqdm\n",
        "import operator\n",
        "import pandas as pd\n",
        "#import keras\n",
        "#import h5py\n",
        "#import cPickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SmPpG06W_vnF",
        "colab_type": "code",
        "outputId": "77969ad0-c3b4-44c9-8699-e6f4abc0dc33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "# RUN ONCE\n",
        "# Run Runtime->Reset ALl Runtimes to reset\n",
        "!git clone https://github.com/tensorflow/models\n",
        "os.chdir('models/tutorials/image/cifar10/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 23133 (delta 21), reused 24 (delta 10), pack-reused 23094\u001b[K\n",
            "Receiving objects: 100% (23133/23133), 562.90 MiB | 29.31 MiB/s, done.\n",
            "Resolving deltas: 100% (13510/13510), done.\n",
            "Checking out files: 100% (2883/2883), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IgohW_DIIcEq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cifar10\n",
        "import cifar10_input\n",
        "# Fix for flags from Burak's Textual Invariants colab.\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uPtumJ1vcCfm",
        "colab_type": "code",
        "outputId": "5763f76a-87ba-4012-b01a-236676a69707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "cifar10.maybe_download_and_extract()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Downloading cifar-10-binary.tar.gz 100.0%\n",
            "Successfully downloaded cifar-10-binary.tar.gz 170052171 bytes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fj7gi7aaUQDi",
        "colab_type": "code",
        "outputId": "b0635023-f79e-47ab-9a72-689486403607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "cell_type": "code",
      "source": [
        "# Get test and train images.\n",
        "with tf.Graph().as_default():\n",
        "  train_images_bin, train_labels_bin = cifar10_input.inputs(False, \"/tmp/cifar10_data/cifar-10-batches-bin\", 50000)\n",
        "  test_images_bin, test_labels_bin = cifar10_input.inputs(True, \"/tmp/cifar10_data/cifar-10-batches-bin\", 10000)\n",
        "  with tf.train.MonitoredSession() as sess:\n",
        "    test_images, test_labels = sess.run([test_images_bin, test_labels_bin])\n",
        "    train_images, train_labels = sess.run([train_images_bin, train_labels_bin])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From cifar10_input.py:232: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:197: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From cifar10_input.py:79: __init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
            "WARNING:tensorflow:From cifar10_input.py:132: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A-jo6PRTQZAd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = cifar10.FLAGS.batch_size\n",
        "\n",
        "def create_model(t_images, shape=None):\n",
        "  # If t_images is None then it creates a placeholder for feeding\n",
        "  # images. The placeholder has the provided shape.\n",
        "  #\n",
        "  # Creates model with images supplied by the t_images tensor.\n",
        "  if t_images is not None:  \n",
        "    t_images_pl = tf.placeholder_with_default(t_images, shape=t_images.shape)\n",
        "  else:\n",
        "    t_images_pl = tf.placeholder(shape)\n",
        "  # Build a Graph that computes the logits predictions from the\n",
        "  # inference model.\n",
        "  t_logits = cifar10.inference(t_images_pl)\n",
        "  return t_images_pl, t_logits\n",
        "\n",
        "def get_prediction(sess, tensor, t_images_pl, images, batch_size=BATCH_SIZE):\n",
        "  \"\"\"\n",
        "  Evaluate given tensor by feeding the provided images.\n",
        "  TODO: Currently partial batches cannot be evaluated so len(images) must\n",
        "  be a multiple of batch_size.\n",
        "  \"\"\"\n",
        "  def get_prediction_batch(image_batch):\n",
        "    feed = {t_images_pl: np.array(image_batch)}\n",
        "    return sess.run(tensor, feed_dict=feed)\n",
        "  \n",
        "  def get_final_batch(images, batch_size):\n",
        "    pad_value = np.zeros(images[0].shape)\n",
        "    final_batch = images[int(len(images)/batch_size)*batch_size:]\n",
        "    pad_size = batch_size-len(final_batch)\n",
        "    pad = [pad_value for _ in range(pad_size)]\n",
        "    final_batch = np.concatenate(tuple([final_batch, pad]), axis=0)\n",
        "    return final_batch, pad_size\n",
        "  \n",
        "  n = len(images)\n",
        "  image_batches = [images[i*batch_size:(i+1)*batch_size] for i in range(int(n/batch_size))]\n",
        "  padding_size = 0\n",
        "  if (n % batch_size) != 0:\n",
        "    # Pad the remainting entries and create an additional batch\n",
        "    final_batch, pad_size = get_final_batch(images, batch_size)\n",
        "    image_batches += [final_batch]\n",
        "  batch_predictions = [get_prediction_batch(b) for b in tqdm(image_batches)]\n",
        "  # remove padding\n",
        "  batch_predictions = batch_predictions[:-1] + [batch_predictions[-1][:-pad_size]]\n",
        "  res = np.concatenate(tuple(batch_predictions), axis=0)\n",
        "  assert res.shape[0] == images.shape[0]\n",
        "  return res\n",
        "  \n",
        "def train_model(save_steps=500):\n",
        "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    t_global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "    # Get images and labels for CIFAR-10.\n",
        "    # Force input pipeline to CPU:0 to avoid operations sometimes ending up on\n",
        "    # GPU and resulting in a slow down.\n",
        "    with tf.device('/cpu:0'):\n",
        "      t_images, t_labels = cifar10.distorted_inputs()\n",
        "\n",
        "    # Build a Graph that computes the logits predictions from the\n",
        "    # inference model.\n",
        "    t_images_pl, t_logits = create_model(t_images)\n",
        "\n",
        "    # Calculate loss.\n",
        "    t_loss = cifar10.loss(t_logits, t_labels)\n",
        "\n",
        "    # Build a Graph that trains the model with one batch of examples and\n",
        "    # updates the model parameters.\n",
        "    t_train_op = cifar10.train(t_loss, t_global_step)\n",
        "    saver_hook = tf.train.CheckpointSaverHook(\n",
        "      checkpoint_dir='./checkpoints',\n",
        "      save_secs=None,\n",
        "      save_steps=save_steps,\n",
        "      saver=tf.train.Saver(),\n",
        "      checkpoint_basename='cifar_model.ckpt',\n",
        "      scaffold=None)\n",
        "    with tf.train.MonitoredTrainingSession(hooks=[saver_hook]) as sess:\n",
        "      while not sess.should_stop():\n",
        "        l, step, _ = sess.run([t_loss, t_global_step, t_train_op])\n",
        "        if step % save_steps == 0:\n",
        "          test_logits = get_prediction(sess, t_logits, t_images_pl, test_images)\n",
        "          test_accuracy = np.mean(np.equal(np.argmax(test_logits, axis=1), test_labels))\n",
        "          print \"Loss at step %d: %f\" % (step , l)\n",
        "          print \"Test accuracy at step %d: %f\" % (step, test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rW16gvv7ifik",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train a new CIFAR10 model"
      ]
    },
    {
      "metadata": {
        "id": "iqAtzqaPiiHj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nBZe6Hx9dHsJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ckpt_path_name = \"./checkpoints/cifar_model.ckpt-84000\"\n",
        "from google.colab import files\n",
        "files.download(ckpt_path_name + '.data-00000-of-00001')\n",
        "files.download(ckpt_path_name + '.index')\n",
        "files.download(ckpt_path_name + '.meta')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "48d22469-1248-4a1a-91b5-f21ec64ef775",
        "id": "UeexhNxP-kL3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "cell_type": "code",
      "source": [
        "!ls ./checkpoints"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\n",
            "cifar_model.ckpt-1.data-00000-of-00001\n",
            "cifar_model.ckpt-1.index\n",
            "cifar_model.ckpt-1.meta\n",
            "cifar_model.ckpt-400.data-00000-of-00001\n",
            "cifar_model.ckpt-400.index\n",
            "cifar_model.ckpt-400.meta\n",
            "cifar_model.ckpt-600.data-00000-of-00001\n",
            "cifar_model.ckpt-600.index\n",
            "cifar_model.ckpt-600.meta\n",
            "cifar_model.ckpt-65100.data-00000-of-00001\n",
            "cifar_model.ckpt-65100.index\n",
            "cifar_model.ckpt-65100.meta\n",
            "cifar_model.ckpt-65200.data-00000-of-00001\n",
            "cifar_model.ckpt-65200.index\n",
            "cifar_model.ckpt-65200.meta\n",
            "cifar_model.ckpt-65300.data-00000-of-00001\n",
            "cifar_model.ckpt-65300.index\n",
            "cifar_model.ckpt-65300.meta\n",
            "cifar_model.ckpt-65400.data-00000-of-00001\n",
            "cifar_model.ckpt-65400.index\n",
            "cifar_model.ckpt-65400.meta\n",
            "cifar_model.ckpt-700.data-00000-of-00001\n",
            "cifar_model.ckpt-700.index\n",
            "cifar_model.ckpt-700.meta\n",
            "cifar_model.ckpt-800.data-00000-of-00001\n",
            "cifar_model.ckpt-800.index\n",
            "cifar_model.ckpt-800.meta\n",
            "cifar_model.ckpt-83500.data-00000-of-00001\n",
            "cifar_model.ckpt-83500.index\n",
            "cifar_model.ckpt-83500.meta\n",
            "cifar_model.ckpt-84000.data-00000-of-00001\n",
            "cifar_model.ckpt-84000.index\n",
            "cifar_model.ckpt-84000.meta\n",
            "cifar_model.ckpt-84500.data-00000-of-00001\n",
            "cifar_model.ckpt-84500.index\n",
            "cifar_model.ckpt-84500.meta\n",
            "cifar_model.ckpt-85000.data-00000-of-00001\n",
            "cifar_model.ckpt-85000.index\n",
            "cifar_model.ckpt-85000.meta\n",
            "cifar_model.ckpt-85500.data-00000-of-00001\n",
            "cifar_model.ckpt-85500.index\n",
            "cifar_model.ckpt-85500.meta\n",
            "cifar_model.ckpt-900.data-00000-of-00001\n",
            "cifar_model.ckpt-900.index\n",
            "events.out.tfevents.1543620789.ed27981316da\n",
            "graph.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oZA0U3oG-kgu"
      },
      "cell_type": "markdown",
      "source": [
        "## Restore model from checkpoint"
      ]
    },
    {
      "metadata": {
        "id": "ORKRKdAB_bkr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "outputId": "dfbdd31c-02c9-4a69-c1b5-12af529ab99c"
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p ./restored_checkpoints\n",
        "!wget https://github.com/hayesconverse/sym_convnn/raw/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.index -O ./restored_checkpoints/cifar_model.ckpt.index\n",
        "!wget https://github.com/hayesconverse/sym_convnn/raw/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.meta -O ./restored_checkpoints/cifar_model.ckpt.meta\n",
        "!wget https://github.com/hayesconverse/sym_convnn/raw/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.data-00000-of-00001 -O ./restored_checkpoints/cifar_model.ckpt.data-00000-of-00001"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-01 22:55:25--  https://github.com/hayesconverse/sym_convnn/raw/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.index\n",
            "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/hayesconverse/sym_convnn/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.index [following]\n",
            "--2018-12-01 22:55:25--  https://raw.githubusercontent.com/hayesconverse/sym_convnn/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.index\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1167 (1.1K) [application/octet-stream]\n",
            "Saving to: ‘./restored_checkpoints/cifar_model.ckpt.index’\n",
            "\n",
            "\r          ./restore   0%[                    ]       0  --.-KB/s               \r./restored_checkpoi 100%[===================>]   1.14K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-12-01 22:55:26 (27.2 MB/s) - ‘./restored_checkpoints/cifar_model.ckpt.index’ saved [1167/1167]\n",
            "\n",
            "--2018-12-01 22:55:27--  https://github.com/hayesconverse/sym_convnn/raw/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.meta\n",
            "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/hayesconverse/sym_convnn/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.meta [following]\n",
            "--2018-12-01 22:55:27--  https://raw.githubusercontent.com/hayesconverse/sym_convnn/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.meta\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 236513 (231K) [application/octet-stream]\n",
            "Saving to: ‘./restored_checkpoints/cifar_model.ckpt.meta’\n",
            "\n",
            "./restored_checkpoi 100%[===================>] 230.97K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2018-12-01 22:55:27 (4.57 MB/s) - ‘./restored_checkpoints/cifar_model.ckpt.meta’ saved [236513/236513]\n",
            "\n",
            "--2018-12-01 22:55:29--  https://github.com/hayesconverse/sym_convnn/raw/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.data-00000-of-00001\n",
            "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/hayesconverse/sym_convnn/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.data-00000-of-00001 [following]\n",
            "--2018-12-01 22:55:30--  https://raw.githubusercontent.com/hayesconverse/sym_convnn/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.data-00000-of-00001\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8546408 (8.2M) [application/octet-stream]\n",
            "Saving to: ‘./restored_checkpoints/cifar_model.ckpt.data-00000-of-00001’\n",
            "\n",
            "./restored_checkpoi 100%[===================>]   8.15M  14.2MB/s    in 0.6s    \n",
            "\n",
            "2018-12-01 22:55:31 (14.2 MB/s) - ‘./restored_checkpoints/cifar_model.ckpt.data-00000-of-00001’ saved [8546408/8546408]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "110410c7-b924-48d1-aaa5-f2befcbd1f6e",
        "id": "cNPbulWH-kgo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# From now on we work with the default graph.\n",
        "t_images_pl, t_logits = create_model(test_images[:BATCH_SIZE])\n",
        "sess = tf.Session()\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(sess, './restored_checkpoints/cifar_model.ckpt')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./restored_checkpoints/cifar_model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "51df34de-1ebb-4e8f-8817-074f03ce0f1b",
        "id": "Xz4RAbzq-kgj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "train_predictions_all = get_prediction(sess, t_logits, t_images_pl, train_images)\n",
        "train_predictions = np.argmax(train_predictions_all, axis=1)\n",
        "test_predictions_all = get_prediction(sess, t_logits, t_images_pl, test_images)\n",
        "test_predictions = np.argmax(test_predictions_all, axis=1)\n",
        "\n",
        "# ACCURACY\n",
        "print \"\"\n",
        "print \"Accuracy\", np.mean(np.equal(test_predictions, test_labels))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 90.50it/s]\n",
            "100%|██████████| 79/79 [00:00<00:00, 91.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy 0.8314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "yR9bYFN9E9lg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Library for IG Attribution and Conductance"
      ]
    },
    {
      "metadata": {
        "id": "L2F_QleSkgvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a8b4126d-45ef-45fa-978f-c5c75112ae7d"
      },
      "cell_type": "code",
      "source": [
        "t_fc1 = sess.graph.get_tensor_by_name('local4/local4:0')\n",
        "t_conv1 = sess.graph.get_tensor_by_name('conv1/conv1:0')\n",
        "t_label = tf.placeholder(tf.int32)\n",
        "t_neuron_id = tf.placeholder(tf.int32)\n",
        "t_grad = tf.gradients(t_logits[:, t_label], t_images_pl)\n",
        "print t_fc1.shape\n",
        "print t_conv1.shape\n",
        "#t_conv2 = sess.graph.get_tensor_by_name('import/h_conv2:0')\n",
        "t_grad_neuron = tf.gradients(t_logits[:, t_label], t_fc1)[0]\n",
        "t_grad_conductance = tf.gradients(t_fc1[:,t_neuron_id], t_images_pl, grad_ys=t_grad_neuron[:, t_neuron_id])\n",
        "print t_fc1.graph\n",
        "print t_logits.graph\n",
        "print sess.graph"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 192)\n",
            "(128, 24, 24, 64)\n",
            "<tensorflow.python.framework.ops.Graph object at 0x7f6edeebeed0>\n",
            "<tensorflow.python.framework.ops.Graph object at 0x7f6edeebeed0>\n",
            "<tensorflow.python.framework.ops.Graph object at 0x7f6edeebeed0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TL92gsWskV0-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def attribute(inp, label, baseline=None, steps=50, use_top_label=False):\n",
        "  def top_label(inp):\n",
        "    return np.argmax(get_prediction(sess, t_logits, t_images_pl, [inp])[0])\n",
        "  if baseline is None:\n",
        "    baseline = 0*inp\n",
        "  scaled_inputs = [baseline + (float(i)/steps)*(inp-baseline) for i in range(0, steps)]\n",
        "  #feed_dict = {W_conv1: convWeightMatrix[0], b_conv1: convBiasMatrix[0], \n",
        "  #           W_conv2: convWeightMatrix[1], b_conv2: convBiasMatrix[1], \n",
        "  #           W_conv3: convWeightMatrix[2], b_conv3: convBiasMatrix[2], \n",
        "  #           W_conv4: convWeightMatrix[3], b_conv4: convBiasMatrix[3], \n",
        "  #           W_fc1: denseWeightMatrix[0], b_fc1: denseBiasMatrix[0], \n",
        "  #           W_fc2: denseWeightMatrix[1], b_fc2: denseBiasMatrix[1]}\n",
        "  if use_top_label:\n",
        "    feed_dict[x] = [inp]\n",
        "    logits = sess.run(t_logits, feed_dict=feed_dict)[0]\n",
        "    label = np.argmax(logits)\n",
        "  feed_dict[x] = scaled_inputs\n",
        "  feed_dict[t_label] = label\n",
        "  grads, scores = sess.run([t_grad, t_logits], feed_dict=feed_dict)  # shapes: <steps+1>, <steps+1, inp.shape>\n",
        "  integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "  print \"FINAL SCORE\", scores[-1][label]\n",
        "  print \"BASELINE SCORE\", scores[0][label]\n",
        "  print \"SUM\", np.sum(integrated_gradients), \"DIFF\", scores[-1][label] - scores[0][label]\n",
        "  return integrated_gradients\n",
        "\n",
        "def conductance(inp, label, neuron_id=None, baseline=None, steps=50):\n",
        "  # neuron_id is the id of the neuron in layer t_fc1 through which conductance\n",
        "  # must be computed. If None, vanilla IG is computed.\n",
        "  if baseline is None:\n",
        "    baseline = 0*inp\n",
        "  scaled_inputs = [baseline + (float(i)/steps)*(inp-baseline) for i in range(0, steps)]\n",
        "  #feed_dict = {W_conv1: convWeightMatrix[0], b_conv1: convBiasMatrix[0], \n",
        "  #           W_conv2: convWeightMatrix[1], b_conv2: convBiasMatrix[1], \n",
        "  #           W_conv3: convWeightMatrix[2], b_conv3: convBiasMatrix[2], \n",
        "  #           W_conv4: convWeightMatrix[3], b_conv4: convBiasMatrix[3], \n",
        "  #           W_fc1: denseWeightMatrix[0], b_fc1: denseBiasMatrix[0], \n",
        "  #           W_fc2: denseWeightMatrix[1], b_fc2: denseBiasMatrix[1]}\n",
        "  feed_dict[x] = scaled_inputs\n",
        "  feed_dict[t_label] = label\n",
        "  if neuron_id != None:\n",
        "    feed_dict[t_neuron_id] = neuron_id\n",
        "    grads, scores = sess.run([t_grad_conductance, t_logits], feed_dict=feed_dict)  # shapes: <steps+1>, <steps+1, inp.shape>\n",
        "    integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "    return integrated_gradients\n",
        "  grads, scores = sess.run([t_grad, t_logits], feed_dict=feed_dict)  # shapes: <steps+1>, <steps+1, inp.shape>    \n",
        "  integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "  print \"FINAL SCORE\", scores[-1][label]\n",
        "  print \"BASELINE SCORE\", scores[0][label]\n",
        "  print \"SUM\", np.sum(integrated_gradients), \"DIFF\", scores[-1][label] - scores[0][label]\n",
        "  return integrated_gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iG5Fq5GIkP6u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Library for Visualizing Images and Attributions"
      ]
    },
    {
      "metadata": {
        "id": "czxZISZYkLaH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "from IPython.display import clear_output, Image, display, HTML\n",
        "import numpy as np\n",
        "from cStringIO import StringIO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_o25suPkTL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "FONT_PATH='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf'\n",
        "IMAGE_SIZE=24\n",
        "\n",
        "def mnist_to_rgb(mnist_img):\n",
        "  \"\"\"\n",
        "  Transformsn an MNIST image (shape: <784>) to a grayscale\n",
        "  RGB image (shape: <28,28,3>)\n",
        "  \"\"\"\n",
        "  pixel_array = mnist_img.reshape(IMAGE_SIZE, IMAGE_SIZE)  # shape: 28,28\n",
        "  rgb_image = np.transpose([pixel_array,pixel_array,pixel_array], axes=[1,2,0])\n",
        "  return rgb_image\n",
        "\n",
        "def cifar_to_rgb(cifar_img):\n",
        "  rgb_image = cifar_img.reshape(IMAGE_SIZE, IMAGE_SIZE, 3) # shape: 32,32,3\n",
        "  return rgb_image\n",
        "\n",
        "def pil_img(a):\n",
        "  '''Returns a PIL image created from the provided RGB array.\n",
        "  '''\n",
        "  a = np.uint8(a)\n",
        "  return PIL.Image.fromarray(a)\n",
        "\n",
        "def mnist_to_pil_img(inp):\n",
        "  rgb_inp = 255*mnist_to_rgb(inp)\n",
        "  vis_inp = pil_img(rgb_inp)\n",
        "  return vis_inp  \n",
        "\n",
        "def cifar_to_pil_img(inp):\n",
        "  rgb_inp = 255*cifar_to_rgb(inp)\n",
        "  vis_inp = pil_img(rgb_inp) \n",
        "\n",
        "def pil_fig(fig):\n",
        "  # Returns a PIL image obtained from the provided PLT figure.\n",
        "  buf = io.BytesIO()\n",
        "  fig.savefig(buf, format='png')\n",
        "  plt.close(fig)\n",
        "  buf.seek(0)\n",
        "  img = PIL.Image.open(buf)\n",
        "  return img\n",
        "\n",
        "def show_img(img, fmt='jpeg'):\n",
        "  '''Displays the provided PIL image\n",
        "  '''\n",
        "  f = StringIO()\n",
        "  img.save(f, fmt)\n",
        "  display(Image(data=f.getvalue()))\n",
        " \n",
        "def show_mnist_img(mnist_img):\n",
        "  show_img(pil_img(255*mnist_to_rgb(mnist_img)))\n",
        "  \n",
        "def show_cifar_img(cifar_img):\n",
        "  show_img(pil_img(255*cifar_to_rgb(cifar_img)))\n",
        "  \n",
        "def gray_scale(img):\n",
        "  '''Converts the provided RGB image to gray scale.\n",
        "  '''\n",
        "  img = np.average(img, axis=2)\n",
        "  return np.transpose([img, img, img], axes=[1,2,0])\n",
        "\n",
        "def normalize(attrs, ptile=99):\n",
        "  '''Normalize the provided attributions so that they fall between\n",
        "     -1.0 and 1.0.\n",
        "  '''\n",
        "  h = np.percentile(attrs, ptile)\n",
        "  l = np.percentile(attrs, 100-ptile)\n",
        "  return np.clip(attrs/max(abs(h), abs(l)), -1.0, 1.0)    \n",
        "\n",
        "def pil_text(strs, shape, start_h=10, start_w=10, font_size=18, color=(0, 0, 0)):\n",
        "  # Returns a PIL image with the provided text.\n",
        "  img = pil_img(255*np.ones(shape))\n",
        "  draw = PIL.ImageDraw.Draw(img)\n",
        "  font = PIL.ImageFont.truetype(FONT_PATH, font_size)\n",
        "  h = start_h\n",
        "  for s in strs: \n",
        "    draw.text((start_w,h), s, fill=color, font=font)\n",
        "    h = h + 30\n",
        "  return img\n",
        "\n",
        "def combine(imgs, horizontal=True):\n",
        "  # Combines the provided PIL Images horizontally or veritically\n",
        "  if horizontal:\n",
        "    w = np.sum([img.size[0]+10 for img in imgs])\n",
        "    h = np.max([img.size[1] for img in imgs])\n",
        "  else:\n",
        "    w = np.max([img.size[0] for img in imgs])\n",
        "    h = np.sum([img.size[1]+10 for img in imgs])\n",
        "  final_img = PIL.Image.new('RGB', (w, h), color='white')\n",
        "  pos = 0\n",
        "  for img in imgs:\n",
        "    if horizontal:\n",
        "      final_img.paste(im=img, box=(pos,0))\n",
        "      pos = pos+img.size[0]+10\n",
        "    else:\n",
        "      final_img.paste(im=img, box=(0,pos))\n",
        "      pos = pos+img.size[1]+10\n",
        "  return final_img\n",
        "\n",
        "def visualize_attrs(img, attrs, ptile=99):\n",
        "  '''Visaualizes the provided attributions by first aggregating them\n",
        "    along the color channel to obtain per-pixel attributions and then\n",
        "    scaling the intensities of the pixels in the original image in\n",
        "    proportion to absolute value of these attributions.\n",
        "\n",
        "    The provided image and attributions must of shape (224, 224, 3).\n",
        "  '''\n",
        "  if np.sum(attrs) == 0.0:\n",
        "    # print \"Attributions are all ZERO\"\n",
        "    return pil_img(0*img)\n",
        "  attrs = gray_scale(attrs)\n",
        "  attrs = abs(attrs)\n",
        "  attrs = np.clip(attrs/np.percentile(attrs, ptile), 0,1)\n",
        "  vis = img*attrs\n",
        "  return pil_img(vis)\n",
        "  \n",
        "  \n",
        "R=np.array([255,0,0])\n",
        "G=np.array([0,255,0])\n",
        "B=np.array([0,0,255])\n",
        "def visualize_attrs2(img, attrs, pos_ch=G, neg_ch=R, ptile=99):\n",
        "  '''Visaualizes the provided attributions by first aggregating them\n",
        "     along the color channel and then overlaying the positive attributions\n",
        "     along pos_ch, and negative attributions along neg_ch.\n",
        "\n",
        "     The provided image and attributions must of shape (224, 224, 3).\n",
        "  '''\n",
        "  if np.sum(attrs) == 0.0:\n",
        "    # print \"Attributions are all ZERO\"\n",
        "    return pil_img(0*img)\n",
        "  attrs = gray_scale(attrs)\n",
        "  attrs = normalize(attrs, ptile)   \n",
        "  pos_attrs = attrs * (attrs >= 0.0)\n",
        "  neg_attrs = -1.0 * attrs * (attrs < 0.0)\n",
        "  attrs_mask = pos_attrs*pos_ch + neg_attrs*neg_ch\n",
        "  vis = 0.3*gray_scale(img) + 0.7*attrs_mask\n",
        "  return pil_img(vis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i_uhUyYiBlL7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extracting Invariant Candidates"
      ]
    },
    {
      "metadata": {
        "id": "MI8OPoEuA_RU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t_fc1 = sess.graph.get_tensor_by_name('local4/local4:0')\n",
        "t_conv1 = sess.graph.get_tensor_by_name('conv1/conv1:0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RToI9ehA1zfk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Test getting suffixes and checking accuracy\n",
        "t_fc1 = sess.graph.get_tensor_by_name('local4/local4:0')\n",
        "t_conv1 = sess.graph.get_tensor_by_name('conv1/conv1:0')\n",
        "\n",
        "T_LAYER = t_fc1\n",
        "def fingerprint_suffix(images):\n",
        "  print \"Getting fingerprint for\", T_LAYER.name\n",
        "  return (get_prediction(sess, T_LAYER, t_images_pl, images)>0.0).astype('int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uJmZBO7-BVhJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "70f93d1b-35d7-4cb0-ca6f-f7fcc412e513"
      },
      "cell_type": "code",
      "source": [
        "train_suffixes = fingerprint_suffix(train_images)\n",
        "test_suffixes = fingerprint_suffix(test_images)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 6/391 [00:00<00:06, 59.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for local4/local4:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 90.47it/s]\n",
            " 13%|█▎        | 10/79 [00:00<00:00, 93.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for local4/local4:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:00<00:00, 92.76it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xxgnKvn04Q92",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def describe_input(i, training=True):\n",
        "  if training:\n",
        "    print \"Input\", i\n",
        "    print \"Groundtruth\", train_labels[i]\n",
        "    print \"Prediction\", train_predictions[i]\n",
        "    print \"Fine-grained prediction\", 10*train_labels[i] + train_predictions[i]\n",
        "    show_cifar_img(train_images[i])\n",
        "  else:\n",
        "    print \"Input\", i\n",
        "    print \"Groundtruth\", test_labels[i]\n",
        "    print \"Prediction\", test_predictions[i]\n",
        "    print \"Fine-grained prediction\", 10*test_labels[i] + test_predictions[i]\n",
        "    show_cifar_img(test_images[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1fcVM2dek4Qo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build the Decision Tree"
      ]
    },
    {
      "metadata": {
        "id": "c_0fvX3aAWHN",
        "colab_type": "code",
        "outputId": "e7d1386d-99d6-4510-8a34-603b17e62753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Basic decision tree\n",
        "basic_estimator = tree.DecisionTreeClassifier()\n",
        "basic_estimator.fit(train_suffixes, train_predictions)\n",
        "\n",
        "# Evaluate basic_estimator on test data\n",
        "basic_estimator_acc = (basic_estimator.predict(test_suffixes) == test_labels)\n",
        "basic_estimator_agreement = (basic_estimator.predict(test_suffixes) == test_predictions)\n",
        "print \"Estimator accuracy\", 1.0*np.sum(basic_estimator_acc)/len(test_suffixes)\n",
        "print \"Estimator agreement\", 1.0*np.sum(basic_estimator_agreement)/len(test_suffixes)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimator accuracy 0.7838\n",
            "Estimator agreement 0.8621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rkvj66ZvX3B9",
        "colab_type": "code",
        "outputId": "5a2055f7-29bb-4760-ec36-e53dfeedefb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# Fine-grained predictions decision tree\n",
        "fine_grained_predictions = 10*train_labels + train_predictions\n",
        "fine_grained_estimator = tree.DecisionTreeClassifier()\n",
        "fine_grained_estimator.fit(train_suffixes, fine_grained_predictions)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "            max_features=None, max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
              "            splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "FfwDuDQtprdh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Decision tree per label\n",
        "def get_relative_predictions(label):\n",
        "  print \"Create relative predictions for label:%d\" % label\n",
        "  res = np.zeros(train_predictions.shape)\n",
        "  for i in range(len(train_predictions)):\n",
        "    pred = train_predictions[i]\n",
        "    gt = cifar_train_labels[i]\n",
        "    if gt == label and pred == gt:\n",
        "      res[i] = 0\n",
        "    elif gt == label and pred != gt:\n",
        "      res[i] = 1\n",
        "    else:\n",
        "      res[i] = 2\n",
        "  print \"Num correct: %d\" % np.sum(res == 0)\n",
        "  print \"Num misclassified: %d\" % np.sum(res == 1)\n",
        "  print \"Num others: %d\" % np.sum(res == 2)\n",
        "  return res\n",
        "\n",
        "def get_relative_estimator(label):\n",
        "  predictions = get_relative_predictions(label)\n",
        "  print \"Creating decision tree for label:%d\" % label\n",
        "  estimator = tree.DecisionTreeClassifier()\n",
        "  estimator.fit(train_suffixes, predictions)\n",
        "  return estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dHJlFvwXzXgF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SLOW; run only if you want to build relative estimators.\n",
        "relative_estimators = [None for _ in range(10)]\n",
        "for i in range(10):\n",
        "  relative_estimators[i] = get_relative_estimator(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yK1Pr1agnsoi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Examine clusters/invariants"
      ]
    },
    {
      "metadata": {
        "id": "ZbOQkQU0zBz6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_decision_path(estimator, inp):\n",
        "  # Extract the decision path taken by an input as an ordered list of indices\n",
        "  # of the neurons that were evaluated.\n",
        "  # See: http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
        "  n_nodes = estimator.tree_.node_count\n",
        "  feature = estimator.tree_.feature\n",
        "\n",
        "  # First let's retrieve the decision path of each sample. The decision_path\n",
        "  # method allows to retrieve the node indicator functions. A non zero element of\n",
        "  # indicator matrix at the position (i, j) indicates that the sample i goes\n",
        "  # through the node j.\n",
        "  X_test = [inp]\n",
        "  node_indicator = estimator.decision_path(X_test)\n",
        "  # Similarly, we can also have the leaves ids reached by each sample.\n",
        "  leaf_id = estimator.apply(X_test)\n",
        "  # Now, it's possible to get the tests that were used to predict a sample or\n",
        "  # a group of samples. First, let's make it for the sample.\n",
        "  node_index = node_indicator.indices[node_indicator.indptr[0]:\n",
        "                                      node_indicator.indptr[1]]\n",
        "  neuron_ids = []\n",
        "  for node_id in node_index:\n",
        "    if leaf_id[0] == node_id:\n",
        "        continue\n",
        "    neuron_ids.append(feature[node_id])\n",
        "  return neuron_ids\n",
        "\n",
        "def get_suffix_cluster(neuron_ids, neuron_sig):\n",
        "  # Get the cluster of inputs that such that all inputs in the cluster\n",
        "  # have provided on/off signature for the provided neurons.\n",
        "  #\n",
        "  # The returned cluster is an array of indices (into cifar_train_images).\n",
        "  return np.where((train_suffixes[:, neuron_ids] == neuron_sig).all(axis=1))[0]\n",
        "\n",
        "def is_consistent_cluster(cluster, predictions):\n",
        "  # Check if all inputs within the cluster have the same prediction.\n",
        "  # 'cluster' is an array of input ids.\n",
        "  pred = predictions[cluster[0]]\n",
        "  for i in cluster:\n",
        "    if predictions[i] != pred:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def is_misclassified(i):\n",
        "  return train_predictions[i] != train_labels[i]\n",
        "\n",
        "def visualize_conductances(img, label, neuron_ids, only_on=False):\n",
        "  # Visualize the conductances for the provided image.\n",
        "  # Args:\n",
        "  # - img: the provided cifar image\n",
        "  # - label: prediction label w.r.t. conductance must be computed\n",
        "  # - neuron_ids: list of neurons indices from the suffix tensor for which\n",
        "  #    conductances must be computed.\n",
        "  # - only_on: If True then conductance is computed only for those neurons\n",
        "  #    that are on for the given image. \n",
        "  vis = [cifar_to_pil_img(img)]\n",
        "  suffix = fingerprint_suffix([img])\n",
        "  for i, id in enumerate(neuron_ids):\n",
        "    if only_on and suffix[i] != 1:\n",
        "      continue  \n",
        "    igc = conductance(img, label, neuron_id=id)\n",
        "    # igc = conductances[id]\n",
        "    vis.append(visualize_attrs2(255*cifar_to_rgb(img), cifar_to_rgb(igc)))\n",
        "  return combine(vis)\n",
        "\n",
        "def get_invariant(estimator, ref_id):\n",
        "  # Returns an invariant found w.r.t. the provided reference input\n",
        "  # Args\n",
        "  #  - ref_id: Index (into cifar_train_images) of the reference input\n",
        "  # Returns:\n",
        "  #  - cluster: Indices of training inputs that satisfy the invariant\n",
        "  #  - neuron_id: A list of neurons such that all inputs that agree with\n",
        "  #    the reference input on the on/off status of these neurons have the\n",
        "  #    same prediction as the reference input.\n",
        "  ref_img = train_images[ref_id]\n",
        "  ref_suffix = train_suffixes[ref_id]\n",
        "  neuron_ids = get_decision_path(estimator, ref_suffix)\n",
        "  neuron_sig = ref_suffix[neuron_ids]\n",
        "  cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "  return cluster, neuron_ids, neuron_sig\n",
        "\n",
        "def get_all_invariants(estimator):\n",
        "  # Returns a dictionary mapping each decision tree prediction class\n",
        "  # to a list of invariants. Each invariant is specified as a triple:\n",
        "  # - neuron ids\n",
        "  # - neuron signature (for the neuron ids)\n",
        "  # - number of training samples that hit it\n",
        "  # The neuron ids and neuron signature can be supplied to get_suffix_cluster\n",
        "  # to obtain the cluster of training instances that hit the invariant.\n",
        "  def is_leaf(node):\n",
        "    return estimator.tree_.children_left[node] == estimator.tree_.children_right[node]\n",
        "\n",
        "  def left_child(node):\n",
        "    return estimator.tree_.children_left[node]\n",
        "\n",
        "  def right_child(node):\n",
        "    return estimator.tree_.children_right[node]\n",
        "  \n",
        "  def get_all_paths_rec(node):\n",
        "    # Returns a list of triples corresponding to paths\n",
        "    # in the decision tree. Each triple consists of\n",
        "    # - neurons encountered along the path\n",
        "    # - signature along the path\n",
        "    # - prediction class at the leaf\n",
        "    # - number of training samples that hit the path\n",
        "    # The prediction class and number of training samples\n",
        "    # are set to -1 when the leaf is \"impure\".\n",
        "    feature = estimator.tree_.feature\n",
        "    if is_leaf(node):\n",
        "      values = estimator.tree_.value[node][0]\n",
        "      if len(np.where(values != 0)[0]) == 1:\n",
        "        cl = estimator.classes_[np.where(values != 0)[0][0]]\n",
        "        nsamples = estimator.tree_.n_node_samples[node]\n",
        "      else:\n",
        "        # impure node\n",
        "        cl = -1\n",
        "        nsamples = -1\n",
        "      return [[[], [], cl, nsamples]]\n",
        "    # If it is not a leaf both left and right childs must exist\n",
        "    paths = [[[feature[node]] + p[0], [0] + p[1], p[2], p[3]] for p in get_all_paths_rec(left_child(node))]\n",
        "    paths += [[[feature[node]] + p[0], [1] + p[1], p[2], p[3]] for p in get_all_paths_rec(right_child(node))]\n",
        "    return paths\n",
        "  paths =  get_all_paths_rec(0)\n",
        "  print \"Obtained all paths\"\n",
        "  invariants = {}\n",
        "  for p in tqdm(paths):\n",
        "    neuron_ids, neuron_sig, cl, nsamples = p\n",
        "    if cl not in invariants:\n",
        "      invariants[cl] = []\n",
        "    # cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "    invariants[cl].append([neuron_ids, neuron_sig, nsamples])\n",
        "  for cl in invariants.keys():\n",
        "    invariants[cl] = sorted(invariants[cl], key=operator.itemgetter(2), reverse=True)\n",
        "  return invariants\n",
        "\n",
        "def describe_cluster(cluster, neuron_ids):\n",
        "  neuron_sig = train_suffixes[cluster[0]][neuron_ids]\n",
        "  print \"Num neurons in invariant\", len(neuron_ids)\n",
        "  print \"Neuron id and signature\", zip(neuron_ids, neuron_sig)\n",
        "  print \"Cluster size: \", len(cluster)\n",
        "  print \"Num misclassified\", len([i for i in cluster if is_misclassified(i)])\n",
        "\n",
        "def describe_all_invariants(all_invariants):\n",
        "  df = []\n",
        "  for cl, invs in all_invariants.iteritems():\n",
        "    # Note the number of invariants, and size of the largest invariant cluster\n",
        "    df.append([cl, sum([inv[2] for inv in invs]), len(invs), len([inv for inv in invs if inv[2]>=10]), invs[0][2]])\n",
        "  df = pd.DataFrame(df, columns=['Prediction Class', 'Num Instances', 'Num Invariants', 'Num Invariants with cluster size >= 10', 'Size of largest invariant cluster'])\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KOUniL-T5-Sa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Examine cluster/invariants containing a given reference input\n",
        "# ref_id is the index of the reference input\n",
        "ref_id =  2\n",
        "print \"### Reference Image ###\"\n",
        "describe_input(ref_id)\n",
        "print \"### Cluster ###\"\n",
        "cluster, neuron_ids, neuron_sig = get_invariant(basic_estimator, ref_id)\n",
        "describe_cluster(cluster, neuron_ids)\n",
        "\n",
        "# Visualize  10 inputs in the cluster\n",
        "for i in cluster[:25]:\n",
        "  describe_input(i)\n",
        "  # show_img(visualize_conductances(mnist.train.images[i], train_predictions[i], neuron_ids, only_on=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cDPikrr-zyHr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "b_all_invariants = get_all_invariants(basic_estimator)\n",
        "df = describe_all_invariants(b_all_invariants)\n",
        "print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "print df.to_string(index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Isr62nPPlROL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get all fine_grained_estimator invariants\n",
        "fge_all_invariants = get_all_invariants(fine_grained_estimator)\n",
        "# Print invariant stats\n",
        "df = describe_all_invariants(fge_all_invariants)\n",
        "print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "print df.to_string(index=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jU7SHWilyZjc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Analyzing clusters of misclassified inputs"
      ]
    },
    {
      "metadata": {
        "id": "6GPZ8nMyYCMj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Examine the cluster for a misclasification (Groundtruth: 4, Prediction: 49)\n",
        "invs = b_all_invariants[0]\n",
        "neuron_ids, neuron_sig, _ = invs[0]\n",
        "cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "describe_cluster(cluster, neuron_ids)\n",
        "\n",
        "# Visualize  10 inputs in the cluster\n",
        "for i in cluster[:10]:\n",
        "  describe_input(i)\n",
        "  # show_img(visualize_conductances(mnist.train.images[i], train_predictions[i], neuron_ids, only_on=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V-yR4qbD44Qr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Confidence modelling"
      ]
    },
    {
      "metadata": {
        "id": "h11oK-pcQMEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "151128ed-128b-400a-c509-b4cf9bdfbbe9"
      },
      "cell_type": "code",
      "source": [
        "def get_estimator_confidence(estimator, suffixes, estimator_predictions, estimator_conf_labels, min_samples):\n",
        "  estimator_leaf_nodes = estimator.apply(suffixes)\n",
        "  # check that the leaf has valid label. (For instance, for a fine grained estimator\n",
        "  # only labels 00, 11, .. are valid.)\n",
        "  estimator_conf = [p in estimator_conf_labels for p in estimator_predictions]\n",
        "  # check that the leaf has a minimum number of samples in its support\n",
        "  estimator_conf *= estimator.tree_.n_node_samples[estimator_leaf_nodes] >= min_samples\n",
        "  # check that the leaf is pure\n",
        "  estimator_pure = np.array([ len(np.where(v != 0)[0]) == 1 for v in estimator.tree_.value[estimator_leaf_nodes][:,0,:]])\n",
        "  estimator_conf *= estimator_pure\n",
        "  return estimator_conf\n",
        "\n",
        "def get_confident_accuracy(estimator, suffixes, orig_model_predictions, gt_labels, estimator_conf_labels, min_samples=10):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    estimator: the estimator used for confidence modeling\n",
        "    suffixes: the suffixes of the examples over which we evaluate. Typically this\n",
        "      would be 'test_suffixes'.\n",
        "    orig_model_predictions: predictions of the original model for these examples. \n",
        "      Typically this would be 'test_predictions'.\n",
        "    gt_labels: groundtruth labels for these examples. Typically this would be\n",
        "      mnist.test.labels.\n",
        "    estimator_conf_labels: prediction labels for the estimator which must be used\n",
        "      for confidence labels. If the estimator predicts a label outside this set then\n",
        "      the prediction on the example is not considered \"confident\". For the basic_estimator\n",
        "      all labels are fine for confidence modeling. For the fine_grained estimator\n",
        "      we would want to only use the \"pure\" labels for confident modeling. For\n",
        "      instance, 00, 11, ... , for the MNIST fine_grained_estimator.\n",
        "    min_samples: minimum number of samples for an estimator leaf for it to be\n",
        "      considered confident\n",
        "  \"\"\"\n",
        "  estimator_predictions = estimator.predict(suffixes)\n",
        "\n",
        "  # The following are all binary vectors of shape <len(suffixes)>  \n",
        "  orig_correct = (orig_model_predictions == gt_labels)\n",
        "  estimator_conf = get_estimator_confidence(estimator, suffixes, estimator_predictions, estimator_conf_labels, min_samples)\n",
        "\n",
        "  conf_frac = 1.0*np.sum(estimator_conf)/len(gt_labels)\n",
        "  overall_acc = 1.0*np.sum(orig_correct)/len(gt_labels)\n",
        "  conf_acc = 1.0*np.sum(orig_correct*estimator_conf)/np.sum(estimator_conf)\n",
        " \n",
        "  return conf_frac, overall_acc, conf_acc\n",
        "\n",
        "# BASIC_ESTIMATOR\n",
        "res = get_confident_accuracy(basic_estimator, test_suffixes, test_predictions, test_labels, np.array(range(10)), min_samples = 100)\n",
        "conf_frac, overall_acc,  conf_acc = res\n",
        "print \"BASIC ESTIMATOR\"\n",
        "print \"Confident fraction\", conf_frac\n",
        "print \"Overall accuracy\", overall_acc\n",
        "print \"Confident accuracy\", conf_acc\n",
        "\n",
        "# FINE_GRAINED_ESTIMATOR\n",
        "res = get_confident_accuracy(fine_grained_estimator, test_suffixes, test_predictions, test_labels, 10*np.array(range(10))+np.array(range(10)) , min_samples = 250)\n",
        "conf_frac, overall_acc,  conf_acc = res\n",
        "\n",
        "print \"\"\n",
        "print \"FINE GRAINED ESTIMATOR\"\n",
        "print \"Confident fraction\", conf_frac\n",
        "print \"Overall accuracy\", overall_acc\n",
        "print \"Confident accuracy\", conf_acc"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BASIC ESTIMATOR\n",
            "Confident fraction 0.6292\n",
            "Overall accuracy 0.8314\n",
            "Confident accuracy 0.9361093452\n",
            "\n",
            "FINE GRAINED ESTIMATOR\n",
            "Confident fraction 0.383\n",
            "Overall accuracy 0.8314\n",
            "Confident accuracy 0.978328981723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "opfvJI1k3XOC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Download and run adversarial examples"
      ]
    },
    {
      "metadata": {
        "id": "wCgnnqCt3dZ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "4d83d5a3-2d33-40ae-89c9-6b4e1dd0c00e"
      },
      "cell_type": "code",
      "source": [
        "!mkdir downloads\n",
        "!curl -sL https://github.com/mzweilin/EvadeML-Zoo/releases/download/v0.1/downloads.tar.gz | tar xzv -C downloads\n",
        "!mkdir results\n",
        "!curl -sL https://github.com/mzweilin/EvadeML-Zoo/releases/download/v0.1/results_MNIST_100_317f6_carlini.tar.gz | tar xzv -C results\n",
        "!curl -sL https://github.com/mzweilin/EvadeML-Zoo/releases/download/v0.1/results_CIFAR-10_100_de671_densenet.tar.gz | tar xzv -C results\n",
        "!curl -sL https://github.com/mzweilin/EvadeML-Zoo/releases/download/v0.1/results_ImageNet_100_a2749_mobilenet.tar.gz | tar xzv -C results"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MagNet/\n",
            "MagNet/defensive_models/\n",
            "MagNet/defensive_models/MNIST_II\n",
            "MagNet/defensive_models/MNIST_I\n",
            "MagNet/defensive_models/CIFAR\n",
            "trained_models/\n",
            "trained_models/MNIST_pgdtrained.keras_weights.h5\n",
            "trained_models/MNIST_pgdbase.keras_weights.h5\n",
            "trained_models/CIFAR-10_carlini.keras_weights.h5\n",
            "trained_models/MNIST_cleverhans.keras_weights.h5\n",
            "trained_models/MNIST_cleverhans_adv_trained.keras_weights.h5\n",
            "trained_models/MNIST_carlini.keras_weights.h5\n",
            "MNIST_100_317f6_carlini/\n",
            "MNIST_100_317f6_carlini/adv_examples/\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_jsma?targeted=next.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_carlinili?targeted=next&batch_size=1&max_iterations=1000&confidence=10.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_bim?eps=0.3&eps_iter=0.06.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_adaptive_carlini_l2?targeted=next&tf_squeezers=median_filter_2_2,binary_filter_0.5&distance_measure=l1&detector_threshold=0.002915.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_carlinil0?targeted=next&batch_size=1&max_iterations=1000&confidence=10.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_adaptive_carlini_l2?targeted=false&tf_squeezers=median_filter_2_2,binary_filter_0.5&distance_measure=l1&detector_threshold=0.002915.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_fgsm?eps=0.3.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=10.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_jsma?targeted=ll.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=10.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_carlinili?targeted=ll&batch_size=1&max_iterations=1000&confidence=10.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_adaptive_carlini_l2?targeted=ll&tf_squeezers=median_filter_2_2,binary_filter_0.5&distance_measure=l1&detector_threshold=0.002915.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_carlinil0?targeted=ll&batch_size=1&max_iterations=1000&confidence=10.pickle\n",
            "CIFAR-10_100_de671_densenet/\n",
            "CIFAR-10_100_de671_densenet/adv_examples/\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinili?targeted=next&confidence=5.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=5.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=5.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinili?targeted=ll&confidence=5.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_deepfool?overshoot=10.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil0?targeted=ll&confidence=5.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_jsma?targeted=ll.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_fgsm?eps=0.0156.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_bim?eps=0.008&eps_iter=0.0012.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_jsma?targeted=next.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil0?targeted=next&confidence=5.pickle\n",
            "ImageNet_100_a2749_mobilenet/\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_bim?eps=0.0040&eps_iter=0.0020.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_fgsm?eps=0.0078.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_carlinil2?max_iterations=1000&batch_size=10&targeted=next&confidence=5.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_carlinil2?max_iterations=1000&batch_size=50&targeted=ll&confidence=5.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_carlinil0?batch_size=1&targeted=next&confidence=5.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_carlinili?batch_size=1&targeted=ll&confidence=5.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_carlinili?batch_size=1&targeted=next&confidence=5.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_deepfool?overshoot=35.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_carlinil0?batch_size=1&targeted=ll&confidence=5.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sMmiuE6i3egJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import cPickle\n",
        "\n",
        "#Load adversarial examples\n",
        "#Questions: where are the labels? and how is this all normalized, if that matters?\n",
        "targetNext1Pickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinili?targeted=next&confidence=5.pickle\", 'rb'))\n",
        "targetNext1Data = targetNext1Pickle[0]\n",
        "targetll1Pickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinili?targeted=ll&confidence=5.pickle\", 'rb'))\n",
        "targetll1Data = targetll1Pickle[0]\n",
        "targetNext2Pickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=5.pickle\", 'rb'))\n",
        "targetNext2Data = targetNext2Pickle[0]\n",
        "targetll2Pickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=5.pickle\", 'rb'))\n",
        "targetll2Data = targetll2Pickle[0]\n",
        "targetNext0Pickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil0?targeted=next&confidence=5.pickle\", 'rb'))\n",
        "targetNext0Data = targetNext0Pickle[0]\n",
        "targetll0Pickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil0?targeted=ll&confidence=5.pickle\", 'rb'))\n",
        "targetll0Data = targetll0Pickle[0]\n",
        "targetNextJsmaPickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_jsma?targeted=next.pickle\", 'rb'))\n",
        "targetNextJsmaData = targetNextJsmaPickle[0]\n",
        "targetllJsmaPickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_jsma?targeted=ll.pickle\", 'rb'))\n",
        "targetllJsmaData = targetllJsmaPickle[0]\n",
        "fgsmPickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_fgsm?eps=0.0156.pickle\", 'rb'))\n",
        "fgsmData = fgsmPickle[0]\n",
        "bimPickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_bim?eps=0.008&eps_iter=0.0012.pickle\", 'rb'))\n",
        "bimData = bimPickle[0]\n",
        "deepfoolPickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_deepfool?overshoot=10.pickle\", 'rb'))\n",
        "deepfoolData = deepfoolPickle[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZCqyixQXXrmO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "521bea0d-9206-4a0a-c9f0-f08c209ce859"
      },
      "cell_type": "code",
      "source": [
        "#Select desired data here\n",
        "centeredData = tf.image.resize_image_with_crop_or_pad(bimData, 24, 24)\n",
        "adversarial_images = sess.run(centeredData)\n",
        "\n",
        "adversarial_predictions_all = get_prediction(sess, t_logits, t_images_pl, adversarial_images)\n",
        "adversarial_predictions = np.argmax(adversarial_predictions_all, axis=1)\n",
        "adversarial_suffixes = fingerprint_suffix(adversarial_images)\n",
        "adversarial_suffixes = (adversarial_suffixes>0).astype('int')\n",
        "\n",
        "fge_adversarial_predictions = fine_grained_estimator.predict(adversarial_suffixes)\n",
        "be_adversarial_predictions = basic_estimator.predict(adversarial_suffixes)\n",
        "\n",
        "#Adjust confidence threshold here\n",
        "be_adversarial_conf = get_estimator_confidence(basic_estimator, adversarial_suffixes, be_adversarial_predictions, np.array(range(10)), min_samples=10)\n",
        "print \"Number of confident adversarial examples from basic estimator\", np.sum(be_adversarial_conf)\n",
        "fge_adversarial_conf = get_estimator_confidence(fine_grained_estimator, adversarial_suffixes, fge_adversarial_predictions, 10*np.array(range(10))+np.array(range(10)), min_samples=10)\n",
        "print \"Number of confident adversarial examples from fine grained estimator\", np.sum(fge_adversarial_conf)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 49.06it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 32.51it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for local4/local4:0\n",
            "Number of confident adversarial examples from basic estimator 25\n",
            "Number of confident adversarial examples from fine grained estimator 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "QhKV-FYLv-Eb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing the Decision Tree"
      ]
    },
    {
      "metadata": {
        "id": "EEZ8ZeuMo_9m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install graphviz\n",
        "!pip install graphviz\n",
        "import graphviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZaOsPspkjuOe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dot_data = tree.export_graphviz(basic_estimator, out_file=None) \n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}