{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MNIST Invariants v2 - Textual.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "iG5Fq5GIkP6u"
      ]
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "t-JL_MKop5Qh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import csv\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "import operator\n",
        "import pandas as pd\n",
        "\n",
        "from io import open\n",
        "from collections import namedtuple\n",
        "from sklearn import tree\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from tensorflow.contrib import learn\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d98vYNy_TWi1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training a Sentiment network (3 conv layers (for bigrams, trigrams and quadgrams) + 1 fully connected)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ns8_eYDFReWq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Yoon Kim's Convolutional Neural Networks for Sentence Classification paper, implemented in Tensorflow by Denny Britz, mostly copied from his Github repo.\n",
        "# This uses some different settings than the paper, like learning embeddings instead of using word2vec (or using both), using another dataset, not SST (Stanford Sentiment Treebank), not having L2 regularization.\n",
        "# TODO - Add options for those settings as well, making it more faithful to the paper.\n",
        "\n",
        "#Fix for flags\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "#Dataset info\n",
        "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
        "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
        "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
        "\n",
        "# Model Hyperparameters\n",
        "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
        "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
        "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
        "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
        "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
        "\n",
        "# Training parameters\n",
        "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
        "tf.flags.DEFINE_integer(\"num_epochs\", 100, \"Number of training epochs (default: 200)\")\n",
        "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
        "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
        "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
        "# Misc Parameters\n",
        "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
        "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
        "\n",
        "FLAGS = tf.flags.FLAGS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5JmDNOSOPHfn",
        "colab_type": "code",
        "outputId": "a202400f-6174-4f16-b5cf-9f3f575429c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "cell_type": "code",
      "source": [
        "#DOWNLOADING DATASET\n",
        "\n",
        "!mkdir ./data/\n",
        "!mkdir ./data/rt-polaritydata/\n",
        "!wget https://github.com/hayesconverse/sym_convnn/raw/master/Textual_Invariants/data/rt-polaritydata/rt-polarity.pos -O ./data/rt-polaritydata/rt-polarity.pos\n",
        "!wget https://github.com/hayesconverse/sym_convnn/raw/master/Textual_Invariants/data/rt-polaritydata/rt-polarity.neg -O ./data/rt-polaritydata/rt-polarity.neg"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./data/’: File exists\n",
            "mkdir: cannot create directory ‘./data/rt-polaritydata/’: File exists\n",
            "--2018-10-25 19:50:42--  https://github.com/hayesconverse/sym_convnn/raw/master/Textual_Invariants/data/rt-polaritydata/rt-polarity.pos\n",
            "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/hayesconverse/sym_convnn/master/Textual_Invariants/data/rt-polaritydata/rt-polarity.pos [following]\n",
            "--2018-10-25 19:50:42--  https://raw.githubusercontent.com/hayesconverse/sym_convnn/master/Textual_Invariants/data/rt-polaritydata/rt-polarity.pos\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 626395 (612K) [text/plain]\n",
            "Saving to: ‘./data/rt-polaritydata/rt-polarity.pos’\n",
            "\n",
            "./data/rt-polarityd 100%[===================>] 611.71K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2018-10-25 19:50:42 (7.46 MB/s) - ‘./data/rt-polaritydata/rt-polarity.pos’ saved [626395/626395]\n",
            "\n",
            "--2018-10-25 19:50:44--  https://github.com/hayesconverse/sym_convnn/raw/master/Textual_Invariants/data/rt-polaritydata/rt-polarity.neg\n",
            "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/hayesconverse/sym_convnn/master/Textual_Invariants/data/rt-polaritydata/rt-polarity.neg [following]\n",
            "--2018-10-25 19:50:44--  https://raw.githubusercontent.com/hayesconverse/sym_convnn/master/Textual_Invariants/data/rt-polaritydata/rt-polarity.neg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 612506 (598K) [text/plain]\n",
            "Saving to: ‘./data/rt-polaritydata/rt-polarity.neg’\n",
            "\n",
            "./data/rt-polarityd 100%[===================>] 598.15K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2018-10-25 19:50:45 (10.9 MB/s) - ‘./data/rt-polaritydata/rt-polarity.neg’ saved [612506/612506]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y3Zs1CzKSqjM",
        "colab_type": "code",
        "outputId": "f2edfb34-ea96-416f-8fa0-0b021158de54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "cell_type": "code",
      "source": [
        "#READ AND PROCESS INPUTS\n",
        "\n",
        "def clean_str(string):\n",
        "  \"\"\"\n",
        "  Tokenization/string cleaning for all datasets except for SST.\n",
        "  Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "  \"\"\"\n",
        "  string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "  string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "  string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "  string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "  string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "  string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "  string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "  string = re.sub(r\",\", \" , \", string)\n",
        "  string = re.sub(r\"!\", \" ! \", string)\n",
        "  string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "  string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "  string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "  string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "  return string.strip().lower()\n",
        "\n",
        "\n",
        "def load_data_and_labels(positive_data_file, negative_data_file):\n",
        "  \"\"\"\n",
        "  Loads MR polarity data from files, splits the data into words and generates labels.\n",
        "  Returns split sentences and labels.\n",
        "  \"\"\"\n",
        "  # Load data from files\n",
        "  positive_examples = list(open(positive_data_file, \"r\", encoding='utf-8').readlines())\n",
        "  positive_examples = [s.strip() for s in positive_examples]\n",
        "  negative_examples = list(open(negative_data_file, \"r\", encoding='utf-8').readlines())\n",
        "  negative_examples = [s.strip() for s in negative_examples]\n",
        "  # Split by words\n",
        "  x_text = positive_examples + negative_examples\n",
        "  x_text = [clean_str(sent) for sent in x_text]\n",
        "  # Generate labels\n",
        "  positive_labels = [[0, 1] for _ in positive_examples]\n",
        "  negative_labels = [[1, 0] for _ in negative_examples]\n",
        "  y = np.concatenate([positive_labels, negative_labels], 0)\n",
        "  return [x_text, y]\n",
        "\n",
        "\n",
        "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
        "  \"\"\"\n",
        "  Generates a batch iterator for a dataset.\n",
        "  \"\"\"\n",
        "  data = np.array(data)\n",
        "  data_size = len(data)\n",
        "  num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
        "  for epoch in range(num_epochs):\n",
        "    # Shuffle the data at each epoch\n",
        "    if shuffle:\n",
        "      shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "      shuffled_data = data[shuffle_indices]\n",
        "    else:\n",
        "      shuffled_data = data\n",
        "    for batch_num in range(num_batches_per_epoch):\n",
        "      start_index = batch_num * batch_size\n",
        "      end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "      yield shuffled_data[start_index:end_index]\n",
        "\n",
        "def preprocess():\n",
        "  # Data Preparation\n",
        "  # ==================================================\n",
        "\n",
        "  # Load data\n",
        "  print(\"Loading data...\")\n",
        "  x_text, y = load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
        "  #print x_text[0]\n",
        "  # Build vocabulary\n",
        "  max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
        "  vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
        "  x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
        "\n",
        "  # Randomly shuffle data\n",
        "  np.random.seed(10)\n",
        "  shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
        "  x_shuffled = x[shuffle_indices]\n",
        "  y_shuffled = y[shuffle_indices]\n",
        "  x_text_shuffled = [x_text[i] for i in list(shuffle_indices)]\n",
        "  #print shuffle_indices\n",
        "\n",
        "  # Split train/test set\n",
        "  # TODO: This is very crude, should use cross-validation\n",
        "  dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
        "  x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
        "  y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
        "  x_text_train, x_text_dev = x_text_shuffled[:dev_sample_index], x_text_shuffled[dev_sample_index:]\n",
        "\n",
        "  del x, y, x_shuffled, y_shuffled\n",
        "\n",
        "  print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
        "  print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
        "  return x_train, y_train, vocab_processor, x_dev, y_dev, x_text_train, x_text_dev\n",
        "\n",
        "x_train, y_train, vocab_processor, x_test, y_test, x_text_train, x_text_test = preprocess()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Vocabulary Size: 18758\n",
            "Train/Dev split: 9596/1066\n",
            "[   65   827 11955  2990  6742   250   532  2152  3564  3434   532 11956\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "like these russo guys lookin' for their mamet instead found their sturges\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "39JpLmTitqdj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weight_variable(shape, name):\n",
        "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "  return tf.Variable(initial, name=name)\n",
        "\n",
        "def bias_variable(shape, name):\n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return tf.Variable(initial, name=name)\n",
        "\n",
        "def conv2d(x, W):\n",
        "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
        "                        strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "#TODO Names are from the original implementation, may need to update them\n",
        "def create_model(sequence_length, num_classes, vocab_size,\n",
        "  embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
        "  \n",
        "  # Placeholders for input, output and dropout\n",
        "  input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
        "  input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
        "  dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "\n",
        "  # Keeping track of l2 regularization loss (optional)\n",
        "  l2_loss = tf.constant(0.0)\n",
        "\n",
        "  # Embedding layer\n",
        "  # tf.name_scope(\"embedding\")\n",
        "  with tf.device('/cpu:0'):\n",
        "    W = tf.Variable(\n",
        "      tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
        "      name=\"W_embed\")\n",
        "    embedded_chars = tf.nn.embedding_lookup(W, input_x)\n",
        "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
        "\n",
        "  # Create a convolution + maxpool layer for each filter size\n",
        "  pooled_outputs = []\n",
        "  relu_layers = []\n",
        "  for i, filter_size in enumerate(filter_sizes):\n",
        "    #with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
        "    # Convolution Layer\n",
        "    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
        "    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W_{}\".format(filter_size))\n",
        "    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b_{}\".format(filter_size))\n",
        "    conv = tf.nn.conv2d(\n",
        "      embedded_chars_expanded,\n",
        "      W,\n",
        "      strides=[1, 1, 1, 1],\n",
        "      padding=\"VALID\",\n",
        "      name=\"conv_{}\".format(filter_size))\n",
        "    # Apply nonlinearity\n",
        "    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu_{}\".format(filter_size))\n",
        "    relu_layers.append(h)\n",
        "    # Maxpooling over the outputs\n",
        "    pooled = tf.nn.max_pool(\n",
        "      h,\n",
        "      ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
        "      strides=[1, 1, 1, 1],\n",
        "      padding='VALID',\n",
        "      name=\"pool\")\n",
        "    pooled_outputs.append(pooled)\n",
        "\n",
        "  # Combine all the pooled features\n",
        "  num_filters_total = num_filters * len(filter_sizes)\n",
        "  h_pool = tf.concat(pooled_outputs, 3)\n",
        "  h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
        "\n",
        "  # Add dropout\n",
        "  with tf.name_scope(\"dropout\"):\n",
        "    h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
        "\n",
        "  # Final (unnormalized) scores and predictions\n",
        "  #with tf.name_scope(\"output\"):\n",
        "  W = tf.get_variable(\n",
        "    \"W_fc\",\n",
        "    shape=[num_filters_total, num_classes],\n",
        "    initializer=tf.contrib.layers.xavier_initializer())\n",
        "  b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b_fc\")\n",
        "  l2_loss += tf.nn.l2_loss(W)\n",
        "  l2_loss += tf.nn.l2_loss(b)\n",
        "  scores = tf.nn.xw_plus_b(h_drop, W, b, name=\"scores\")\n",
        "  predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
        "\n",
        "  # Calculate mean cross-entropy loss\n",
        "  with tf.name_scope(\"loss\"):\n",
        "    losses = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
        "    loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "  # Accuracy\n",
        "  with tf.name_scope(\"accuracy\"):\n",
        "    correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
        "  \n",
        "  cross_entropy = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=input_y, logits=scores))\n",
        "  \n",
        "  return cross_entropy, accuracy, input_x, dropout_keep_prob, scores, input_y, relu_layers\n",
        "  \n",
        "  ###ANKUR'S CODE:\n",
        "  #x = tf.placeholder(tf.float32, shape=[None, 784], name='input')\n",
        "  #y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "  #W_conv1 = weight_variable([5, 5, 1, 32], 'w_conv1')\n",
        "  #b_conv1 = bias_variable([32], 'b_conv1')\n",
        "\n",
        "  #x_image = tf.reshape(x, [-1,28,28,1])\n",
        "  #h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
        "  #h_pool1 = max_pool_2x2(h_conv1)\n",
        "  #h_pool1 = tf.identity(h_pool1, name=\"conv1\")\n",
        "\n",
        "  #W_conv2 = weight_variable([5, 5, 32, 64], 'w_conv2')\n",
        "  #b_conv2 = bias_variable([64], 'b_conv2')\n",
        "\n",
        "  #h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "  #h_pool2 = max_pool_2x2(h_conv2)\n",
        "  #h_pool2 = tf.identity(h_pool2, name=\"conv2\")\n",
        "\n",
        "  #W_fc1 = weight_variable([7 * 7 * 64, 1024], 'w_fc1')\n",
        "  #b_fc1 = bias_variable([1024], 'b_fc1')\n",
        "\n",
        "  #h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
        "  #h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name='fc1')\n",
        "\n",
        "  #keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "  #h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "\n",
        "  #W_fc2 = weight_variable([1024, 10], 'w_fc2')\n",
        "  #b_fc2 = bias_variable([10], 'b_fc2')\n",
        "\n",
        "  #y_conv = tf.add(tf.matmul(h_fc1_drop, W_fc2), b_fc2, name='prediction')\n",
        "\n",
        "  #cross_entropy = tf.reduce_mean(\n",
        "  #    tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y_conv))\n",
        "\n",
        "  #correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
        "  #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jv3U6Xs_T2el",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train a new model"
      ]
    },
    {
      "metadata": {
        "id": "Z-2x6UTcuJUK",
        "colab_type": "code",
        "outputId": "d856a393-b39e-421c-c7e2-43db8d36bacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        "#TODO Need to make the dataset compliant to tf.data format so that I can use next_batch or things like that.\n",
        "\n",
        "#tf.reset_default_graph()\n",
        "#sess = tf.InteractiveSession()\n",
        "#cross_entropy, accuracy, x, keep_prob, y_conv, y_ = create_model(sequence_length=x_train.shape[1],\n",
        "#        num_classes=y_train.shape[1],\n",
        "#        vocab_size=len(vocab_processor.vocabulary_),\n",
        "#        embedding_size=FLAGS.embedding_dim,\n",
        "#        filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
        "#        num_filters=FLAGS.num_filters,\n",
        "#        l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
        "#train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "#saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
        "#sess.run(tf.global_variables_initializer())\n",
        "#for i in range(0, 1200):\n",
        "#  batch = train_dataset.batch(FLAGS.batch_size)\n",
        "#  train_step.run(feed_dict={x: batch[0], y_: np.eye(10)[batch[1]], keep_prob: 0.5})\n",
        "#  if i%100 == 0:\n",
        "#    test_accuracy = accuracy.eval(feed_dict={\n",
        "#        x:test_dataset.images, y_: np.eye(10)[test_dataset.labels], keep_prob: 1.0})\n",
        "#    print(\"step %d, test accuracy %g\"%(i, test_accuracy))    \n",
        "#ckpt_path_name = saver.save(sess, './checkpoints/mnist_invariant.ckpt', global_step=i)\n",
        "#print \"Checkpoint saved at: %s\" % ckpt_path_name\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "cross_entropy, accuracy, x, keep_prob, y_conv, y_, relu_layers = create_model(sequence_length=x_train.shape[1],\n",
        "        num_classes=y_train.shape[1],\n",
        "        vocab_size=len(vocab_processor.vocabulary_),\n",
        "        embedding_size=FLAGS.embedding_dim,\n",
        "        filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
        "        num_filters=FLAGS.num_filters,\n",
        "        l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
        "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
        "saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# Generate batches\n",
        "batches = batch_iter(\n",
        "  list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
        "for i,batch in enumerate(batches):\n",
        "  x_batch, y_batch = zip(*batch)\n",
        "  train_step.run(feed_dict={x: x_batch, y_: y_batch, keep_prob: FLAGS.dropout_keep_prob})\n",
        "  if i%1000 == 0 and i > 0:\n",
        "    test_accuracy = accuracy.eval(feed_dict={\n",
        "        x:x_test, y_: y_test, keep_prob: 1.0})\n",
        "    print(\"step %d, test accuracy %g\"%(i, test_accuracy))    \n",
        "ckpt_path_name = saver.save(sess, './checkpoints/text_invariant.ckpt', global_step=i)\n",
        "print \"Checkpoint saved at: %s\" % ckpt_path_name\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-b0fb6a639eab>:87: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "step 1000, test accuracy 0.702627\n",
            "step 2000, test accuracy 0.732645\n",
            "step 3000, test accuracy 0.723265\n",
            "step 4000, test accuracy 0.721388\n",
            "step 5000, test accuracy 0.727955\n",
            "step 6000, test accuracy 0.74015\n",
            "step 7000, test accuracy 0.725141\n",
            "Checkpoint saved at: ./checkpoints/text_invariant.ckpt-7499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ylVCQ9lfTfWF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(ckpt_path_name + '.index')\n",
        "files.download(ckpt_path_name + '.meta')\n",
        "files.download(ckpt_path_name + '.data-00000-of-00001')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uy6_gvmpT4Wv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Restore a pretrained model"
      ]
    },
    {
      "metadata": {
        "id": "BMibD_lwWdVG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p ./checkpoints\n",
        "!wget https://github.com/hayesconverse/sym_convnn/raw/master/Textual_Invariants/text_checkpoint/text_invariant.ckpt.index -O ./checkpoints/text_invariants.ckpt.index\n",
        "!wget https://github.com/hayesconverse/sym_convnn/raw/master/Textual_Invariants/text_checkpoint/text_invariant.ckpt.meta -O ./checkpoints/text_invariants.ckpt.meta\n",
        "!wget https://github.com/hayesconverse/sym_convnn/raw/master/Textual_Invariants/text_checkpoint/text_invariant.ckpt.data-00000-of-00001 -O ./checkpoints/text_invariants.ckpt.data-00000-of-00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7JmFzaPPT6ql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "cross_entropy, accuracy, x, keep_prob, y_conv, y_ = create_model(sequence_length=x_train.shape[1],\n",
        "        num_classes=y_train.shape[1],\n",
        "        vocab_size=len(vocab_processor.vocabulary_),\n",
        "        embedding_size=FLAGS.embedding_dim,\n",
        "        filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
        "        num_filters=FLAGS.num_filters,\n",
        "        l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
        "saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
        "saver.restore(sess, './checkpoints/text_invariants.ckpt')\n",
        "\n",
        "test_accuracy = accuracy.eval(feed_dict={\n",
        "    x:x_test, y_: y_test, keep_prob: 1.0})\n",
        "print(\"Test accuracy %g\"%(test_accuracy))    \n",
        "\n",
        "train_accuracy = accuracy.eval(feed_dict={\n",
        "    x:x_train, y_: y_train, keep_prob: 1.0})\n",
        "print(\"Train accuracy %g\"%(train_accuracy))  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Nc9oYmUcsBn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parameters of the network"
      ]
    },
    {
      "metadata": {
        "id": "Uf1lzDiXcu0U",
        "colab_type": "code",
        "outputId": "e99fd25c-a5a4-410c-87c0-caf8e37cb986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\")))\n",
        "\n",
        "#w_embed = sess.run(sess.graph.get_tensor_by_name('W_embed:0'))\n",
        "#print \"Embedding, weight shape\", w_embed.shape\n",
        "\n",
        "w_convs = [None] * len(filter_sizes)\n",
        "b_convs = [None] * len(filter_sizes)\n",
        "relu_convs = [None] * len(filter_sizes)\n",
        "\n",
        "for i, filter_size in enumerate(filter_sizes):\n",
        "  w_convs[i] = sess.run(sess.graph.get_tensor_by_name('W_{}:0'.format(filter_size)))\n",
        "  b_convs[i] = sess.run(sess.graph.get_tensor_by_name('W_{}:0'.format(filter_size)))\n",
        "  relu_convs[i] = sess.graph.get_tensor_by_name('relu_{}:0'.format(filter_size))\n",
        "  print \"Conv with filter size {}, weight and bias shape\".format(filter_size), w_convs[i].shape, b_convs[i].shape  \n",
        "  \n",
        "w_fc = sess.run(sess.graph.get_tensor_by_name('W_fc:0'))\n",
        "b_fc = sess.run(sess.graph.get_tensor_by_name('b_fc:0'))\n",
        "print \"FC, weight and bias shape\", w_fc.shape, b_fc.shape\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conv with filter size 3, weight and bias shape (3, 128, 1, 128) (3, 128, 1, 128)\n",
            "Conv with filter size 4, weight and bias shape (4, 128, 1, 128) (4, 128, 1, 128)\n",
            "Conv with filter size 5, weight and bias shape (5, 128, 1, 128) (5, 128, 1, 128)\n",
            "FC, weight and bias shape (384, 2) (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yR9bYFN9E9lg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Library for IG Attribution and Conductance"
      ]
    },
    {
      "metadata": {
        "id": "L2F_QleSkgvY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t_label = tf.placeholder(tf.int32)\n",
        "t_neuron_id = tf.placeholder(tf.int32)\n",
        "t_grad = tf.gradients(y_conv[:, t_label], x)\n",
        "t_fc1 = sess.graph.get_tensor_by_name('fc1:0')\n",
        "t_conv1 = sess.graph.get_tensor_by_name('conv1:0')\n",
        "t_conv1 = sess.graph.get_tensor_by_name('conv2:0')\n",
        "t_grad_neuron = tf.gradients(y_conv[:, t_label], t_fc1)[0]\n",
        "t_grad_conductance = tf.gradients(t_fc1[:,t_neuron_id], x, grad_ys=t_grad_neuron[:, t_neuron_id])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TL92gsWskV0-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def get_prediction(inps, tensor=y_conv, batch_size=100):\n",
        "  def get_prediction_batch(batch):\n",
        "    feed = {x: np.array(batch), keep_prob:1.0}\n",
        "    return sess.run(tensor, feed_dict=feed)\n",
        "  n = len(inps)\n",
        "  if n%batch_size == 0:\n",
        "    batches = [inps[i*batch_size:(i+1)*batch_size] for i in range(int(n/batch_size))]\n",
        "  else:\n",
        "    batches = [inps[i*batch_size:(i+1)*batch_size] for i in range(int(n/batch_size) +1)]\n",
        "  #print len(batches)\n",
        "  batch_predictions = [get_prediction_batch(b) for b in tqdm(batches)]\n",
        "  #print len(batch_predictions)\n",
        "  #print batch_predictions[0]\n",
        "  #print batch_predictions[1]\n",
        "  return np.concatenate(tuple(batch_predictions), axis=0)\n",
        "\n",
        "def attribute(inp, label, baseline=None, steps=50, use_top_label=False):\n",
        "  def top_label(inp):\n",
        "    return np.argmax(get_prediction([inp])[0])\n",
        "  if baseline is None:\n",
        "    baseline = 0*inp\n",
        "  scaled_inputs = [baseline + (float(i)/steps)*(inp-baseline) for i in range(0, steps)]\n",
        "  feed = {keep_prob:1.0}\n",
        "  if use_top_label:\n",
        "    feed[x] = [inp]\n",
        "    logits = sess.run(y_conv, feed_dict=feed)[0]\n",
        "    label = np.argmax(logits)\n",
        "  feed[x] = scaled_inputs\n",
        "  feed[t_label] = label\n",
        "  grads, scores = sess.run([t_grad, y_conv], feed_dict=feed)  # shapes: <steps+1>, <steps+1, inp.shape>\n",
        "  integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "  print \"FINAL SCORE\", scores[-1][label]\n",
        "  print \"BASELINE SCORE\", scores[0][label]\n",
        "  print \"SUM\", np.sum(integrated_gradients), \"DIFF\", scores[-1][label] - scores[0][label]\n",
        "  return integrated_gradients\n",
        "\n",
        "def conductance(inp, label, neuron_id=None, baseline=None, steps=50):\n",
        "  # neuron_id is the id of the neuron in layer t_fc1 through which conductance\n",
        "  # must be computed. If None, vanilla IG is computed.\n",
        "  if baseline is None:\n",
        "    baseline = 0*inp\n",
        "  scaled_inputs = [baseline + (float(i)/steps)*(inp-baseline) for i in range(0, steps)]\n",
        "  feed = {keep_prob:1.0}\n",
        "  feed[x] = scaled_inputs\n",
        "  feed[t_label] = label\n",
        "  if neuron_id != None:\n",
        "    feed[t_neuron_id] = neuron_id\n",
        "    grads, scores = sess.run([t_grad_conductance, y_conv], feed_dict=feed)  # shapes: <steps+1>, <steps+1, inp.shape>\n",
        "    integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "    return integrated_gradients\n",
        "  grads, scores = sess.run([t_grad, y_conv], feed_dict=feed)  # shapes: <steps+1>, <steps+1, inp.shape>    \n",
        "  integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "  print \"FINAL SCORE\", scores[-1][label]\n",
        "  print \"BASELINE SCORE\", scores[0][label]\n",
        "  print \"SUM\", np.sum(integrated_gradients), \"DIFF\", scores[-1][label] - scores[0][label]\n",
        "  return integrated_gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iG5Fq5GIkP6u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Library for Visualizing Images and Attributions"
      ]
    },
    {
      "metadata": {
        "id": "czxZISZYkLaH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "from IPython.display import clear_output, Image, display, HTML\n",
        "import numpy as np\n",
        "from cStringIO import StringIO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_o25suPkTL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "FONT_PATH='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf'\n",
        "IMAGE_SIZE = 28\n",
        "\n",
        "def mnist_to_rgb(mnist_img):\n",
        "  \"\"\"\n",
        "  Transformsn an MNIST image (shape: <784>) to a grayscale\n",
        "  RGB image (shape: <28,28,3>)\n",
        "  \"\"\"\n",
        "  pixel_array = mnist_img.reshape(IMAGE_SIZE, IMAGE_SIZE)  # shape: 28,28\n",
        "  rgb_image = np.transpose([pixel_array,pixel_array,pixel_array], axes=[1,2,0])\n",
        "  return rgb_image\n",
        "\n",
        "def pil_img(a):\n",
        "  '''Returns a PIL image created from the provided RGB array.\n",
        "  '''\n",
        "  a = np.uint8(a)\n",
        "  return PIL.Image.fromarray(a)\n",
        "\n",
        "def mnist_to_pil_img(inp):\n",
        "  rgb_inp = 255*mnist_to_rgb(inp)\n",
        "  vis_inp = pil_img(rgb_inp)\n",
        "  return vis_inp  \n",
        "\n",
        "def pil_fig(fig):\n",
        "  # Returns a PIL image obtained from the provided PLT figure.\n",
        "  buf = io.BytesIO()\n",
        "  fig.savefig(buf, format='png')\n",
        "  plt.close(fig)\n",
        "  buf.seek(0)\n",
        "  img = PIL.Image.open(buf)\n",
        "  return img\n",
        "\n",
        "def show_img(img, fmt='jpeg'):\n",
        "  '''Displays the provided PIL image\n",
        "  '''\n",
        "  f = StringIO()\n",
        "  img.save(f, fmt)\n",
        "  display(Image(data=f.getvalue()))\n",
        " \n",
        "def show_mnist_img(mnist_img):\n",
        "  show_img(pil_img(255*mnist_to_rgb(mnist_img)))\n",
        "  \n",
        "def gray_scale(img):\n",
        "  '''Converts the provided RGB image to gray scale.\n",
        "  '''\n",
        "  img = np.average(img, axis=2)\n",
        "  return np.transpose([img, img, img], axes=[1,2,0])\n",
        "\n",
        "def normalize(attrs, ptile=99):\n",
        "  '''Normalize the provided attributions so that they fall between\n",
        "     -1.0 and 1.0.\n",
        "  '''\n",
        "  h = np.percentile(attrs, ptile)\n",
        "  l = np.percentile(attrs, 100-ptile)\n",
        "  return np.clip(attrs/max(abs(h), abs(l)), -1.0, 1.0)    \n",
        "\n",
        "def pil_text(strs, shape, start_h=10, start_w=10, font_size=18, color=(0, 0, 0)):\n",
        "  # Returns a PIL image with the provided text.\n",
        "  img = pil_img(255*np.ones(shape))\n",
        "  draw = PIL.ImageDraw.Draw(img)\n",
        "  font = PIL.ImageFont.truetype(FONT_PATH, font_size)\n",
        "  h = start_h\n",
        "  for s in strs: \n",
        "    draw.text((start_w,h), s, fill=color, font=font)\n",
        "    h = h + 30\n",
        "  return img\n",
        "\n",
        "def combine(imgs, horizontal=True):\n",
        "  # Combines the provided PIL Images horizontally or veritically\n",
        "  if horizontal:\n",
        "    w = np.sum([img.size[0]+10 for img in imgs])\n",
        "    h = np.max([img.size[1] for img in imgs])\n",
        "  else:\n",
        "    w = np.max([img.size[0] for img in imgs])\n",
        "    h = np.sum([img.size[1]+10 for img in imgs])\n",
        "  final_img = PIL.Image.new('RGB', (w, h), color='white')\n",
        "  pos = 0\n",
        "  for img in imgs:\n",
        "    if horizontal:\n",
        "      final_img.paste(im=img, box=(pos,0))\n",
        "      pos = pos+img.size[0]+10\n",
        "    else:\n",
        "      final_img.paste(im=img, box=(0,pos))\n",
        "      pos = pos+img.size[1]+10\n",
        "  return final_img\n",
        "\n",
        "def visualize_attrs(img, attrs, ptile=99):\n",
        "  '''Visaualizes the provided attributions by first aggregating them\n",
        "    along the color channel to obtain per-pixel attributions and then\n",
        "    scaling the intensities of the pixels in the original image in\n",
        "    proportion to absolute value of these attributions.\n",
        "\n",
        "    The provided image and attributions must of shape (224, 224, 3).\n",
        "  '''\n",
        "  if np.sum(attrs) == 0.0:\n",
        "    # print \"Attributions are all ZERO\"\n",
        "    return pil_img(0*img)\n",
        "  attrs = gray_scale(attrs)\n",
        "  attrs = abs(attrs)\n",
        "  attrs = np.clip(attrs/np.percentile(attrs, ptile), 0,1)\n",
        "  vis = img*attrs\n",
        "  return pil_img(vis)\n",
        "  \n",
        "  \n",
        "R=np.array([255,0,0])\n",
        "G=np.array([0,255,0])\n",
        "B=np.array([0,0,255])\n",
        "def visualize_attrs2(img, attrs, pos_ch=G, neg_ch=R, ptile=99):\n",
        "  '''Visaualizes the provided attributions by first aggregating them\n",
        "     along the color channel and then overlaying the positive attributions\n",
        "     along pos_ch, and negative attributions along neg_ch.\n",
        "\n",
        "     The provided image and attributions must of shape (224, 224, 3).\n",
        "  '''\n",
        "  if np.sum(attrs) == 0.0:\n",
        "    # print \"Attributions are all ZERO\"\n",
        "    return pil_img(0*img)\n",
        "  attrs = gray_scale(attrs)\n",
        "  attrs = normalize(attrs, ptile)   \n",
        "  pos_attrs = attrs * (attrs >= 0.0)\n",
        "  neg_attrs = -1.0 * attrs * (attrs < 0.0)\n",
        "  attrs_mask = pos_attrs*pos_ch + neg_attrs*neg_ch\n",
        "  vis = 0.3*gray_scale(img) + 0.7*attrs_mask\n",
        "  return pil_img(vis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i_uhUyYiBlL7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extracting Invariant Candidates"
      ]
    },
    {
      "metadata": {
        "id": "RPvUSoGyFkdt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fingerprint_suffix(inps):\n",
        "  #relu_convs[0] is the output of first convolutional filter after relu, we can also use second or third filter for suffixes.\n",
        "  return (get_prediction(inps, tensor=relu_convs[0], batch_size=1) > 0.0).astype('int')\n",
        "\n",
        "def fingerprint_prefix(inps):\n",
        "  return (get_prediction(inps, tensor=tf.reshape(w_convs[0], [-1, 128*128*3]), batch_size=1)>0.0).astype('int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3QarR8VNHZNk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_suffixes, train_predictions are in the same order\n",
        "# as mnist.train.images. Henceforth when we use the index i we will\n",
        "# be referring to mnist.train.images[i].\n",
        "#print len(x_train)\n",
        "#print len(y_train)\n",
        "#print len(x_test)\n",
        "#print len(y_test)\n",
        "\n",
        "\n",
        "train_suffixes = fingerprint_suffix(x_train)\n",
        "print \"Suffixes computed for all training data\"\n",
        "train_predictions = np.argmax(get_prediction(x_train, tensor=y_conv, batch_size=1), axis=1)\n",
        "print \"Predictions computed for all training data\"\n",
        "train_suffixes = train_suffixes.reshape(len(x_train),54*128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "slN931B53r_t",
        "colab_type": "code",
        "outputId": "bb85b379-8214-4644-ae62-309cf19643fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "# test_suffixes, test_predictions are in the same order\n",
        "# as mnist.train.images. Henceforth when we use the index i we will\n",
        "# be referring to mnist.train.images[i].\n",
        "test_suffixes = fingerprint_suffix(x_test)\n",
        "print \"Suffixes computed for all test data\"\n",
        "test_predictions = np.argmax(get_prediction(x_test,tensor=y_conv, batch_size=10), axis=1)\n",
        "print \"Predictions computed for all test data\"\n",
        "test_suffixes = test_suffixes.reshape(len(x_test),54*128)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1066/1066 [00:01<00:00, 866.64it/s]\n",
            " 36%|███▋      | 39/107 [00:00<00:00, 385.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Suffixes computed for all test data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 107/107 [00:00<00:00, 337.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predictions computed for all test data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xxgnKvn04Q92",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def describe_input(i, training=True):\n",
        "  #print \"Input:\", x_train[i]\n",
        "  print \"Input in words:\", x_text_train[i]\n",
        "  print \"Groundtruth:\", np.argmax(y_train,axis=1)[i]\n",
        "  print \"Prediction:\", train_predictions[i]\n",
        "  print \"Fine-grained prediction\", 10*np.argmax(y_train, axis=1)[i] + train_predictions[i]\n",
        "  #show_mnist_img(mnist.train.images[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1fcVM2dek4Qo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build the Decision Tree"
      ]
    },
    {
      "metadata": {
        "id": "c_0fvX3aAWHN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Basic decision tree\n",
        "print len(x_train)\n",
        "print len(train_suffixes)#???\n",
        "print len(train_predictions)\n",
        "print len(train_suffixes[0])\n",
        "\n",
        "#print len(x_test)\n",
        "#print len(test_suffixes)#???\n",
        "#print len(test_predictions)\n",
        "\n",
        "#basic_estimator = tree.DecisionTreeClassifier()\n",
        "#basic_estimator = basic_estimator.fit(train_suffixes, train_predictions)\n",
        "#get_all_invariants(basic_estimator)\n",
        "\n",
        "basic_estimator = tree.DecisionTreeClassifier()\n",
        "basic_estimator = basic_estimator.fit(train_suffixes, train_predictions)\n",
        "get_all_invariants(basic_estimator)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rkvj66ZvX3B9",
        "colab_type": "code",
        "outputId": "61294b24-e263-40ce-f532-634192bacdad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "# Fine-grained predictions decision tree\n",
        "fine_grained_predictions = 10*np.argmax(y_train, axis=1) + train_predictions\n",
        "print 'Misclassified in training data' if 10 in fine_grained_predictions or 1 in fine_grained_predictions else 'No misclassified in training data'\n",
        "fine_grained_estimator = tree.DecisionTreeClassifier()\n",
        "fine_grained_estimator = fine_grained_estimator.fit(train_suffixes, fine_grained_predictions)\n",
        "get_all_invariants(fine_grained_estimator)\n",
        "print fine_grained_predictions"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No misclassified in training data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 893/893 [00:00<00:00, 216591.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained all paths\n",
            "[ 0  0  0 ... 11 11  0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "FfwDuDQtprdh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Decision tree per label\n",
        "def get_relative_predictions(label):\n",
        "  print \"Create relative predictions for label:%d\" % label\n",
        "  res = np.zeros(train_predictions.shape)\n",
        "  for i in range(len(train_predictions)):\n",
        "    pred = train_predictions[i]\n",
        "    gt = y_train[i]\n",
        "    if gt == label and pred == gt:\n",
        "      res[i] = 0\n",
        "    elif gt == label and pred != gt:\n",
        "      res[i] = 1\n",
        "    else:\n",
        "      res[i] = 2\n",
        "  print \"Num correct: %d\" % np.sum(res == 0)\n",
        "  print \"Num misclassified: %d\" % np.sum(res == 1)\n",
        "  print \"Num others: %d\" % np.sum(res == 2)\n",
        "  return res\n",
        "\n",
        "def get_relative_estimator(label):\n",
        "  predictions = get_relative_predictions(label)\n",
        "  print \"Creating decision tree for label:%d\" % label\n",
        "  estimator = tree.DecisionTreeClassifier()\n",
        "  estimator.fit(train_suffixes, predictions)\n",
        "  return estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dHJlFvwXzXgF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SLOW; run only if you want to build relative estimators.\n",
        "relative_estimators = [None for _ in range(10)]\n",
        "for i in range(10):\n",
        "  relative_estimators[i] = get_relative_estimator(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yK1Pr1agnsoi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Examine clusters/invariants"
      ]
    },
    {
      "metadata": {
        "id": "ZbOQkQU0zBz6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_decision_path(estimator, inp):\n",
        "  # Extract the decision path taken by an input as an ordered list of indices\n",
        "  # of the neurons that were evaluated.\n",
        "  # See: http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
        "  n_nodes = estimator.tree_.node_count\n",
        "  feature = estimator.tree_.feature\n",
        "\n",
        "  # First let's retrieve the decision path of each sample. The decision_path\n",
        "  # method allows to retrieve the node indicator functions. A non zero element of\n",
        "  # indicator matrix at the position (i, j) indicates that the sample i goes\n",
        "  # through the node j.\n",
        "  X_test = [inp]\n",
        "  node_indicator = estimator.decision_path(X_test)\n",
        "  # Similarly, we can also have the leaves ids reached by each sample.\n",
        "  leaf_id = estimator.apply(X_test)\n",
        "  # Now, it's possible to get the tests that were used to predict a sample or\n",
        "  # a group of samples. First, let's make it for the sample.\n",
        "  node_index = node_indicator.indices[node_indicator.indptr[0]:\n",
        "                                      node_indicator.indptr[1]]\n",
        "  neuron_ids = []\n",
        "  for node_id in node_index:\n",
        "    if leaf_id[0] == node_id:\n",
        "        continue\n",
        "    neuron_ids.append(feature[node_id])\n",
        "  return neuron_ids\n",
        "\n",
        "def get_suffix_cluster(neuron_ids, neuron_sig):\n",
        "  # Get the cluster of inputs that such that all inputs in the cluster\n",
        "  # have provided on/off signature for the provided neurons.\n",
        "  #\n",
        "  # The returned cluster is an array of indices (into mnist.train.images).\n",
        "  return np.where((train_suffixes[:, neuron_ids] == neuron_sig).all(axis=1))[0]\n",
        "\n",
        "def is_consistent_cluster(cluster, predictions):\n",
        "  # Check if all inputs within the cluster have the same prediction.\n",
        "  # 'cluster' is an array of input ids.\n",
        "  pred = predictions[cluster[0]]\n",
        "  for i in cluster:\n",
        "    if predictions[i] != pred:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def is_misclassified(i):\n",
        "  return train_predictions[i] != np.argmax(y_train,axis=1)[i]\n",
        "\n",
        "def visualize_conductances(img, label, neuron_ids, only_on=False):\n",
        "  # Visualize the conductances for the provided image.\n",
        "  # Args:\n",
        "  # - img: the provided mnist image\n",
        "  # - label: prediction label w.r.t. conductance must be computed\n",
        "  # - neuron_ids: list of neurons indices from the suffix tensor for which\n",
        "  #    conductances must be computed.\n",
        "  # - only_on: If True then conductance is computed only for those neurons\n",
        "  #    that are on for the given image. \n",
        "  vis = [mnist_to_pil_img(img)]\n",
        "  suffix = fingerprint_suffix([img])\n",
        "  for i, id in enumerate(neuron_ids):\n",
        "    if only_on and suffix[i] != 1:\n",
        "      continue  \n",
        "    igc = conductance(img, label, neuron_id=id)\n",
        "    # igc = conductances[id]\n",
        "    vis.append(visualize_attrs2(255*mnist_to_rgb(img), mnist_to_rgb(igc)))\n",
        "  return combine(vis)\n",
        "\n",
        "def get_invariant(estimator, ref_id):\n",
        "  # Returns an invariant found w.r.t. the provided reference input\n",
        "  # Args\n",
        "  #  - ref_id: Index (into mnist.train.images) of the reference input\n",
        "  # Returns:\n",
        "  #  - cluster: Indices of training inputs that satisfy the invariant\n",
        "  #  - neuron_id: A list of neurons such that all inputs that agree with\n",
        "  #    the reference input on the on/off status of these neurons have the\n",
        "  #    same prediction as the reference input.\n",
        "  ref_img = x_train[ref_id]\n",
        "  ref_suffix = train_suffixes[ref_id]\n",
        "  neuron_ids = get_decision_path(estimator, ref_suffix)\n",
        "  neuron_sig = ref_suffix[neuron_ids]\n",
        "  cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "  return cluster, neuron_ids, neuron_sig\n",
        "\n",
        "def get_all_invariants(estimator):\n",
        "  # Returns a dictionary mapping each decision tree prediction class\n",
        "  # to a list of invariants. Each invariant is specified as a triple:\n",
        "  # - neuron ids\n",
        "  # - neuron signature (for the neuron ids)\n",
        "  # - number of training samples that hit it\n",
        "  # The neuron ids and neuron signature can be supplied to get_suffix_cluster\n",
        "  # to obtain the cluster of training instances that hit the invariant.\n",
        "  def is_leaf(node):\n",
        "    return estimator.tree_.children_left[node] == estimator.tree_.children_right[node]\n",
        "\n",
        "  def left_child(node):\n",
        "    return estimator.tree_.children_left[node]\n",
        "\n",
        "  def right_child(node):\n",
        "    return estimator.tree_.children_right[node]\n",
        "  \n",
        "  def get_all_paths_rec(node):\n",
        "    # Returns a list of triples corresponding to paths\n",
        "    # in the decision tree. Each triple consists of\n",
        "    # - neurons encountered along the path\n",
        "    # - signature along the path\n",
        "    # - prediction class at the leaf\n",
        "    # - number of training samples that hit the path\n",
        "    # The prediction class and number of training samples\n",
        "    # are set to -1 when the leaf is \"impure\".\n",
        "    feature = estimator.tree_.feature\n",
        "    if is_leaf(node):\n",
        "      values = estimator.tree_.value[node][0]\n",
        "      if len(np.where(values != 0)[0]) == 1:\n",
        "        cl = estimator.classes_[np.where(values != 0)[0][0]]\n",
        "        nsamples = estimator.tree_.n_node_samples[node]\n",
        "      else:\n",
        "        # impure node\n",
        "        cl = -1\n",
        "        nsamples = -1\n",
        "      return [[[], [], cl, nsamples]]\n",
        "    # If it is not a leaf both left and right childs must exist\n",
        "    paths = [[[feature[node]] + p[0], [0] + p[1], p[2], p[3]] for p in get_all_paths_rec(left_child(node))]\n",
        "    paths += [[[feature[node]] + p[0], [1] + p[1], p[2], p[3]] for p in get_all_paths_rec(right_child(node))]\n",
        "    return paths\n",
        "  paths =  get_all_paths_rec(0)\n",
        "  print \"Obtained all paths\"\n",
        "  invariants = {}\n",
        "  for p in tqdm(paths):\n",
        "    neuron_ids, neuron_sig, cl, nsamples = p\n",
        "    if cl not in invariants:\n",
        "      invariants[cl] = []\n",
        "    # cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "    invariants[cl].append([neuron_ids, neuron_sig, nsamples])\n",
        "  for cl in invariants.keys():\n",
        "    invariants[cl] = sorted(invariants[cl], key=operator.itemgetter(2), reverse=True)\n",
        "  return invariants\n",
        "\n",
        "def describe_cluster(cluster, neuron_ids):\n",
        "  neuron_sig = train_suffixes[cluster[0]][neuron_ids]\n",
        "  print \"Num neurons in invariant\", len(neuron_ids)\n",
        "  print \"Neuron id and signature\", zip(neuron_ids, neuron_sig)\n",
        "  print \"Cluster size: \", len(cluster)\n",
        "  print \"Num misclassified\", len([i for i in cluster if is_misclassified(i)])\n",
        "\n",
        "def describe_all_invariants(all_invariants):\n",
        "  df = []\n",
        "  for cl, invs in all_invariants.iteritems():\n",
        "    # Note the number of invariants, and size of the largest invariant cluster\n",
        "    df.append([cl, sum([inv[2] for inv in invs]), len(invs), len([inv for inv in invs if inv[2]>=10]), invs[0][2]])\n",
        "  df = pd.DataFrame(df, columns=['Prediction Class', 'Num Instances', 'Num Invariants', 'Num Invariants with cluster size >= 10', 'Size of largest invariant cluster'])\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KOUniL-T5-Sa",
        "colab_type": "code",
        "outputId": "db04c057-a3cd-4a2d-866c-c434e54e1b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        }
      },
      "cell_type": "code",
      "source": [
        "get_all_invariants(basic_estimator)\n",
        "# Examine cluster/invariants containing a given reference input\n",
        "# ref_id is the index of the reference input\n",
        "ref_id =  1\n",
        "print \"### Reference Sentence ###\"\n",
        "#TODO Convert reference sentence back to words\n",
        "describe_input(ref_id)\n",
        "print \"### Cluster ###\"\n",
        "cluster, neuron_ids, neuron_sig = get_invariant(fine_grained_estimator, ref_id)\n",
        "describe_cluster(cluster, neuron_ids)\n",
        "\n",
        "# Visualize  10 inputs in the cluster\n",
        "for i in cluster[:10]:\n",
        "  describe_input(i)\n",
        "  "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 897/897 [00:00<00:00, 353432.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained all paths\n",
            "### Reference Sentence ###\n",
            "Input in words: at 90 minutes this movie is short , but it feels much longer\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "### Cluster ###\n",
            "Num neurons in invariant 18\n",
            "Neuron id and signature [(54, 0), (389, 0), (457, 1), (1462, 0), (901, 0), (4632, 0), (49, 0), (1709, 0), (527, 0), (2639, 0), (306, 0), (1620, 0), (4285, 0), (1476, 0), (3532, 0), (219, 0), (178, 1), (17, 0)]\n",
            "Cluster size:  27\n",
            "Num misclassified 0\n",
            "Input in words: at 90 minutes this movie is short , but it feels much longer\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: the premise for this kegger comedy probably sounded brilliant four six packs and a pitcher of margaritas in , but the film must have been written in the thrall of a vicious hangover\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: a long winded , predictable scenario\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: all in all , road to perdition is more in love with strangeness than excellence\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: as saccharine movies go , this is likely to cause massive cardiac arrest if taken in large doses\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: i could have used my two hours better watching being john malkovich again\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: hypnotically dull , relentlessly downbeat , laughably predictable wail pitched to the cadence of a depressed fifteen year old 's suicidal poetry\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: afraid to pitch into farce , yet only half hearted in its spy mechanics , all the queen 's men is finally just one long drag\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: philosophically , intellectually and logistically a mess\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: the three leads produce adequate performances , but what 's missing from this material is any depth of feeling\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Isr62nPPlROL",
        "colab_type": "code",
        "outputId": "348e1a91-ee9b-4623-fcb7-ac20b24f83c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "cell_type": "code",
      "source": [
        "# Get all fine_grained_estimator invariants\n",
        "fge_all_invariants = get_all_invariants(fine_grained_estimator)\n",
        "# Print invariant stats\n",
        "df = describe_all_invariants(fge_all_invariants)\n",
        "print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "print df.to_string(index=False)\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 901/901 [00:00<00:00, 218002.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained all paths\n",
            "Total num invariants: 901\n",
            "Total num invariants with cluster size >= 10: 190\n",
            "Prediction Class  Num Instances  Num Invariants  Num Invariants with cluster size >= 10  Size of largest invariant cluster\n",
            "               0           4780             449                                      95                                352\n",
            "              11           4816             452                                      95                                399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "jU7SHWilyZjc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Analyzing clusters of misclassified inputs"
      ]
    },
    {
      "metadata": {
        "id": "6GPZ8nMyYCMj",
        "colab_type": "code",
        "outputId": "b1ebd1c9-eec3-49fb-eda8-902bfb4751b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        }
      },
      "cell_type": "code",
      "source": [
        "# Examine the cluster for a misclasification (Groundtruth: 4, Prediction: 49)\n",
        "invs = fge_all_invariants[00]\n",
        "neuron_ids, neuron_sig, _ = invs[0]\n",
        "cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "describe_cluster(cluster, neuron_ids)\n",
        "\n",
        "# Visualize  10 inputs in the cluster\n",
        "for i in cluster[:10]:\n",
        "  describe_input(i)\n",
        "  # show_img(visualize_conductances(mnist.train.images[i], train_predictions[i], neuron_ids, only_on=False))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num neurons in invariant 17\n",
            "Neuron id and signature [(54, 0), (389, 0), (457, 1), (1462, 0), (901, 0), (4632, 0), (49, 0), (1709, 0), (527, 0), (2639, 0), (306, 0), (1620, 0), (4285, 0), (1476, 0), (3532, 0), (219, 0), (178, 0)]\n",
            "Cluster size:  352\n",
            "Num misclassified 0\n",
            "Input in words: a rote exercise in both animation and storytelling\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: two hours of junk\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: in execution , this clever idea is far less funny than the original , killers from space\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: i have to admit that i am baffled by jason x\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: this thing is just garbage\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: the idea is more interesting than the screenplay , which lags badly in the middle and lurches between not very funny comedy , unconvincing dramatics and some last minute action strongly reminiscent of run lola run\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: no one but a convict guilty of some truly heinous crime should have to sit through the master of disguise\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: what happened with pluto nash \\? how did it ever get made \\?\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: a boring , formulaic mix of serial killers and stalk'n 'slash\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n",
            "Input in words: guilty of the worst sin of attributable to a movie like this it 's not scary in the slightest\n",
            "Groundtruth: 0\n",
            "Prediction: 0\n",
            "Fine-grained prediction 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V-yR4qbD44Qr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test Accuracy Improvements"
      ]
    },
    {
      "metadata": {
        "id": "KBZDfuH42u42",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We use the fine_grained_estimator to check if an input belongs\n",
        "# to a pure cluster (i.e., prediction id of the form 10*label + label).\n",
        "# If so, we declare the network's prediction as a \"condident prediction\".\n",
        "# We measure the accuracy of confident_predictions.\n",
        "fine_grained_estimator_test_predictions = fine_grained_estimator.predict(test_suffixes)\n",
        "fine_grained_estimator_leaf_nodes = fine_grained_estimator.apply(test_suffixes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HT-Fqrp7Kt8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_accuracy_for_label(label):\n",
        "  def get_confidence():\n",
        "    is_confident = (fine_grained_estimator_test_predictions == 10*label + label)\n",
        "    sufficient_samples = fine_grained_estimator.tree_.n_node_samples[fine_grained_estimator_leaf_nodes] >= 10\n",
        "    is_confident *= sufficient_samples\n",
        "    return is_confident\n",
        "  # Following are boolean array. For e.g., with_label[i] is True if\n",
        "  # image i has the given label\n",
        "  with_label = (np.argmax(y_test, axis=1) == label)\n",
        "  print with_label\n",
        "  is_correct = (test_predictions == np.argmax(y_test, axis=1))\n",
        "  with_label_and_correct = with_label*is_correct\n",
        "  is_confident = get_confidence()\n",
        "  with_label_and_correct_and_confident = with_label_and_correct*is_confident\n",
        "  with_label_and_confident = with_label*is_confident\n",
        "\n",
        "  total = np.sum(with_label)\n",
        "  num_conf = np.sum(with_label_and_confident) \n",
        "  num_correct = np.sum(with_label_and_correct)\n",
        "  num_correct_conf = np.sum(with_label_and_correct_and_confident)\n",
        "  return total, num_conf, num_correct, num_correct_conf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f1ed8-tT82wW",
        "colab_type": "code",
        "outputId": "59a51c79-9c19-4ff8-b9f1-1913dbe9d736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "df = []\n",
        "grand_total = 0\n",
        "grand_correct = 0\n",
        "grand_conf = 0\n",
        "grand_correct_conf = 0\n",
        "for i in range(10):\n",
        "  total, num_conf, num_correct, num_correct_conf = test_accuracy_for_label(i)\n",
        "  grand_total += total\n",
        "  grand_conf += num_conf\n",
        "  grand_correct += num_correct\n",
        "  grand_correct_conf += num_correct_conf\n",
        "  acc = 1.0*num_correct/total\n",
        "  conf_acc = 1.0*num_correct_conf/num_conf\n",
        "  df.append([i, total, num_conf, acc, conf_acc])\n",
        "df = pd.DataFrame(df, columns=['Label', 'Instances', 'ConfidentInstances',  'Acc', 'ConfidentAcc',])\n",
        "display(df)\n",
        "print \"Total Instances\", grand_total\n",
        "print \"Num Confident Instances\", grand_conf\n",
        "print \"Orig Accuracy\", 1.0*grand_correct/grand_total\n",
        "print \"Confident Accuracy\", 1.0*grand_correct_conf/grand_conf"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True False  True ... False  True False]\n",
            "[False  True False ...  True False  True]\n",
            "[False False False ... False False False]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-8128112f657f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mgrand_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mgrand_correct_conf\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_correct_conf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_correct\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mconf_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_correct_conf\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_conf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "QhKV-FYLv-Eb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing the Decision Tree"
      ]
    },
    {
      "metadata": {
        "id": "EEZ8ZeuMo_9m",
        "colab_type": "code",
        "outputId": "fba6022b-363e-4e6f-ec5f-7e07504759f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3227
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install graphviz\n",
        "!pip install graphviz\n",
        "import graphviz"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fontconfig libann0 libcairo2 libcdt5 libcgraph6 libdatrie1 libgd3\n",
            "  libgts-0.7-5 libgts-bin libgvc6 libgvpr2 libjbig0 liblab-gamut1 libltdl7\n",
            "  libpango-1.0-0 libpangocairo-1.0-0 libpangoft2-1.0-0 libpathplan4\n",
            "  libpixman-1-0 libthai-data libthai0 libtiff5 libwebp6 libxaw7 libxcb-render0\n",
            "  libxcb-shm0 libxmu6 libxpm4 libxt6\n",
            "Suggested packages:\n",
            "  gsfonts graphviz-doc libgd-tools\n",
            "The following NEW packages will be installed:\n",
            "  fontconfig graphviz libann0 libcairo2 libcdt5 libcgraph6 libdatrie1 libgd3\n",
            "  libgts-0.7-5 libgts-bin libgvc6 libgvpr2 libjbig0 liblab-gamut1 libltdl7\n",
            "  libpango-1.0-0 libpangocairo-1.0-0 libpangoft2-1.0-0 libpathplan4\n",
            "  libpixman-1-0 libthai-data libthai0 libtiff5 libwebp6 libxaw7 libxcb-render0\n",
            "  libxcb-shm0 libxmu6 libxpm4 libxt6\n",
            "0 upgraded, 30 newly installed, 0 to remove and 2 not upgraded.\n",
            "Need to get 4,154 kB of archives.\n",
            "After this operation, 16.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 fontconfig amd64 2.12.6-0ubuntu2 [169 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libann0 amd64 1.1.2+doc-6 [24.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libcdt5 amd64 2.40.1-2 [19.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libcgraph6 amd64 2.40.1-2 [40.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libjbig0 amd64 2.1-3.1build1 [26.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtiff5 amd64 4.0.9-5 [152 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwebp6 amd64 0.6.1-2 [185 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxpm4 amd64 1:3.5.12-1 [34.0 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgd3 amd64 2.2.5-4ubuntu0.2 [119 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgts-0.7-5 amd64 0.7.6+darcs121130-4 [150 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpixman-1-0 amd64 0.34.0-2 [229 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcb-render0 amd64 1.13-1 [14.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcb-shm0 amd64 1.13-1 [5,572 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcairo2 amd64 1.15.10-2 [580 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libltdl7 amd64 2.4.6-2 [38.8 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libthai-data all 0.1.27-2 [133 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdatrie1 amd64 0.2.10-7 [17.8 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libthai0 amd64 0.1.27-2 [18.0 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpango-1.0-0 amd64 1.40.14-1ubuntu0.1 [153 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpangoft2-1.0-0 amd64 1.40.14-1ubuntu0.1 [33.2 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpangocairo-1.0-0 amd64 1.40.14-1ubuntu0.1 [20.8 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libpathplan4 amd64 2.40.1-2 [22.6 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgvc6 amd64 2.40.1-2 [601 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgvpr2 amd64 2.40.1-2 [169 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/universe amd64 liblab-gamut1 amd64 2.40.1-2 [178 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxt6 amd64 1:1.1.5-1 [160 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxmu6 amd64 2:1.1.2-2 [46.0 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxaw7 amd64 2:1.0.13-1 [173 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/universe amd64 graphviz amd64 2.40.1-2 [601 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgts-bin amd64 0.7.6+darcs121130-4 [41.3 kB]\n",
            "Fetched 4,154 kB in 2s (2,225 kB/s)\n",
            "Selecting previously unselected package fontconfig.\n",
            "(Reading database ... 22280 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fontconfig_2.12.6-0ubuntu2_amd64.deb ...\n",
            "Unpacking fontconfig (2.12.6-0ubuntu2) ...\n",
            "Selecting previously unselected package libann0.\n",
            "Preparing to unpack .../01-libann0_1.1.2+doc-6_amd64.deb ...\n",
            "Unpacking libann0 (1.1.2+doc-6) ...\n",
            "Selecting previously unselected package libcdt5.\n",
            "Preparing to unpack .../02-libcdt5_2.40.1-2_amd64.deb ...\n",
            "Unpacking libcdt5 (2.40.1-2) ...\n",
            "Selecting previously unselected package libcgraph6.\n",
            "Preparing to unpack .../03-libcgraph6_2.40.1-2_amd64.deb ...\n",
            "Unpacking libcgraph6 (2.40.1-2) ...\n",
            "Selecting previously unselected package libjbig0:amd64.\n",
            "Preparing to unpack .../04-libjbig0_2.1-3.1build1_amd64.deb ...\n",
            "Unpacking libjbig0:amd64 (2.1-3.1build1) ...\n",
            "Selecting previously unselected package libtiff5:amd64.\n",
            "Preparing to unpack .../05-libtiff5_4.0.9-5_amd64.deb ...\n",
            "Unpacking libtiff5:amd64 (4.0.9-5) ...\n",
            "Selecting previously unselected package libwebp6:amd64.\n",
            "Preparing to unpack .../06-libwebp6_0.6.1-2_amd64.deb ...\n",
            "Unpacking libwebp6:amd64 (0.6.1-2) ...\n",
            "Selecting previously unselected package libxpm4:amd64.\n",
            "Preparing to unpack .../07-libxpm4_1%3a3.5.12-1_amd64.deb ...\n",
            "Unpacking libxpm4:amd64 (1:3.5.12-1) ...\n",
            "Selecting previously unselected package libgd3:amd64.\n",
            "Preparing to unpack .../08-libgd3_2.2.5-4ubuntu0.2_amd64.deb ...\n",
            "Unpacking libgd3:amd64 (2.2.5-4ubuntu0.2) ...\n",
            "Selecting previously unselected package libgts-0.7-5:amd64.\n",
            "Preparing to unpack .../09-libgts-0.7-5_0.7.6+darcs121130-4_amd64.deb ...\n",
            "Unpacking libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\n",
            "Selecting previously unselected package libpixman-1-0:amd64.\n",
            "Preparing to unpack .../10-libpixman-1-0_0.34.0-2_amd64.deb ...\n",
            "Unpacking libpixman-1-0:amd64 (0.34.0-2) ...\n",
            "Selecting previously unselected package libxcb-render0:amd64.\n",
            "Preparing to unpack .../11-libxcb-render0_1.13-1_amd64.deb ...\n",
            "Unpacking libxcb-render0:amd64 (1.13-1) ...\n",
            "Selecting previously unselected package libxcb-shm0:amd64.\n",
            "Preparing to unpack .../12-libxcb-shm0_1.13-1_amd64.deb ...\n",
            "Unpacking libxcb-shm0:amd64 (1.13-1) ...\n",
            "Selecting previously unselected package libcairo2:amd64.\n",
            "Preparing to unpack .../13-libcairo2_1.15.10-2_amd64.deb ...\n",
            "Unpacking libcairo2:amd64 (1.15.10-2) ...\n",
            "Selecting previously unselected package libltdl7:amd64.\n",
            "Preparing to unpack .../14-libltdl7_2.4.6-2_amd64.deb ...\n",
            "Unpacking libltdl7:amd64 (2.4.6-2) ...\n",
            "Selecting previously unselected package libthai-data.\n",
            "Preparing to unpack .../15-libthai-data_0.1.27-2_all.deb ...\n",
            "Unpacking libthai-data (0.1.27-2) ...\n",
            "Selecting previously unselected package libdatrie1:amd64.\n",
            "Preparing to unpack .../16-libdatrie1_0.2.10-7_amd64.deb ...\n",
            "Unpacking libdatrie1:amd64 (0.2.10-7) ...\n",
            "Selecting previously unselected package libthai0:amd64.\n",
            "Preparing to unpack .../17-libthai0_0.1.27-2_amd64.deb ...\n",
            "Unpacking libthai0:amd64 (0.1.27-2) ...\n",
            "Selecting previously unselected package libpango-1.0-0:amd64.\n",
            "Preparing to unpack .../18-libpango-1.0-0_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpango-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpangoft2-1.0-0:amd64.\n",
            "Preparing to unpack .../19-libpangoft2-1.0-0_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpangoft2-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpangocairo-1.0-0:amd64.\n",
            "Preparing to unpack .../20-libpangocairo-1.0-0_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpangocairo-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpathplan4.\n",
            "Preparing to unpack .../21-libpathplan4_2.40.1-2_amd64.deb ...\n",
            "Unpacking libpathplan4 (2.40.1-2) ...\n",
            "Selecting previously unselected package libgvc6.\n",
            "Preparing to unpack .../22-libgvc6_2.40.1-2_amd64.deb ...\n",
            "Unpacking libgvc6 (2.40.1-2) ...\n",
            "Selecting previously unselected package libgvpr2.\n",
            "Preparing to unpack .../23-libgvpr2_2.40.1-2_amd64.deb ...\n",
            "Unpacking libgvpr2 (2.40.1-2) ...\n",
            "Selecting previously unselected package liblab-gamut1.\n",
            "Preparing to unpack .../24-liblab-gamut1_2.40.1-2_amd64.deb ...\n",
            "Unpacking liblab-gamut1 (2.40.1-2) ...\n",
            "Selecting previously unselected package libxt6:amd64.\n",
            "Preparing to unpack .../25-libxt6_1%3a1.1.5-1_amd64.deb ...\n",
            "Unpacking libxt6:amd64 (1:1.1.5-1) ...\n",
            "Selecting previously unselected package libxmu6:amd64.\n",
            "Preparing to unpack .../26-libxmu6_2%3a1.1.2-2_amd64.deb ...\n",
            "Unpacking libxmu6:amd64 (2:1.1.2-2) ...\n",
            "Selecting previously unselected package libxaw7:amd64.\n",
            "Preparing to unpack .../27-libxaw7_2%3a1.0.13-1_amd64.deb ...\n",
            "Unpacking libxaw7:amd64 (2:1.0.13-1) ...\n",
            "Selecting previously unselected package graphviz.\n",
            "Preparing to unpack .../28-graphviz_2.40.1-2_amd64.deb ...\n",
            "Unpacking graphviz (2.40.1-2) ...\n",
            "Selecting previously unselected package libgts-bin.\n",
            "Preparing to unpack .../29-libgts-bin_0.7.6+darcs121130-4_amd64.deb ...\n",
            "Unpacking libgts-bin (0.7.6+darcs121130-4) ...\n",
            "Setting up libgts-0.7-5:amd64 (0.7.6+darcs121130-4) ...\n",
            "Setting up libpathplan4 (2.40.1-2) ...\n",
            "Setting up liblab-gamut1 (2.40.1-2) ...\n",
            "Setting up libxcb-render0:amd64 (1.13-1) ...\n",
            "Setting up libjbig0:amd64 (2.1-3.1build1) ...\n",
            "Setting up libdatrie1:amd64 (0.2.10-7) ...\n",
            "Setting up libtiff5:amd64 (4.0.9-5) ...\n",
            "Setting up libpixman-1-0:amd64 (0.34.0-2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libltdl7:amd64 (2.4.6-2) ...\n",
            "Setting up libann0 (1.1.2+doc-6) ...\n",
            "Setting up libxcb-shm0:amd64 (1.13-1) ...\n",
            "Setting up libxpm4:amd64 (1:3.5.12-1) ...\n",
            "Setting up libxt6:amd64 (1:1.1.5-1) ...\n",
            "Setting up libgts-bin (0.7.6+darcs121130-4) ...\n",
            "Setting up libthai-data (0.1.27-2) ...\n",
            "Setting up libcdt5 (2.40.1-2) ...\n",
            "Setting up fontconfig (2.12.6-0ubuntu2) ...\n",
            "Regenerating fonts cache... done.\n",
            "Setting up libcgraph6 (2.40.1-2) ...\n",
            "Setting up libwebp6:amd64 (0.6.1-2) ...\n",
            "Setting up libcairo2:amd64 (1.15.10-2) ...\n",
            "Setting up libgvpr2 (2.40.1-2) ...\n",
            "Setting up libgd3:amd64 (2.2.5-4ubuntu0.2) ...\n",
            "Setting up libthai0:amd64 (0.1.27-2) ...\n",
            "Setting up libxmu6:amd64 (2:1.1.2-2) ...\n",
            "Setting up libpango-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Setting up libxaw7:amd64 (2:1.0.13-1) ...\n",
            "Setting up libpangoft2-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Setting up libpangocairo-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Setting up libgvc6 (2.40.1-2) ...\n",
            "Setting up graphviz (2.40.1-2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Collecting graphviz\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/e2/ef2581b5b86625657afd32030f90cf2717456c1d2b711ba074bf007c0f1a/graphviz-0.10.1-py2.py3-none-any.whl\n",
            "Installing collected packages: graphviz\n",
            "Successfully installed graphviz-0.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZaOsPspkjuOe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dot_data = tree.export_graphviz(basic_estimator, out_file=None) \n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zmQuqo4Nn1zY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dot_data = tree.export_graphviz(fine_grained_estimator, out_file=None) \n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}