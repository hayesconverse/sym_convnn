{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR Invariants v2 [FINAL].ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "_fzGgbXrjF3I",
        "rW16gvv7ifik",
        "oZA0U3oG-kgu",
        "yR9bYFN9E9lg",
        "iG5Fq5GIkP6u",
        "i_uhUyYiBlL7",
        "QhKV-FYLv-Eb"
      ]
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWtW7m9i8Q1e",
        "colab_type": "code",
        "outputId": "a27ffe20-90f0-41bb-8277-06d4315cec4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!pip install tensorflow-datasets"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-datasets\n",
            "\u001b[31m  Could not find a version that satisfies the requirement tensorflow-datasets (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for tensorflow-datasets\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-JL_MKop5Qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import io\n",
        "import os\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn import tree\n",
        "from tqdm import tqdm\n",
        "import operator\n",
        "import pandas as pd\n",
        "#import keras\n",
        "#import h5py\n",
        "#import cPickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEicCONRlAWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Go back to top of filesystem\n",
        "os.chdir('/content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y90-yo7ki7JG",
        "colab_type": "text"
      },
      "source": [
        "## Restore Keras Model (Self-contained, do not mix with Tensorflow setup)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw8twivbjC-Y",
        "colab_type": "code",
        "outputId": "138bf8bc-eeab-448b-e11d-e75f9b26e1d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# RUN ONCE\n",
        "# Run Runtime->Reset ALL Runtimes to reset\n",
        "!git clone https://github.com/mzweilin/EvadeML-Zoo\n",
        "os.chdir('EvadeML-Zoo')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'EvadeML-Zoo'...\n",
            "remote: Enumerating objects: 952, done.\u001b[K\n",
            "remote: Total 952 (delta 0), reused 0 (delta 0), pack-reused 952\u001b[K\n",
            "Receiving objects: 100% (952/952), 27.54 MiB | 28.23 MiB/s, done.\n",
            "Resolving deltas: 100% (606/606), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fscNTMGNjQTP",
        "colab_type": "code",
        "outputId": "3fdcefae-7b61-4498-cca5-b5ef5017f396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!mkdir downloads\n",
        "!curl -sL https://github.com/mzweilin/EvadeML-Zoo/releases/download/v0.1/downloads.tar.gz | tar xzv -C downloads"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MagNet/\n",
            "MagNet/defensive_models/\n",
            "MagNet/defensive_models/MNIST_II\n",
            "MagNet/defensive_models/MNIST_I\n",
            "MagNet/defensive_models/CIFAR\n",
            "trained_models/\n",
            "trained_models/MNIST_pgdtrained.keras_weights.h5\n",
            "trained_models/MNIST_pgdbase.keras_weights.h5\n",
            "trained_models/CIFAR-10_carlini.keras_weights.h5\n",
            "trained_models/MNIST_cleverhans.keras_weights.h5\n",
            "trained_models/MNIST_cleverhans_adv_trained.keras_weights.h5\n",
            "trained_models/MNIST_carlini.keras_weights.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdMqHDtXjUkE",
        "colab_type": "code",
        "outputId": "b3a7d37f-6744-49f5-a12e-ad17b6948aab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Imported from the EvadeML-Zoo project downloaded above\n",
        "from models import carlini_models\n",
        "#from models import densenet_models\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.datasets import mnist as keras_mnist\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2nW1f7AjXFB",
        "colab_type": "code",
        "outputId": "3404f4c2-7ac5-4d67-f42b-f249d3266884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "K.clear_session()\n",
        "eml_carlini_cifar_model = carlini_models.carlini_cifar10_model(logits=False, input_range_type=1)\n",
        "eml_carlini_cifar_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['acc'])\n",
        "\n",
        "dataset_name = \"CIFAR-10\"\n",
        "model_name = \"carlini\"\n",
        "model_weights_fpath = \"%s_%s.keras_weights.h5\" % (dataset_name, model_name)\n",
        "model_weights_fpath = os.path.join('downloads/trained_models', model_weights_fpath)\n",
        "eml_carlini_cifar_model.load_weights(model_weights_fpath)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkKPDNx09iEQ",
        "colab_type": "code",
        "outputId": "eda5acb6-8501-44f0-ed9c-5c3384912689",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "eml_carlini_cifar_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_1 (Lambda)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "lambda_2 (Lambda)            (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 30, 30, 64)        1792      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 128)       147584    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 10, 10, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               819456    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,147,978\n",
            "Trainable params: 1,147,978\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oap31V_1jbBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = K.get_session()\n",
        "graph = sess.graph\n",
        "\n",
        "t_image = graph.get_tensor_by_name('lambda_1_input:0')\n",
        "t_prediction = graph.get_tensor_by_name('activation_7/Softmax:0')\n",
        "\n",
        "#Layer selection. Large ones crash runtime when making basic estimator.\n",
        "#t_layer = graph.get_tensor_by_name('max_pooling2d_1/MaxPool:0') \n",
        "#t_layer = tf.reshape(t_layer, [-1, 12544])\n",
        "#t_layer = graph.get_tensor_by_name('activation_3/Relu:0')\n",
        "#t_layer = tf.reshape(t_layer, [-1, 18432])\n",
        "#t_layer = graph.get_tensor_by_name('activation_4/Relu:0')\n",
        "#t_layer = tf.reshape(t_layer, [-1, 12800])\n",
        "t_layer = graph.get_tensor_by_name('max_pooling2d_1/MaxPool:0')\n",
        "t_layer = tf.reshape(t_layer, [-1, 12544])\n",
        "#t_layer = graph.get_tensor_by_name('flatten_1/Reshape:0')\n",
        "#t_layer = graph.get_tensor_by_name('activation_5/Relu:0') #Shape: 256\n",
        "#t_layer = graph.get_tensor_by_name('activation_6/Relu:0') #Shape: 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtzIpfg_903s",
        "colab_type": "code",
        "outputId": "ac25952c-79aa-448a-dd84-f1b0f9da89be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "graph.get_tensor_by_name('flatten_1/Reshape:0')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'flatten_1/Reshape:0' shape=(?, ?) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnVcWF1UlJGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10 as keras_cifar\n",
        "from keras.utils import np_utils\n",
        "\n",
        "def get_prediction_keras(images, tensor=None, batch_size=100):\n",
        "  images = np.reshape(images, [len(images), 32, 32, 3])\n",
        "  if tensor is None:\n",
        "    return eml_carlini_cifar_model.predict(images, batch_size=100)\n",
        "  def get_prediction_batch(batch):\n",
        "    feed = {t_image: np.array(batch)}\n",
        "    return sess.run(tensor, feed_dict=feed)\n",
        "  n = len(images)\n",
        "  if n%batch_size == 0:\n",
        "    batches = [images[i*batch_size:(i+1)*batch_size] for i in range(int(n/batch_size))]\n",
        "  else:\n",
        "    batches = [images[i*batch_size:(i+1)*batch_size] for i in range(int(n/batch_size) +1)]    \n",
        "  batch_predictions = [get_prediction_batch(b) for b in tqdm(batches)]\n",
        "  return np.concatenate(tuple(batch_predictions), axis=0)\n",
        "\n",
        "def get_test_dataset():\n",
        "  (X_train, y_train), (X_test, y_test) = keras_cifar.load_data()\n",
        "  X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
        "  X_test = X_test.astype('float32')\n",
        "  X_test /= 255\n",
        "  Y_test = np_utils.to_categorical(y_test, 10)\n",
        "  del X_train, y_train\n",
        "  return X_test, Y_test\n",
        "\n",
        "def get_train_dataset():\n",
        "  (X_train, y_train), (X_test, y_test) = keras_cifar.load_data()\n",
        "  X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
        "  X_train = X_train.astype('float32')\n",
        "  X_train /= 255\n",
        "  Y_train = np_utils.to_categorical(y_train, 10)\n",
        "  del X_test, y_test\n",
        "  return X_train, Y_train\n",
        "\n",
        "# t_layer is defined right where the Keras model is defined\n",
        "def fingerprint_suffix_keras(images, tensor=t_layer):\n",
        "  print \"Getting fingerprint for\", tensor.name\n",
        "  return (get_prediction_keras(images, tensor=tensor)>0.0).astype('int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-meqrEGlcE2",
        "colab_type": "code",
        "outputId": "c3f0b9b6-fa39-4c21-ef66-fab2bdc7e1ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Evaluate model accuracy\n",
        "test_images, test_labels = get_test_dataset()\n",
        "test_labels = np.argmax(test_labels, axis=1)\n",
        "test_predictions = get_prediction_keras(test_images)\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "print \"Test accuracy\", 1.0*np.sum(test_predictions == test_labels)/len(test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 21s 0us/step\n",
            "170508288/170498071 [==============================] - 21s 0us/step\n",
            "Test accuracy 0.7796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwlMAnU7qOD6",
        "colab_type": "code",
        "outputId": "99259ea0-538a-4b74-9f9d-d96e8cbbba7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_images, train_labels = get_train_dataset()\n",
        "train_labels = np.argmax(train_labels, axis=1)\n",
        "\n",
        "train_predictions_all = get_prediction_keras(train_images)\n",
        "train_predictions = np.argmax(train_predictions_all, axis=1)\n",
        "print \"Predictions computed for all training data\"\n",
        "test_predictions_all = get_prediction_keras(test_images)\n",
        "test_predictions = np.argmax(test_predictions_all, axis=1)\n",
        "print \"Predictions computed for all test data\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions computed for all training data\n",
            "Predictions computed for all test data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBOKgW1gRPwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_suffixes = fingerprint_suffix_keras(train_images)\n",
        "print \"Suffixes computed for all training data\"\n",
        "\n",
        "\n",
        "test_suffixes = fingerprint_suffix_keras(test_images)\n",
        "print \"Suffixes computed for all test data\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fzGgbXrjF3I",
        "colab_type": "text"
      },
      "source": [
        "## Set Up Tensorflow Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmPpG06W_vnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RUN ONCE\n",
        "# Run Runtime->Reset ALL Runtimes to reset\n",
        "!git clone https://github.com/tensorflow/models\n",
        "os.chdir('models/tutorials/image/cifar10/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgohW_DIIcEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imported from the Tensorflow deep cnn tutorial scripts, cloned above.\n",
        "import cifar10\n",
        "import cifar10_input\n",
        "# Fix for flags from Burak's Textual Invariants colab.\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPtumJ1vcCfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cifar10.maybe_download_and_extract()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj7gi7aaUQDi",
        "colab_type": "code",
        "outputId": "121c2d35-2ca5-4664-cb16-642a9d9cdc82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "# Get test and train images.\n",
        "with tf.Graph().as_default():\n",
        "  train_images_bin, train_labels_bin = cifar10_input.inputs(False, \"/tmp/cifar10_data/cifar-10-batches-bin\", 50000)\n",
        "  test_images_bin, test_labels_bin = cifar10_input.inputs(True, \"/tmp/cifar10_data/cifar-10-batches-bin\", 10000)\n",
        "  with tf.train.MonitoredSession() as sess:\n",
        "    test_images, test_labels = sess.run([test_images_bin, test_labels_bin])\n",
        "    train_images, train_labels = sess.run([train_images_bin, train_labels_bin])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From cifar10_input.py:232: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:197: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From cifar10_input.py:79: __init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\n",
            "WARNING:tensorflow:From cifar10_input.py:132: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-jo6PRTQZAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = cifar10.FLAGS.batch_size\n",
        "\n",
        "def create_model(t_images, shape=None):\n",
        "  # If t_images is None then it creates a placeholder for feeding\n",
        "  # images. The placeholder has the provided shape.\n",
        "  #\n",
        "  # Creates model with images supplied by the t_images tensor.\n",
        "  if t_images is not None:  \n",
        "    t_images_pl = tf.placeholder_with_default(t_images, shape=t_images.shape)\n",
        "  else:\n",
        "    t_images_pl = tf.placeholder(shape)\n",
        "  # Build a Graph that computes the logits predictions from the\n",
        "  # inference model.\n",
        "  t_logits = cifar10.inference(t_images_pl)\n",
        "  return t_images_pl, t_logits\n",
        "\n",
        "def get_prediction(sess, tensor, t_images_pl, images, batch_size=BATCH_SIZE):\n",
        "  \"\"\"\n",
        "  Evaluate given tensor by feeding the provided images.\n",
        "  TODO: Currently partial batches cannot be evaluated so len(images) must\n",
        "  be a multiple of batch_size.\n",
        "  \"\"\"\n",
        "  def get_prediction_batch(image_batch):\n",
        "    feed = {t_images_pl: np.array(image_batch)}\n",
        "    return sess.run(tensor, feed_dict=feed)\n",
        "  \n",
        "  def get_final_batch(images, batch_size):\n",
        "    pad_value = np.zeros(images[0].shape)\n",
        "    final_batch = images[int(len(images)/batch_size)*batch_size:]\n",
        "    pad_size = batch_size-len(final_batch)\n",
        "    pad = [pad_value for _ in range(pad_size)]\n",
        "    final_batch = np.concatenate(tuple([final_batch, pad]), axis=0)\n",
        "    return final_batch, pad_size\n",
        "  \n",
        "  n = len(images)\n",
        "  image_batches = [images[i*batch_size:(i+1)*batch_size] for i in range(int(n/batch_size))]\n",
        "  padding_size = 0\n",
        "  if (n % batch_size) != 0:\n",
        "    # Pad the remainting entries and create an additional batch\n",
        "    final_batch, pad_size = get_final_batch(images, batch_size)\n",
        "    image_batches += [final_batch]\n",
        "  batch_predictions = [get_prediction_batch(b) for b in tqdm(image_batches)]\n",
        "  # remove padding\n",
        "  batch_predictions = batch_predictions[:-1] + [batch_predictions[-1][:-pad_size]]\n",
        "  res = np.concatenate(tuple(batch_predictions), axis=0)\n",
        "  assert res.shape[0] == images.shape[0]\n",
        "  return res\n",
        "  \n",
        "def train_model(save_steps=500):\n",
        "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
        "  with tf.Graph().as_default():\n",
        "    t_global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "    # Get images and labels for CIFAR-10.\n",
        "    # Force input pipeline to CPU:0 to avoid operations sometimes ending up on\n",
        "    # GPU and resulting in a slow down.\n",
        "    with tf.device('/cpu:0'):\n",
        "      t_images, t_labels = cifar10.distorted_inputs()\n",
        "\n",
        "    # Build a Graph that computes the logits predictions from the\n",
        "    # inference model.\n",
        "    t_images_pl, t_logits = create_model(t_images)\n",
        "\n",
        "    # Calculate loss.\n",
        "    t_loss = cifar10.loss(t_logits, t_labels)\n",
        "\n",
        "    # Build a Graph that trains the model with one batch of examples and\n",
        "    # updates the model parameters.\n",
        "    t_train_op = cifar10.train(t_loss, t_global_step)\n",
        "    saver_hook = tf.train.CheckpointSaverHook(\n",
        "      checkpoint_dir='./checkpoints',\n",
        "      save_secs=None,\n",
        "      save_steps=save_steps,\n",
        "      saver=tf.train.Saver(),\n",
        "      checkpoint_basename='cifar_model.ckpt',\n",
        "      scaffold=None)\n",
        "    with tf.train.MonitoredTrainingSession(hooks=[saver_hook]) as sess:\n",
        "      while not sess.should_stop():\n",
        "        l, step, _ = sess.run([t_loss, t_global_step, t_train_op])\n",
        "        if step % save_steps == 0:\n",
        "          test_logits = get_prediction(sess, t_logits, t_images_pl, test_images)\n",
        "          test_accuracy = np.mean(np.equal(np.argmax(test_logits, axis=1), test_labels))\n",
        "          print \"Loss at step %d: %f\" % (step , l)\n",
        "          print \"Test accuracy at step %d: %f\" % (step, test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW16gvv7ifik",
        "colab_type": "text"
      },
      "source": [
        "## Train a new CIFAR10 Tensorflow model, save checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqAtzqaPiiHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBZe6Hx9dHsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ckpt_path_name = \"./checkpoints/cifar_model.ckpt-84000\"\n",
        "from google.colab import files\n",
        "files.download(ckpt_path_name + '.data-00000-of-00001')\n",
        "files.download(ckpt_path_name + '.index')\n",
        "files.download(ckpt_path_name + '.meta')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "48d22469-1248-4a1a-91b5-f21ec64ef775",
        "id": "UeexhNxP-kL3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "!ls ./checkpoints"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\n",
            "cifar_model.ckpt-1.data-00000-of-00001\n",
            "cifar_model.ckpt-1.index\n",
            "cifar_model.ckpt-1.meta\n",
            "cifar_model.ckpt-400.data-00000-of-00001\n",
            "cifar_model.ckpt-400.index\n",
            "cifar_model.ckpt-400.meta\n",
            "cifar_model.ckpt-600.data-00000-of-00001\n",
            "cifar_model.ckpt-600.index\n",
            "cifar_model.ckpt-600.meta\n",
            "cifar_model.ckpt-65100.data-00000-of-00001\n",
            "cifar_model.ckpt-65100.index\n",
            "cifar_model.ckpt-65100.meta\n",
            "cifar_model.ckpt-65200.data-00000-of-00001\n",
            "cifar_model.ckpt-65200.index\n",
            "cifar_model.ckpt-65200.meta\n",
            "cifar_model.ckpt-65300.data-00000-of-00001\n",
            "cifar_model.ckpt-65300.index\n",
            "cifar_model.ckpt-65300.meta\n",
            "cifar_model.ckpt-65400.data-00000-of-00001\n",
            "cifar_model.ckpt-65400.index\n",
            "cifar_model.ckpt-65400.meta\n",
            "cifar_model.ckpt-700.data-00000-of-00001\n",
            "cifar_model.ckpt-700.index\n",
            "cifar_model.ckpt-700.meta\n",
            "cifar_model.ckpt-800.data-00000-of-00001\n",
            "cifar_model.ckpt-800.index\n",
            "cifar_model.ckpt-800.meta\n",
            "cifar_model.ckpt-83500.data-00000-of-00001\n",
            "cifar_model.ckpt-83500.index\n",
            "cifar_model.ckpt-83500.meta\n",
            "cifar_model.ckpt-84000.data-00000-of-00001\n",
            "cifar_model.ckpt-84000.index\n",
            "cifar_model.ckpt-84000.meta\n",
            "cifar_model.ckpt-84500.data-00000-of-00001\n",
            "cifar_model.ckpt-84500.index\n",
            "cifar_model.ckpt-84500.meta\n",
            "cifar_model.ckpt-85000.data-00000-of-00001\n",
            "cifar_model.ckpt-85000.index\n",
            "cifar_model.ckpt-85000.meta\n",
            "cifar_model.ckpt-85500.data-00000-of-00001\n",
            "cifar_model.ckpt-85500.index\n",
            "cifar_model.ckpt-85500.meta\n",
            "cifar_model.ckpt-900.data-00000-of-00001\n",
            "cifar_model.ckpt-900.index\n",
            "events.out.tfevents.1543620789.ed27981316da\n",
            "graph.pbtxt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oZA0U3oG-kgu"
      },
      "source": [
        "## Restore Tensorflow model from checkpoint on Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORKRKdAB_bkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ./restored_checkpoints\n",
        "!wget https://github.com/hayesconverse/sym_convnn/raw/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.index -O ./restored_checkpoints/cifar_model.ckpt.index\n",
        "!wget https://github.com/hayesconverse/sym_convnn/raw/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.meta -O ./restored_checkpoints/cifar_model.ckpt.meta\n",
        "!wget https://github.com/hayesconverse/sym_convnn/raw/master/CIFAR10_colab_checkpoint/checkpoints_ataly_model/cifar_model.ckpt-84000.data-00000-of-00001 -O ./restored_checkpoints/cifar_model.ckpt.data-00000-of-00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f8b1af94-0f34-468f-fc02-9df58dba4987",
        "id": "cNPbulWH-kgo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# From now on we work with the default graph.\n",
        "t_images_pl, t_logits = create_model(test_images[:BATCH_SIZE])\n",
        "sess = tf.Session()\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(sess, './restored_checkpoints/cifar_model.ckpt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./restored_checkpoints/cifar_model.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0e30a629-5ede-43a6-b0cd-e74af60ea025",
        "id": "Xz4RAbzq-kgj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_predictions_all = get_prediction(sess, t_logits, t_images_pl, train_images)\n",
        "train_predictions = np.argmax(train_predictions_all, axis=1)\n",
        "test_predictions_all = get_prediction(sess, t_logits, t_images_pl, test_images)\n",
        "test_predictions = np.argmax(test_predictions_all, axis=1)\n",
        "\n",
        "# ACCURACY\n",
        "print \"\"\n",
        "print \"Accuracy\", np.mean(np.equal(test_predictions, test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:05<00:00, 74.86it/s]\n",
            "100%|██████████| 79/79 [00:00<00:00, 91.08it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy 0.8314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR9bYFN9E9lg",
        "colab_type": "text"
      },
      "source": [
        "## Library for IG Attribution and Conductance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2F_QleSkgvY",
        "colab_type": "code",
        "outputId": "840c43f7-98fa-4dc8-c075-3b709d2756ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "t_fc1 = sess.graph.get_tensor_by_name('local4/local4:0')\n",
        "t_conv1 = sess.graph.get_tensor_by_name('conv1/conv1:0')\n",
        "t_label = tf.placeholder(tf.int32)\n",
        "t_neuron_id = tf.placeholder(tf.int32)\n",
        "t_grad = tf.gradients(t_logits[:, t_label], t_images_pl)\n",
        "print t_fc1.shape\n",
        "print t_conv1.shape\n",
        "#t_conv2 = sess.graph.get_tensor_by_name('import/h_conv2:0')\n",
        "t_grad_neuron = tf.gradients(t_logits[:, t_label], t_fc1)[0]\n",
        "t_grad_conductance = tf.gradients(t_fc1[:,t_neuron_id], t_images_pl, grad_ys=t_grad_neuron[:, t_neuron_id])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 192)\n",
            "(128, 24, 24, 64)\n",
            "<tensorflow.python.framework.ops.Graph object at 0x7f4178da41d0>\n",
            "<tensorflow.python.framework.ops.Graph object at 0x7f4178da41d0>\n",
            "<tensorflow.python.framework.ops.Graph object at 0x7f4178da41d0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL92gsWskV0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "\n",
        "def attribute(inp, label, baseline=None, steps=50, use_top_label=False):\n",
        "  def top_label(inp):\n",
        "    return np.argmax(get_prediction(sess, t_logits, t_images_pl, [inp])[0])\n",
        "  if baseline is None:\n",
        "    baseline = 0*inp\n",
        "  scaled_inputs = [baseline + (float(i)/steps)*(inp-baseline) for i in range(0, steps)]\n",
        "  #feed_dict = {W_conv1: convWeightMatrix[0], b_conv1: convBiasMatrix[0], \n",
        "  #           W_conv2: convWeightMatrix[1], b_conv2: convBiasMatrix[1], \n",
        "  #           W_conv3: convWeightMatrix[2], b_conv3: convBiasMatrix[2], \n",
        "  #           W_conv4: convWeightMatrix[3], b_conv4: convBiasMatrix[3], \n",
        "  #           W_fc1: denseWeightMatrix[0], b_fc1: denseBiasMatrix[0], \n",
        "  #           W_fc2: denseWeightMatrix[1], b_fc2: denseBiasMatrix[1]}\n",
        "  if use_top_label:\n",
        "    feed_dict[x] = [inp]\n",
        "    logits = sess.run(t_logits, feed_dict=feed_dict)[0]\n",
        "    label = np.argmax(logits)\n",
        "  feed_dict[x] = scaled_inputs\n",
        "  feed_dict[t_label] = label\n",
        "  grads, scores = sess.run([t_grad, t_logits], feed_dict=feed_dict)  # shapes: <steps+1>, <steps+1, inp.shape>\n",
        "  integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "  print \"FINAL SCORE\", scores[-1][label]\n",
        "  print \"BASELINE SCORE\", scores[0][label]\n",
        "  print \"SUM\", np.sum(integrated_gradients), \"DIFF\", scores[-1][label] - scores[0][label]\n",
        "  return integrated_gradients\n",
        "\n",
        "def conductance(inp, label, neuron_id=None, baseline=None, steps=50):\n",
        "  # neuron_id is the id of the neuron in layer t_fc1 through which conductance\n",
        "  # must be computed. If None, vanilla IG is computed.\n",
        "  if baseline is None:\n",
        "    baseline = 0*inp\n",
        "  scaled_inputs = [baseline + (float(i)/steps)*(inp-baseline) for i in range(0, steps)]\n",
        "  #feed_dict = {W_conv1: convWeightMatrix[0], b_conv1: convBiasMatrix[0], \n",
        "  #           W_conv2: convWeightMatrix[1], b_conv2: convBiasMatrix[1], \n",
        "  #           W_conv3: convWeightMatrix[2], b_conv3: convBiasMatrix[2], \n",
        "  #           W_conv4: convWeightMatrix[3], b_conv4: convBiasMatrix[3], \n",
        "  #           W_fc1: denseWeightMatrix[0], b_fc1: denseBiasMatrix[0], \n",
        "  #           W_fc2: denseWeightMatrix[1], b_fc2: denseBiasMatrix[1]}\n",
        "  feed_dict[x] = scaled_inputs\n",
        "  feed_dict[t_label] = label\n",
        "  if neuron_id != None:\n",
        "    feed_dict[t_neuron_id] = neuron_id\n",
        "    grads, scores = sess.run([t_grad_conductance, t_logits], feed_dict=feed_dict)  # shapes: <steps+1>, <steps+1, inp.shape>\n",
        "    integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "    return integrated_gradients\n",
        "  grads, scores = sess.run([t_grad, t_logits], feed_dict=feed_dict)  # shapes: <steps+1>, <steps+1, inp.shape>    \n",
        "  integrated_gradients = (inp-baseline)*np.average(grads[0], axis=0)  # shape: <inp.shape>\n",
        "  print \"FINAL SCORE\", scores[-1][label]\n",
        "  print \"BASELINE SCORE\", scores[0][label]\n",
        "  print \"SUM\", np.sum(integrated_gradients), \"DIFF\", scores[-1][label] - scores[0][label]\n",
        "  return integrated_gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG5Fq5GIkP6u",
        "colab_type": "text"
      },
      "source": [
        "## Library for Visualizing Images and Attributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czxZISZYkLaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import PIL.Image\n",
        "from IPython.display import clear_output, Image, display, HTML\n",
        "import numpy as np\n",
        "from cStringIO import StringIO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_o25suPkTL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FONT_PATH='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf'\n",
        "IMAGE_SIZE=24\n",
        "\n",
        "def mnist_to_rgb(mnist_img):\n",
        "  \"\"\"\n",
        "  Transformsn an MNIST image (shape: <784>) to a grayscale\n",
        "  RGB image (shape: <28,28,3>)\n",
        "  \"\"\"\n",
        "  pixel_array = mnist_img.reshape(IMAGE_SIZE, IMAGE_SIZE)  # shape: 28,28\n",
        "  rgb_image = np.transpose([pixel_array,pixel_array,pixel_array], axes=[1,2,0])\n",
        "  return rgb_image\n",
        "\n",
        "def cifar_to_rgb(cifar_img):\n",
        "  rgb_image = cifar_img.reshape(IMAGE_SIZE, IMAGE_SIZE, 3) # shape: 32,32,3\n",
        "  return rgb_image\n",
        "\n",
        "def pil_img(a):\n",
        "  '''Returns a PIL image created from the provided RGB array.\n",
        "  '''\n",
        "  a = np.uint8(a)\n",
        "  return PIL.Image.fromarray(a)\n",
        "\n",
        "def mnist_to_pil_img(inp):\n",
        "  rgb_inp = 255*mnist_to_rgb(inp)\n",
        "  vis_inp = pil_img(rgb_inp)\n",
        "  return vis_inp  \n",
        "\n",
        "def cifar_to_pil_img(inp):\n",
        "  rgb_inp = 255*cifar_to_rgb(inp)\n",
        "  vis_inp = pil_img(rgb_inp) \n",
        "\n",
        "def pil_fig(fig):\n",
        "  # Returns a PIL image obtained from the provided PLT figure.\n",
        "  buf = io.BytesIO()\n",
        "  fig.savefig(buf, format='png')\n",
        "  plt.close(fig)\n",
        "  buf.seek(0)\n",
        "  img = PIL.Image.open(buf)\n",
        "  return img\n",
        "\n",
        "def show_img(img, fmt='jpeg'):\n",
        "  '''Displays the provided PIL image\n",
        "  '''\n",
        "  f = StringIO()\n",
        "  img.save(f, fmt)\n",
        "  display(Image(data=f.getvalue()))\n",
        " \n",
        "def show_mnist_img(mnist_img):\n",
        "  show_img(pil_img(255*mnist_to_rgb(mnist_img)))\n",
        "  \n",
        "def show_cifar_img(cifar_img):\n",
        "  show_img(pil_img(255*cifar_to_rgb(cifar_img)))\n",
        "  \n",
        "def gray_scale(img):\n",
        "  '''Converts the provided RGB image to gray scale.\n",
        "  '''\n",
        "  img = np.average(img, axis=2)\n",
        "  return np.transpose([img, img, img], axes=[1,2,0])\n",
        "\n",
        "def normalize(attrs, ptile=99):\n",
        "  '''Normalize the provided attributions so that they fall between\n",
        "     -1.0 and 1.0.\n",
        "  '''\n",
        "  h = np.percentile(attrs, ptile)\n",
        "  l = np.percentile(attrs, 100-ptile)\n",
        "  return np.clip(attrs/max(abs(h), abs(l)), -1.0, 1.0)    \n",
        "\n",
        "def pil_text(strs, shape, start_h=10, start_w=10, font_size=18, color=(0, 0, 0)):\n",
        "  # Returns a PIL image with the provided text.\n",
        "  img = pil_img(255*np.ones(shape))\n",
        "  draw = PIL.ImageDraw.Draw(img)\n",
        "  font = PIL.ImageFont.truetype(FONT_PATH, font_size)\n",
        "  h = start_h\n",
        "  for s in strs: \n",
        "    draw.text((start_w,h), s, fill=color, font=font)\n",
        "    h = h + 30\n",
        "  return img\n",
        "\n",
        "def combine(imgs, horizontal=True):\n",
        "  # Combines the provided PIL Images horizontally or veritically\n",
        "  if horizontal:\n",
        "    w = np.sum([img.size[0]+10 for img in imgs])\n",
        "    h = np.max([img.size[1] for img in imgs])\n",
        "  else:\n",
        "    w = np.max([img.size[0] for img in imgs])\n",
        "    h = np.sum([img.size[1]+10 for img in imgs])\n",
        "  final_img = PIL.Image.new('RGB', (w, h), color='white')\n",
        "  pos = 0\n",
        "  for img in imgs:\n",
        "    if horizontal:\n",
        "      final_img.paste(im=img, box=(pos,0))\n",
        "      pos = pos+img.size[0]+10\n",
        "    else:\n",
        "      final_img.paste(im=img, box=(0,pos))\n",
        "      pos = pos+img.size[1]+10\n",
        "  return final_img\n",
        "\n",
        "def visualize_attrs(img, attrs, ptile=99):\n",
        "  '''Visaualizes the provided attributions by first aggregating them\n",
        "    along the color channel to obtain per-pixel attributions and then\n",
        "    scaling the intensities of the pixels in the original image in\n",
        "    proportion to absolute value of these attributions.\n",
        "\n",
        "    The provided image and attributions must of shape (224, 224, 3).\n",
        "  '''\n",
        "  if np.sum(attrs) == 0.0:\n",
        "    # print \"Attributions are all ZERO\"\n",
        "    return pil_img(0*img)\n",
        "  attrs = gray_scale(attrs)\n",
        "  attrs = abs(attrs)\n",
        "  attrs = np.clip(attrs/np.percentile(attrs, ptile), 0,1)\n",
        "  vis = img*attrs\n",
        "  return pil_img(vis)\n",
        "  \n",
        "  \n",
        "R=np.array([255,0,0])\n",
        "G=np.array([0,255,0])\n",
        "B=np.array([0,0,255])\n",
        "def visualize_attrs2(img, attrs, pos_ch=G, neg_ch=R, ptile=99):\n",
        "  '''Visaualizes the provided attributions by first aggregating them\n",
        "     along the color channel and then overlaying the positive attributions\n",
        "     along pos_ch, and negative attributions along neg_ch.\n",
        "\n",
        "     The provided image and attributions must of shape (224, 224, 3).\n",
        "  '''\n",
        "  if np.sum(attrs) == 0.0:\n",
        "    # print \"Attributions are all ZERO\"\n",
        "    return pil_img(0*img)\n",
        "  attrs = gray_scale(attrs)\n",
        "  attrs = normalize(attrs, ptile)   \n",
        "  pos_attrs = attrs * (attrs >= 0.0)\n",
        "  neg_attrs = -1.0 * attrs * (attrs < 0.0)\n",
        "  attrs_mask = pos_attrs*pos_ch + neg_attrs*neg_ch\n",
        "  vis = 0.3*gray_scale(img) + 0.7*attrs_mask\n",
        "  return pil_img(vis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_uhUyYiBlL7",
        "colab_type": "text"
      },
      "source": [
        "## Extracting Invariant Candidates for Tensorflow Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI8OPoEuA_RU",
        "colab_type": "code",
        "outputId": "54fc1f38-bead-4a89-d632-0b512a51aedb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "t_fc1 = sess.graph.get_tensor_by_name('local4/local4:0')\n",
        "t_conv1 = sess.graph.get_tensor_by_name('conv1/conv1:0')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-668a5bca85e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_fc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local4/local4:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mt_conv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv1/conv1:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_tensor_by_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RToI9ehA1zfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Test getting suffixes and checking accuracy\n",
        "t_fc1 = sess.graph.get_tensor_by_name('local4/local4:0')\n",
        "t_conv1 = sess.graph.get_tensor_by_name('conv1/conv1:0')\n",
        "\n",
        "T_LAYER = t_fc1\n",
        "def fingerprint_suffix(images):\n",
        "  print \"Getting fingerprint for\", T_LAYER.name\n",
        "  return (get_prediction(sess, T_LAYER, t_images_pl, images)>0.0).astype('int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJmZBO7-BVhJ",
        "colab_type": "code",
        "outputId": "f8265c95-a06d-4eb4-c2e0-2911592d1491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_suffixes = fingerprint_suffix(train_images)\n",
        "test_suffixes = fingerprint_suffix(test_images)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 6/391 [00:00<00:06, 57.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for local4/local4:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [00:04<00:00, 91.53it/s]\n",
            " 13%|█▎        | 10/79 [00:00<00:00, 98.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for local4/local4:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:00<00:00, 95.21it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxgnKvn04Q92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def describe_input(i, training=True):\n",
        "  if training:\n",
        "    print \"Input\", i\n",
        "    print \"Groundtruth\", train_labels[i]\n",
        "    print \"Prediction\", train_predictions[i]\n",
        "    print \"Fine-grained prediction\", 10*train_labels[i] + train_predictions[i]\n",
        "    show_cifar_img(train_images[i])\n",
        "  else:\n",
        "    print \"Input\", i\n",
        "    print \"Groundtruth\", test_labels[i]\n",
        "    print \"Prediction\", test_predictions[i]\n",
        "    print \"Fine-grained prediction\", 10*test_labels[i] + test_predictions[i]\n",
        "    show_cifar_img(test_images[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fcVM2dek4Qo",
        "colab_type": "text"
      },
      "source": [
        "## Build the Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_0fvX3aAWHN",
        "colab_type": "code",
        "outputId": "180e2b41-1943-4337-ccec-e94eb3f5b346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Basic decision tree\n",
        "basic_estimator = tree.DecisionTreeClassifier()\n",
        "basic_estimator.fit(train_suffixes, train_predictions)\n",
        "\n",
        "# Evaluate basic_estimator on test data\n",
        "basic_estimator_acc = (basic_estimator.predict(test_suffixes) == test_labels)\n",
        "basic_estimator_agreement = (basic_estimator.predict(test_suffixes) == test_predictions)\n",
        "print \"Estimator accuracy\", 1.0*np.sum(basic_estimator_acc)/len(test_suffixes)\n",
        "print \"Estimator agreement\", 1.0*np.sum(basic_estimator_agreement)/len(test_suffixes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimator accuracy 0.67\n",
            "Estimator agreement 0.7238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkvj66ZvX3B9",
        "colab_type": "code",
        "outputId": "f7daa30f-adab-4905-dc94-2f1cd434f1e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Fine-grained predictions decision tree\n",
        "fine_grained_predictions = 10*train_labels + train_predictions\n",
        "fine_grained_estimator = tree.DecisionTreeClassifier()\n",
        "fine_grained_estimator.fit(train_suffixes, fine_grained_predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "            max_features=None, max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
              "            splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfwDuDQtprdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decision tree per label\n",
        "def get_relative_predictions(label):\n",
        "  print \"Create relative predictions for label:%d\" % label\n",
        "  res = np.zeros(train_predictions.shape)\n",
        "  for i in range(len(train_predictions)):\n",
        "    pred = train_predictions[i]\n",
        "    gt = cifar_train_labels[i]\n",
        "    if gt == label and pred == gt:\n",
        "      res[i] = 0\n",
        "    elif gt == label and pred != gt:\n",
        "      res[i] = 1\n",
        "    else:\n",
        "      res[i] = 2\n",
        "  print \"Num correct: %d\" % np.sum(res == 0)\n",
        "  print \"Num misclassified: %d\" % np.sum(res == 1)\n",
        "  print \"Num others: %d\" % np.sum(res == 2)\n",
        "  return res\n",
        "\n",
        "def get_relative_estimator(label):\n",
        "  predictions = get_relative_predictions(label)\n",
        "  print \"Creating decision tree for label:%d\" % label\n",
        "  estimator = tree.DecisionTreeClassifier()\n",
        "  estimator.fit(train_suffixes, predictions)\n",
        "  return estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHJlFvwXzXgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SLOW; run only if you want to build relative estimators.\n",
        "relative_estimators = [None for _ in range(10)]\n",
        "for i in range(10):\n",
        "  relative_estimators[i] = get_relative_estimator(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xqkL17z28jI7"
      },
      "source": [
        "## Distillation Analysis for Paper (CAV 2019)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ry0IsBw38jI8",
        "colab": {}
      },
      "source": [
        "# Run Restore Keras Model and Jump to this place"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7d480fbf-8e7a-4670-d961-528cbec55343",
        "id": "ZoIG96P98jI-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "t_flatten = graph.get_tensor_by_name('flatten_1/Reshape:0') #Shape: 3200\n",
        "train_suffixes_flatten = fingerprint_suffix_keras(train_images, tensor=t_flatten)\n",
        "test_suffixes_flatten = fingerprint_suffix_keras(test_images, tensor=t_flatten)\n",
        "\n",
        "t_dense1 = graph.get_tensor_by_name('activation_5/Relu:0') #Shape: 256\n",
        "train_suffixes_dense1 = fingerprint_suffix_keras(train_images, tensor=t_dense1)\n",
        "test_suffixes_dense1 = fingerprint_suffix_keras(test_images, tensor=t_dense1)\n",
        "\n",
        "t_dense2 = graph.get_tensor_by_name('activation_6/Relu:0') #Shape: 256\n",
        "train_suffixes_dense2 = fingerprint_suffix_keras(train_images, tensor=t_dense2)\n",
        "test_suffixes_dense2 = fingerprint_suffix_keras(test_images, tensor=t_dense2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 8/500 [00:00<00:06, 78.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for flatten_1/Reshape:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:05<00:00, 96.36it/s] \n",
            " 11%|█         | 11/100 [00:00<00:00, 104.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for flatten_1/Reshape:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 98.49it/s]\n",
            "  2%|▏         | 8/500 [00:00<00:06, 78.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for activation_5/Relu:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:05<00:00, 97.12it/s]\n",
            " 10%|█         | 10/100 [00:00<00:00, 98.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for activation_5/Relu:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 93.84it/s]\n",
            "  2%|▏         | 11/500 [00:00<00:04, 100.37it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for activation_6/Relu:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:05<00:00, 96.45it/s]\n",
            " 11%|█         | 11/100 [00:00<00:00, 101.36it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for activation_6/Relu:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 92.99it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2675e243-bfdf-4fc4-9b08-82d4fcd4a170",
        "id": "JQWQ1DYK8jJC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "def eval_basic_estimator(estimator, suffixes, orig_model_predictions, gt_predictions):\n",
        "  estimator_predictions = estimator.predict(suffixes)\n",
        "  acc = (estimator_predictions == gt_predictions)\n",
        "  agreement = (estimator_predictions == orig_model_predictions)\n",
        "  print \"Estimator accuracy\", 1.0*np.sum(acc)/len(suffixes)\n",
        "  print \"Estimator agreement\", 1.0*np.sum(agreement)/len(suffixes)\n",
        "\n",
        "print \"Flatten\"\n",
        "basic_estimator_flatten = tree.DecisionTreeClassifier()\n",
        "basic_estimator_flatten.fit(train_suffixes_flatten, train_predictions)\n",
        "# Evaluation\n",
        "eval_basic_estimator(basic_estimator_flatten, test_suffixes_flatten, test_predictions, test_labels)\n",
        "\n",
        "print \"Dense1\"\n",
        "basic_estimator_dense1 = tree.DecisionTreeClassifier()\n",
        "basic_estimator_dense1.fit(train_suffixes_dense1, train_predictions)\n",
        "# Evaluation\n",
        "eval_basic_estimator(basic_estimator_dense1, test_suffixes_dense1, test_predictions, test_labels)\n",
        "\n",
        "print \"Dense2\"\n",
        "basic_estimator_dense2 = tree.DecisionTreeClassifier()\n",
        "basic_estimator_dense2.fit(train_suffixes_dense2, train_predictions)\n",
        "# Evaluation\n",
        "eval_basic_estimator(basic_estimator_dense2, test_suffixes_dense2, test_predictions, test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Flatten\n",
            "Estimator accuracy 0.3795\n",
            "Estimator agreement 0.3888\n",
            "Dense1\n",
            "Estimator accuracy 0.6715\n",
            "Estimator agreement 0.7251\n",
            "Dense2\n",
            "Estimator accuracy 0.7057\n",
            "Estimator agreement 0.7872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mkbRl6J98jJH",
        "colab": {}
      },
      "source": [
        "def get_basic_estimator_confidence(estimator, suffixes):\n",
        "  leaf_nodes = estimator.apply(suffixes)\n",
        "  conf_score = estimator.tree_.n_node_samples[leaf_nodes]\n",
        "  # check that the leaf is pure\n",
        "  is_pure = np.array([ len(np.where(v != 0)[0]) == 1 for v in estimator.tree_.value[leaf_nodes][:,0,:]])\n",
        "  print \"Num pure instances\", np.sum(is_pure)\n",
        "  conf_score *= is_pure\n",
        "  return conf_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ef9fQFl18jJK",
        "colab": {}
      },
      "source": [
        "def plot_bars(ax, bar_vals, bar_names, ptiles, baseline_val=None, baseline_label=None, width=0.2):\n",
        "  ax.set_ylabel(\"Agreement\")\n",
        "  ax.set_xlabel(\"Proportion of Examples\")\n",
        "  #ax.set_ylim((min([min(b) for b in bar_vals])-0.05, 1.1))\n",
        "  # ax.set_title(title)\n",
        "  # TODO: why 'plt' below and not 'ax'?\n",
        "  ind = np.arange(len(ptiles))\n",
        "  ax.set_xticks(ind)\n",
        "  ax.set_xticklabels([\"%d%%\"%(100-ptile) for ptile in ptiles])\n",
        "  if baseline_val != None:\n",
        "    ax.axhline(y=baseline_val, color='b', linestyle='-', label=baseline_label)\n",
        "  \n",
        "  colors = ['red', 'blue', 'yellow', 'green']\n",
        "  bars = []\n",
        "  for i, b in enumerate(bar_vals):\n",
        "    assert len(b) == len(ptiles)\n",
        "    bars.append(ax.bar(ind+i*width, b, width=width, color=colors[i]))\n",
        "  ax.legend(bars, bar_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "382dfbe3-d2c4-484e-e61a-fffdcf5a1a30",
        "id": "2QlTO9Lk8jJN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_confidence_plot(conf_scores, conf_score_names, all_estimator_predictions, orig_model_predictions,  gt_predictions, ptiles):\n",
        "  def get_acc_trend(score, agreement, ptiles):\n",
        "    acc_trend = []\n",
        "    for ptile in ptiles:\n",
        "      above_ptile = (score >= np.percentile(score, ptile))\n",
        "      # print np.sum(agreement*above_ptile), np.sum(above_ptile)\n",
        "      acc_trend.append(1.0*np.sum(agreement*above_ptile)/np.sum(above_ptile))\n",
        "    return acc_trend   \n",
        "  orig_correct = (orig_model_predictions == gt_predictions)\n",
        "  acc_trends = []\n",
        "  for i, score in enumerate(conf_scores):\n",
        "    estimator_predictions = all_estimator_predictions[i] \n",
        "    agreement = (orig_model_predictions == estimator_predictions)\n",
        "    acc_trends.append(get_acc_trend(score, agreement, ptiles))\n",
        "  fig, ax = plt.subplots(1, 1, sharey=True, figsize=(10,7))\n",
        "  plot_bars(\n",
        "      ax,\n",
        "      acc_trends,\n",
        "      conf_score_names,\n",
        "      ptiles,\n",
        "  )\n",
        "  plt.show(fig)\n",
        "\n",
        "basic_estimator_flatten_predictions = basic_estimator_flatten.predict(test_suffixes_flatten)\n",
        "conf_score_flatten = get_basic_estimator_confidence(basic_estimator_flatten, test_suffixes_flatten)\n",
        "basic_estimator_dense1_predictions = basic_estimator_dense1.predict(test_suffixes_dense1)\n",
        "conf_score_dense1 = get_basic_estimator_confidence(basic_estimator_dense1, test_suffixes_dense1)\n",
        "basic_estimator_dense2_predictions = basic_estimator_dense2.predict(test_suffixes_dense2)\n",
        "conf_score_dense2 = get_basic_estimator_confidence(basic_estimator_dense2, test_suffixes_dense2)\n",
        "prediction_score = test_predictions_all.max(axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num pure instances 10000\n",
            "Num pure instances 10000\n",
            "Num pure instances 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "70806c0f-08fe-4fd2-f519-2644c10483f0",
        "id": "sRJ20ilG8jJR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "show_confidence_plot(\n",
        "  [conf_score_flatten, conf_score_dense1, conf_score_dense2],\n",
        "  ['Conf at Layer 4', 'Conf at Layer 5', 'Conf at Layer 6'],\n",
        "  [basic_estimator_flatten_predictions, basic_estimator_dense2_predictions, basic_estimator_dense1_predictions],\n",
        "  test_predictions,\n",
        "  test_labels,\n",
        "  10*np.arange(10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAGtCAYAAAC4HmhdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkEmSJJXs/wejkwdMQ71GMgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-yR4qbD44Qr",
        "colab_type": "text"
      },
      "source": [
        "## Confidence Modeling Results for Paper (CAV 2019)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q6Sl2xVcAcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run Restore Keras Model and Jump to this place"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr9e_aqFbnbi",
        "colab_type": "code",
        "outputId": "caf7a8b9-3bf4-4f67-fd7b-f3f917fc8c92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "t_flatten = graph.get_tensor_by_name('flatten_1/Reshape:0') #Shape: 3200\n",
        "train_suffixes_flatten = fingerprint_suffix_keras(train_images, tensor=t_flatten)\n",
        "test_suffixes_flatten = fingerprint_suffix_keras(test_images, tensor=t_flatten)\n",
        "\n",
        "t_dense1 = graph.get_tensor_by_name('activation_5/Relu:0') #Shape: 256\n",
        "train_suffixes_dense1 = fingerprint_suffix_keras(train_images, tensor=t_dense1)\n",
        "test_suffixes_dense1 = fingerprint_suffix_keras(test_images, tensor=t_dense1)\n",
        "\n",
        "t_dense2 = graph.get_tensor_by_name('activation_6/Relu:0') #Shape: 256\n",
        "train_suffixes_dense2 = fingerprint_suffix_keras(train_images, tensor=t_dense2)\n",
        "test_suffixes_dense2 = fingerprint_suffix_keras(test_images, tensor=t_dense2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 8/500 [00:00<00:06, 78.76it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for flatten_1/Reshape:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:05<00:00, 96.51it/s] \n",
            " 11%|█         | 11/100 [00:00<00:00, 104.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for flatten_1/Reshape:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 98.27it/s]\n",
            "  2%|▏         | 10/500 [00:00<00:05, 96.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for activation_5/Relu:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:05<00:00, 96.51it/s]\n",
            " 11%|█         | 11/100 [00:00<00:00, 101.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for activation_5/Relu:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 93.81it/s]\n",
            "  2%|▏         | 10/500 [00:00<00:05, 96.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for activation_6/Relu:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:05<00:00, 97.42it/s]\n",
            " 10%|█         | 10/100 [00:00<00:00, 95.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting fingerprint for activation_6/Relu:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 93.32it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsYrWTmQch8K",
        "colab_type": "code",
        "outputId": "d65e3e9c-a454-4903-bd2d-4f89af8cefbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "def eval_basic_estimator(estimator, suffixes, orig_model_predictions, gt_predictions):\n",
        "  estimator_predictions = estimator.predict(suffixes)\n",
        "  acc = (estimator_predictions == gt_predictions)\n",
        "  agreement = (estimator_predictions == orig_model_predictions)\n",
        "  print \"Estimator accuracy\", 1.0*np.sum(acc)/len(suffixes)\n",
        "  print \"Estimator agreement\", 1.0*np.sum(agreement)/len(suffixes)\n",
        "\n",
        "print \"Flatten\"\n",
        "basic_estimator_flatten = tree.DecisionTreeClassifier()\n",
        "basic_estimator_flatten.fit(train_suffixes_flatten, train_predictions)\n",
        "# Evaluation\n",
        "eval_basic_estimator(basic_estimator_flatten, test_suffixes_flatten, test_predictions, test_labels)\n",
        "\n",
        "print \"Dense1\"\n",
        "basic_estimator_dense1 = tree.DecisionTreeClassifier()\n",
        "basic_estimator_dense1.fit(train_suffixes_dense1, train_predictions)\n",
        "# Evaluation\n",
        "eval_basic_estimator(basic_estimator_dense1, test_suffixes_dense1, test_predictions, test_labels)\n",
        "\n",
        "print \"Dense2\"\n",
        "basic_estimator_dense2 = tree.DecisionTreeClassifier()\n",
        "basic_estimator_dense2.fit(train_suffixes_dense2, train_predictions)\n",
        "# Evaluation\n",
        "eval_basic_estimator(basic_estimator_dense2, test_suffixes_dense2, test_predictions, test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Flatten\n",
            "Estimator accuracy 0.3823\n",
            "Estimator agreement 0.3918\n",
            "Dense1\n",
            "Estimator accuracy 0.6709\n",
            "Estimator agreement 0.7234\n",
            "Dense2\n",
            "Estimator accuracy 0.7051\n",
            "Estimator agreement 0.7825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emDaCsEcL3-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_basic_estimator_confidence(estimator, suffixes):\n",
        "  leaf_nodes = estimator.apply(suffixes)\n",
        "  conf_score = estimator.tree_.n_node_samples[leaf_nodes]\n",
        "  # check that the leaf is pure\n",
        "  is_pure = np.array([ len(np.where(v != 0)[0]) == 1 for v in estimator.tree_.value[leaf_nodes][:,0,:]])\n",
        "  print \"Num pure instances\", np.sum(is_pure)\n",
        "  conf_score *= is_pure\n",
        "  return conf_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDKkdr1IUn-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_bars(ax, bar_vals, bar_names, ptiles, baseline_val=None, baseline_label=None, width=0.2):\n",
        "  ax.set_ylabel(\"Accuracy\")\n",
        "  ax.set_xlabel(\"Proportion of Examples\")\n",
        "  #ax.set_ylim((min([min(b) for b in bar_vals])-0.05, 1.1))\n",
        "  # ax.set_title(title)\n",
        "  # TODO: why 'plt' below and not 'ax'?\n",
        "  ind = np.arange(len(ptiles))\n",
        "  ax.set_xticks(ind)\n",
        "  ax.set_xticklabels([\"%d%%\"%(100-ptile) for ptile in ptiles])\n",
        "  if baseline_val != None:\n",
        "    ax.axhline(y=baseline_val, color='b', linestyle='-', label=baseline_label)\n",
        "  \n",
        "  colors = ['red', 'blue', 'yellow', 'green']\n",
        "  bars = []\n",
        "  for i, b in enumerate(bar_vals):\n",
        "    assert len(b) == len(ptiles)\n",
        "    bars.append(ax.bar(ind+i*width, b, width=width, color=colors[i]))\n",
        "  ax.legend(bars, bar_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwC8TwssL5EU",
        "colab_type": "code",
        "outputId": "e04d8a24-1a0b-4c60-cc79-3593c46b6614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_confidence_plot(conf_scores, conf_score_names, orig_model_predictions, gt_predictions, ptiles):\n",
        "  def get_acc_trend(score, ptiles):\n",
        "    acc_trend = []\n",
        "    for ptile in ptiles:\n",
        "      above_ptile = (score >= np.percentile(score, ptile))\n",
        "      acc_trend.append(1.0*np.sum(orig_correct*above_ptile)/np.sum(above_ptile))\n",
        "    return acc_trend   \n",
        "  orig_correct = (orig_model_predictions == gt_predictions)\n",
        "  acc_trends = []\n",
        "  for score in conf_scores:\n",
        "    acc_trends.append(get_acc_trend(score, ptiles))\n",
        "  fig, ax = plt.subplots(1, 1, sharey=True, figsize=(10,7))\n",
        "  plot_bars(\n",
        "      ax,\n",
        "      acc_trends,\n",
        "      conf_score_names,\n",
        "      ptiles,\n",
        "  )\n",
        "  plt.show(fig)\n",
        "\n",
        "conf_score_flatten = get_basic_estimator_confidence(basic_estimator_flatten, test_suffixes_flatten)\n",
        "conf_score_dense1 = get_basic_estimator_confidence(basic_estimator_dense1, test_suffixes_dense1)\n",
        "conf_score_dense2 = get_basic_estimator_confidence(basic_estimator_dense2, test_suffixes_dense2)\n",
        "prediction_score = test_predictions_all.max(axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num pure instances 10000\n",
            "Num pure instances 10000\n",
            "Num pure instances 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd8geETFMGC1",
        "colab_type": "code",
        "outputId": "cb803698-69a8-4f9c-fbd5-77ae44aa1143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "show_confidence_plot(\n",
        "  [conf_score_flatten, conf_score_dense1, conf_score_dense2, prediction_score],\n",
        "  ['Conf at Layer 4', 'Conf at Layer 5', 'Conf at Layer 6', 'Prediction probability'],\n",
        "  test_predictions,\n",
        "  test_labels,\n",
        "  10*np.arange(10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAGtCAYAAAC4HmhdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGcgkSZJyZiCTJEnKmYFMkiQpZ/8/b8EkCbNbxbkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK1Pr1agnsoi",
        "colab_type": "text"
      },
      "source": [
        "## Examine clusters/invariants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbOQkQU0zBz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_decision_path(estimator, inp):\n",
        "  # Extract the decision path taken by an input as an ordered list of indices\n",
        "  # of the neurons that were evaluated.\n",
        "  # See: http://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
        "  n_nodes = estimator.tree_.node_count\n",
        "  feature = estimator.tree_.feature\n",
        "\n",
        "  # First let's retrieve the decision path of each sample. The decision_path\n",
        "  # method allows to retrieve the node indicator functions. A non zero element of\n",
        "  # indicator matrix at the position (i, j) indicates that the sample i goes\n",
        "  # through the node j.\n",
        "  X_test = [inp]\n",
        "  node_indicator = estimator.decision_path(X_test)\n",
        "  # Similarly, we can also have the leaves ids reached by each sample.\n",
        "  leaf_id = estimator.apply(X_test)\n",
        "  # Now, it's possible to get the tests that were used to predict a sample or\n",
        "  # a group of samples. First, let's make it for the sample.\n",
        "  node_index = node_indicator.indices[node_indicator.indptr[0]:\n",
        "                                      node_indicator.indptr[1]]\n",
        "  neuron_ids = []\n",
        "  for node_id in node_index:\n",
        "    if leaf_id[0] == node_id:\n",
        "        continue\n",
        "    neuron_ids.append(feature[node_id])\n",
        "  return neuron_ids\n",
        "\n",
        "def get_suffix_cluster(neuron_ids, neuron_sig):\n",
        "  # Get the cluster of inputs that such that all inputs in the cluster\n",
        "  # have provided on/off signature for the provided neurons.\n",
        "  #\n",
        "  # The returned cluster is an array of indices (into cifar_train_images).\n",
        "  return np.where((train_suffixes[:, neuron_ids] == neuron_sig).all(axis=1))[0]\n",
        "\n",
        "def is_consistent_cluster(cluster, predictions):\n",
        "  # Check if all inputs within the cluster have the same prediction.\n",
        "  # 'cluster' is an array of input ids.\n",
        "  pred = predictions[cluster[0]]\n",
        "  for i in cluster:\n",
        "    if predictions[i] != pred:\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "def is_misclassified(i):\n",
        "  return train_predictions[i] != train_labels[i]\n",
        "\n",
        "def visualize_conductances(img, label, neuron_ids, only_on=False):\n",
        "  # Visualize the conductances for the provided image.\n",
        "  # Args:\n",
        "  # - img: the provided cifar image\n",
        "  # - label: prediction label w.r.t. conductance must be computed\n",
        "  # - neuron_ids: list of neurons indices from the suffix tensor for which\n",
        "  #    conductances must be computed.\n",
        "  # - only_on: If True then conductance is computed only for those neurons\n",
        "  #    that are on for the given image. \n",
        "  vis = [cifar_to_pil_img(img)]\n",
        "  suffix = fingerprint_suffix([img])\n",
        "  for i, id in enumerate(neuron_ids):\n",
        "    if only_on and suffix[i] != 1:\n",
        "      continue  \n",
        "    igc = conductance(img, label, neuron_id=id)\n",
        "    # igc = conductances[id]\n",
        "    vis.append(visualize_attrs2(255*cifar_to_rgb(img), cifar_to_rgb(igc)))\n",
        "  return combine(vis)\n",
        "\n",
        "def get_invariant(estimator, ref_id):\n",
        "  # Returns an invariant found w.r.t. the provided reference input\n",
        "  # Args\n",
        "  #  - ref_id: Index (into cifar_train_images) of the reference input\n",
        "  # Returns:\n",
        "  #  - cluster: Indices of training inputs that satisfy the invariant\n",
        "  #  - neuron_id: A list of neurons such that all inputs that agree with\n",
        "  #    the reference input on the on/off status of these neurons have the\n",
        "  #    same prediction as the reference input.\n",
        "  ref_img = train_images[ref_id]\n",
        "  ref_suffix = train_suffixes[ref_id]\n",
        "  neuron_ids = get_decision_path(estimator, ref_suffix)\n",
        "  neuron_sig = ref_suffix[neuron_ids]\n",
        "  cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "  return cluster, neuron_ids, neuron_sig\n",
        "\n",
        "def get_invariant_generic(estimator, images, suffixes, ref_id):\n",
        "  # Returns an invariant found w.r.t. the provided reference input\n",
        "  # Args\n",
        "  #  - estimator: estimator to use\n",
        "  #  - images: images to pull from\n",
        "  #  - suffixes: corresponding suffixes\n",
        "  #  - ref_id: Index (into data) of the reference input\n",
        "  # Returns:\n",
        "  #  - cluster: Indices of training inputs that satisfy the invariant\n",
        "  #  - neuron_id: A list of neurons such that all inputs that agree with\n",
        "  #    the reference input on the on/off status of these neurons have the\n",
        "  #    same prediction as the reference input.\n",
        "  ref_img = images[ref_id]\n",
        "  ref_suffix = suffixes[ref_id]\n",
        "  neuron_ids = get_decision_path(estimator, ref_suffix)\n",
        "  neuron_sig = ref_suffix[neuron_ids]\n",
        "  cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "  return cluster, neuron_ids, neuron_sig\n",
        "\n",
        "def get_all_invariants(estimator):\n",
        "  # Returns a dictionary mapping each decision tree prediction class\n",
        "  # to a list of invariants. Each invariant is specified as a triple:\n",
        "  # - neuron ids\n",
        "  # - neuron signature (for the neuron ids)\n",
        "  # - number of training samples that hit it\n",
        "  # The neuron ids and neuron signature can be supplied to get_suffix_cluster\n",
        "  # to obtain the cluster of training instances that hit the invariant.\n",
        "  def is_leaf(node):\n",
        "    return estimator.tree_.children_left[node] == estimator.tree_.children_right[node]\n",
        "\n",
        "  def left_child(node):\n",
        "    return estimator.tree_.children_left[node]\n",
        "\n",
        "  def right_child(node):\n",
        "    return estimator.tree_.children_right[node]\n",
        "  \n",
        "  def get_all_paths_rec(node):\n",
        "    # Returns a list of triples corresponding to paths\n",
        "    # in the decision tree. Each triple consists of\n",
        "    # - neurons encountered along the path\n",
        "    # - signature along the path\n",
        "    # - prediction class at the leaf\n",
        "    # - number of training samples that hit the path\n",
        "    # The prediction class and number of training samples\n",
        "    # are set to -1 when the leaf is \"impure\".\n",
        "    feature = estimator.tree_.feature\n",
        "    if is_leaf(node):\n",
        "      values = estimator.tree_.value[node][0]\n",
        "      if len(np.where(values != 0)[0]) == 1:\n",
        "        cl = estimator.classes_[np.where(values != 0)[0][0]]\n",
        "        nsamples = estimator.tree_.n_node_samples[node]\n",
        "      else:\n",
        "        # impure node\n",
        "        cl = -1\n",
        "        nsamples = -1\n",
        "      return [[[], [], cl, nsamples]]\n",
        "    # If it is not a leaf both left and right childs must exist\n",
        "    paths = [[[feature[node]] + p[0], [0] + p[1], p[2], p[3]] for p in get_all_paths_rec(left_child(node))]\n",
        "    paths += [[[feature[node]] + p[0], [1] + p[1], p[2], p[3]] for p in get_all_paths_rec(right_child(node))]\n",
        "    return paths\n",
        "  paths =  get_all_paths_rec(0)\n",
        "  print \"Obtained all paths\"\n",
        "  invariants = {}\n",
        "  for p in tqdm(paths):\n",
        "    neuron_ids, neuron_sig, cl, nsamples = p\n",
        "    if cl not in invariants:\n",
        "      invariants[cl] = []\n",
        "    # cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "    invariants[cl].append([neuron_ids, neuron_sig, nsamples])\n",
        "  for cl in invariants.keys():\n",
        "    invariants[cl] = sorted(invariants[cl], key=operator.itemgetter(2), reverse=True)\n",
        "  return invariants\n",
        "\n",
        "def describe_cluster(cluster, neuron_ids):\n",
        "  neuron_sig = train_suffixes[cluster[0]][neuron_ids]\n",
        "  print \"Num neurons in invariant\", len(neuron_ids)\n",
        "  print \"Neuron id and signature\", zip(neuron_ids, neuron_sig)\n",
        "  print \"Cluster size: \", len(cluster)\n",
        "  print \"Num misclassified\", len([i for i in cluster if is_misclassified(i)])\n",
        "\n",
        "def describe_all_invariants(all_invariants):\n",
        "  df = []\n",
        "  for cl, invs in all_invariants.iteritems():\n",
        "    # Note the number of invariants, and size of the largest invariant cluster\n",
        "    df.append([cl, sum([inv[2] for inv in invs]), len(invs), len([inv for inv in invs if inv[2]>=10]), invs[0][2]])\n",
        "  df = pd.DataFrame(df, columns=['Prediction Class', 'Num Instances', 'Num Invariants', 'Num Invariants with cluster size >= 10', 'Size of largest invariant cluster'])\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOUniL-T5-Sa",
        "colab_type": "code",
        "outputId": "160d41aa-2171-4f0d-9264-949ff6ddfa57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# Examine cluster/invariants containing a given reference input\n",
        "# ref_id is the index of the reference input\n",
        "ref_id =  2\n",
        "print \"### Reference Image ###\"\n",
        "describe_input(ref_id)\n",
        "print \"### Cluster ###\"\n",
        "cluster, neuron_ids, neuron_sig = get_invariant(basic_estimator, ref_id)\n",
        "describe_cluster(cluster, neuron_ids)\n",
        "\n",
        "# Visualize  10 inputs in the cluster\n",
        "for i in cluster[:25]:\n",
        "  describe_input(i)\n",
        "  # show_img(visualize_conductances(mnist.train.images[i], train_predictions[i], neuron_ids, only_on=False))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### Reference Image ###\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d0b16d709459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mref_id\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"### Reference Image ###\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdescribe_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"### Cluster ###\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_invariant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'describe_input' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDPikrr-zyHr",
        "colab_type": "code",
        "outputId": "ec903d6f-d9c2-498d-847a-a5a8076af053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "b_all_invariants = get_all_invariants(basic_estimator)\n",
        "df = describe_all_invariants(b_all_invariants)\n",
        "print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "print df.to_string(index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3379/3379 [00:00<00:00, 471601.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Obtained all paths\n",
            "Total num invariants: 3379\n",
            "Total num invariants with cluster size >= 10: 459\n",
            "Prediction Class  Num Instances  Num Invariants  Num Invariants with cluster size >= 10  Size of largest invariant cluster\n",
            "               0           5055             349                                      46                               1929\n",
            "               1           5086             232                                      27                               3558\n",
            "               2           5010             480                                      63                                873\n",
            "               3           5081             473                                      68                               1298\n",
            "               4           5032             366                                      50                               2056\n",
            "               5           4956             355                                      52                               1792\n",
            "               6           4994             282                                      42                               2205\n",
            "               7           4966             281                                      36                               2785\n",
            "               8           4928             240                                      36                               2410\n",
            "               9           4892             321                                      39                               1916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isr62nPPlROL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all fine_grained_estimator invariants\n",
        "fge_all_invariants = get_all_invariants(fine_grained_estimator)\n",
        "# Print invariant stats\n",
        "df = describe_all_invariants(fge_all_invariants)\n",
        "print \"Total num invariants:\", df['Num Invariants'].sum()\n",
        "print \"Total num invariants with cluster size >= 10:\", df['Num Invariants with cluster size >= 10'].sum()\n",
        "print df.to_string(index=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU7SHWilyZjc",
        "colab_type": "text"
      },
      "source": [
        "### Analyzing clusters of misclassified inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GPZ8nMyYCMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Examine the cluster for a misclasification (Groundtruth: 4, Prediction: 49)\n",
        "invs = b_all_invariants[0]\n",
        "neuron_ids, neuron_sig, _ = invs[0]\n",
        "cluster = get_suffix_cluster(neuron_ids, neuron_sig)\n",
        "describe_cluster(cluster, neuron_ids)\n",
        "\n",
        "# Visualize  10 inputs in the cluster\n",
        "for i in cluster[:10]:\n",
        "  describe_input(i)\n",
        "  # show_img(visualize_conductances(mnist.train.images[i], train_predictions[i], neuron_ids, only_on=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGVZuKOqbHSS",
        "colab_type": "text"
      },
      "source": [
        "## Confidence Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h11oK-pcQMEi",
        "colab_type": "code",
        "outputId": "04fada6c-0876-440e-8652-f5e4f5119b13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "def get_estimator_confidence(estimator, suffixes, estimator_predictions, estimator_conf_labels, min_samples):\n",
        "  estimator_leaf_nodes = estimator.apply(suffixes)\n",
        "  # check that the leaf has valid label. (For instance, for a fine grained estimator\n",
        "  # only labels 00, 11, .. are valid.)\n",
        "  estimator_conf = [p in estimator_conf_labels for p in estimator_predictions]\n",
        "  # check that the leaf has a minimum number of samples in its support\n",
        "  estimator_conf *= estimator.tree_.n_node_samples[estimator_leaf_nodes] >= min_samples\n",
        "  # check that the leaf is pure\n",
        "  estimator_pure = np.array([ len(np.where(v != 0)[0]) == 1 for v in estimator.tree_.value[estimator_leaf_nodes][:,0,:]])\n",
        "  estimator_conf *= estimator_pure\n",
        "  return estimator_conf\n",
        "\n",
        "def get_estimator_confidence_by_leaf_visits(\n",
        "    estimator, suffixes, estimator_predictions,\n",
        "    estimator_leaf_nodes, estimator_conf_labels):\n",
        "  # The confidence for each input is the number of samples that visit the same\n",
        "  # leaf as the input. The exception is inputs that reach impure leaves or leaves\n",
        "  # with labels outside estimator_conf_labels. For them the confidence is 0. \n",
        "  #\n",
        "  # Compute number of samples that visit the leaves\n",
        "  estimator_conf = estimator.tree_.n_node_samples[estimator_leaf_nodes]\n",
        "  # check that the leaf has valid label. (For instance, for a fine grained estimator\n",
        "  # only labels 00, 11, .. are valid.)\n",
        "  estimator_conf *= np.array([p in estimator_conf_labels for p in estimator_predictions])\n",
        "  # check that the leaf is pure\n",
        "  estimator_conf *= (estimator.tree_.impurity[estimator_leaf_nodes] == 0.0)\n",
        "  return estimator_conf\n",
        "\n",
        "def get_confident_accuracy(estimator, suffixes, orig_model_predictions, gt_labels, estimator_conf_labels, min_samples=10):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    estimator: the estimator used for confidence modeling\n",
        "    suffixes: the suffixes of the examples over which we evaluate. Typically this\n",
        "      would be 'test_suffixes'.\n",
        "    orig_model_predictions: predictions of the original model for these examples. \n",
        "      Typically this would be 'test_predictions'.\n",
        "    gt_labels: groundtruth labels for these examples. Typically this would be\n",
        "      mnist.test.labels.\n",
        "    estimator_conf_labels: prediction labels for the estimator which must be used\n",
        "      for confidence labels. If the estimator predicts a label outside this set then\n",
        "      the prediction on the example is not considered \"confident\". For the basic_estimator\n",
        "      all labels are fine for confidence modeling. For the fine_grained estimator\n",
        "      we would want to only use the \"pure\" labels for confident modeling. For\n",
        "      instance, 00, 11, ... , for the MNIST fine_grained_estimator.\n",
        "    min_samples: minimum number of samples for an estimator leaf for it to be\n",
        "      considered confident\n",
        "  \"\"\"\n",
        "  estimator_predictions = estimator.predict(suffixes)\n",
        "\n",
        "  # The following are all binary vectors of shape <len(suffixes)>  \n",
        "  orig_correct = (orig_model_predictions == gt_labels)\n",
        "  estimator_conf = get_estimator_confidence(estimator, suffixes, estimator_predictions, estimator_conf_labels, min_samples)\n",
        "\n",
        "  conf_frac = 1.0*np.sum(estimator_conf)/len(gt_labels)\n",
        "  overall_acc = 1.0*np.sum(orig_correct)/len(gt_labels)\n",
        "  conf_acc = 1.0*np.sum(orig_correct*estimator_conf)/np.sum(estimator_conf)\n",
        " \n",
        "  return conf_frac, overall_acc, conf_acc\n",
        "\n",
        "# BASIC_ESTIMATOR\n",
        "res = get_confident_accuracy(basic_estimator, test_suffixes, test_predictions, test_labels, np.array(range(10)), min_samples = 2500)\n",
        "conf_frac, overall_acc,  conf_acc = res\n",
        "print \"BASIC ESTIMATOR\"\n",
        "print \"Confident fraction\", conf_frac\n",
        "print \"Overall accuracy\", overall_acc\n",
        "print \"Confident accuracy\", conf_acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BASIC ESTIMATOR\n",
            "Confident fraction 0.0465\n",
            "Overall accuracy 0.7796\n",
            "Confident accuracy 0.956989247312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW9G5-54KlhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FINE_GRAINED_ESTIMATOR\n",
        "res = get_confident_accuracy(fine_grained_estimator, test_suffixes, test_predictions, test_labels, 10*np.array(range(10))+np.array(range(10)) , min_samples = 250)\n",
        "conf_frac, overall_acc,  conf_acc = res\n",
        "\n",
        "print \"\"\n",
        "print \"FINE GRAINED ESTIMATOR\"\n",
        "print \"Confident fraction\", conf_frac\n",
        "print \"Overall accuracy\", overall_acc\n",
        "print \"Confident accuracy\", conf_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0aKzL-HVvg7",
        "colab_type": "code",
        "outputId": "e5cd7d78-6807-4286-9ccb-93afcc9b4cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "basic_conf_fracs = []\n",
        "basic_overall_accs = []\n",
        "basic_conf_accs = []\n",
        "fg_conf_fracs = []\n",
        "fg_overall_accs = []\n",
        "fg_conf_accs = []\n",
        "max_samples = 50\n",
        "for i in tqdm(range(1, max_samples)):\n",
        "  res = get_confident_accuracy(basic_estimator, test_suffixes, test_predictions, test_labels, np.array(range(10)), min_samples = i)\n",
        "  conf_frac, overall_acc, conf_acc = res\n",
        "  basic_conf_fracs.append(conf_frac)\n",
        "  basic_overall_accs.append(overall_acc)\n",
        "  basic_conf_accs.append(conf_acc)\n",
        "  res = get_confident_accuracy(fine_grained_estimator, test_suffixes, test_predictions, test_labels, 10*np.array(range(10))+np.array(range(10)), min_samples = i)\n",
        "  conf_frac, overall_acc,  conf_acc = res\n",
        "  fg_conf_fracs.append(conf_frac)\n",
        "  fg_overall_accs.append(overall_acc)\n",
        "  fg_conf_accs.append(conf_acc)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "axs[0].scatter([range(1, max_samples)], basic_conf_fracs, label='Conf Frac')\n",
        "#axs[1,0].scatter([range(max_samples)], basic_overall_accs)\n",
        "axs[0].scatter([range(1, max_samples)], basic_conf_accs, label='Conf Acc')\n",
        "axs[0].set_title(\"Basic Estimator\")\n",
        "axs[0].legend()\n",
        "#axs[0,0].set_title(\"Basic Confident Fractions\")\n",
        "#axs[1,0].set_title(\"Basic Overall Accuracies\")\n",
        "#axs[1,0].set_title(\"Basic Confident Accuracies\")\n",
        "\n",
        "axs[1].scatter([range(1, max_samples)], fg_conf_fracs, label='Conf Frac')\n",
        "#axs[1,1].scatter([range(max_samples)], fg_overall_accs)\n",
        "axs[1].scatter([range(1, max_samples)], fg_conf_accs, label='Conf Acc')\n",
        "axs[1].set_title(\"Fine-Grained Estimator\")\n",
        "axs[1].legend()\n",
        "#axs[0,1].set_title(\"FG Confident Fractions\")\n",
        "#axs[1,1].set_title(\"FG Overall Accuracies\")\n",
        "#axs[1,1].set_title(\"FG Confident Accuracies\")\n",
        "\n",
        "plt.subplots_adjust(hspace=0.3)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 49/49 [00:07<00:00,  6.30it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFZCAYAAACizedRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xl4U1X6B/Bv9rakW0rKUopgtTIW\nC1UWARWsZREGdVyrQ1kVF1AcRcG64AYqP3BERhERNxStMqCgDggCKrJDASkDlbpQtjZp0tLYNmmT\n/P7oJKRpti5JbpLv53l8Hm5ultPY2/eec97zHpHVarWCiIiIBEMc7AYQERFRUwzOREREAsPgTERE\nJDAMzkRERALD4ExERCQwDM5EREQCIw12AyLVJZdcgu7du0MikcBqtSI1NRVz5sxBampqq97vo48+\nglarxcMPP+zT83ft2oUpU6agW7duzc6tX7/e7etMJhO++eYb3HTTTSgrK8OUKVPw1VdftarNzn79\n9VdUVFSgf//+7fJ+RO44Xn82KSkpWL58OSZMmIDHH38cGRkZ7fZ527Ztw5tvvgmdTgeLxYL4+HhM\nnDgRY8aMafF7Pf744xg1ahSys7PbpW0TJ07EDTfcgJtvvrnJ44sXL8YHH3yAjh07Nnk8MzMT8+fP\nd/t+jtfxxo0bsXnzZrz00kvt0tYff/wRaWlp6Nq1a7u8n6BZKSjS09OtZ86csR8vWLDAeu+99wbs\n83fu3GnNyclp8esKCwutEyZMaP8GWa3WpUuXWt944w2/vDeRI+frz5++//5761VXXWUtLCy0P7Zn\nzx7rgAEDrD/88ENA2uDJhAkTrP/+97+bPf76669b8/PzW/x+/ryOJ0+ebN2zZ49f3lto2HMWiCuv\nvBKbN2+2H3/++ed49913YTaboVarMX/+fKSkpKCsrAyPP/44NBoNTCYTxowZg3/84x9YvHgxzp49\ni7lz56K0tBSzZ89GeXk54uLi8Pzzz7e4F+Dqc/Ly8jB9+nQYDAbcddddmD9/PkaMGIEjR45g9erV\n2Lp1K2QyGfbt24eePXti2rRpWLBgAU6cOIEZM2bgjjvugMViwQsvvIDt27ejvr4eV1xxBebNm4cf\nf/wRS5cuhUwmw7lz5zB79mx8+OGH+PTTT2GxWNCzZ0/MnTsXKpUKs2fPRnx8PLZv344HHngA119/\nfXv/76AIlp2djfnz56Nz587Izc3F1KlT8fnnn6OyshJPPPEERo8eDavVijfeeAPr1q2DyWTCdddd\nhyeeeKJJT9xm0aJFmDFjBvr27Wt/rF+/fvjuu++gVCoBNPZSy8rKcPToUfz1r3/F+PHjXV4nMpkM\neXl5uPXWW3HjjTfikksuwSuvvIL3338fWq0Wd999NyZOnAgAKCgowHvvvQeTyYS+ffti3rx5iIqK\nQmlpKR555BHo9Xr06dMHZrO5Vd/T7t278dJLL8FoNMJqteKhhx6CQqFoch2np6dj7dq1eP/99zF7\n9mx06tQJ+/fvxy+//ILbb78dqamp+PDDD/Hnn3/itddeQ2ZmJrRaLWbNmoVTp07BZDIhLy8PkyZN\nwmuvvYadO3fi119/xWOPPYbrrrsOc+fOxa5duyAWizF06FA89thjkEgkyM7Oxs0334x169bhvffe\nC8meNuecBcBkMmHt2rX2YaqKigo8//zzeO+99/Dtt9+ie/fuePPNNwEA77//Pvr3749vvvkG69at\nQ2lpKcrLy5u839NPP40xY8Zg48aNuP/++/H444+3uE2uPsdiseCRRx5B3759sXLlymav+fHHHzF9\n+nR8++23KCkpwTvvvIOPP/4Yc+fOtbd/48aN2Lt3L7766iv85z//QVFREb755htkZ2dj+PDhGD9+\nPGbPno0DBw5g+fLlWLFiBdavX4+uXbti4cKF9s/asWMHVq1axcBMfqXX6yEWi7Fu3Trk5+fjtdde\nAwB8+eWXWL9+PVatWoWNGzeitLQUn3zySbPX19TUoKioCEOHDm12zhaYbb7//nu8/fbbmDhxotvr\nxJXjx4/jiy++wJtvvolXX30VZrMZe/fuxaJFi/DBBx9g8+bNUCqVWLRoEQBgwYIFGDRoEDZt2oQJ\nEyZg//79rfpuXnnlFTzxxBP45ptvsGTJEmzatKnZdezshx9+wNKlS/Hhhx/inXfegU6nw7p16zBy\n5EisWLECALBkyRJ069YN69evxwcffICFCxfizJkzePjhh9GpUyf83//9H0aPHo0PPvgAZ8+exddf\nf401a9bYvy+bsrIybNiwISQDM8A556DKy8uDRCJBRUUF1Go13njjDQBAUlIS9u3bB7lcDqDxLvvL\nL7+0n9u0aROuvPJKZGVl4dVXX23ynkajEbt27cLrr78OALjuuuswaNAgl59/5swZjBo1qsljw4YN\nw+zZs71+jisXXXQRevbsCQC44IILcNVVV0EikSA9Pd1+AzFy5Ehce+21kMlkAIDLLrsMpaWlzd5r\n69atGDlyJJKSkgAAt912G+677z77+UGDBkGhUHhtE5E7tuvPpl+/fnjxxRebPKehocE+F5uRkYHT\np08DALZs2YJbbrkFsbGxABp/Pz/88EOMGzeuyeurq6thtVqRmJhof+y+++7D77//DpPJhEsuuQRL\nliwBAPTp0wcqlQqA79cJANx444329hmNRlRUVGDz5s0YPXo0OnXqBAC48847MX36dMyaNQt79+7F\n/fffD6Bx/vjCCy90+x1t2LAB+/bta/LYQw89hNGjRyMpKQlffPEFkpKSkJaW1uTm2Z3BgwcjJiYG\nF198MSwWC6699loAQHp6Oo4cOQIAeOqpp+y9+dTUVKjVapw8eRJdunRp8l5bt27F5MmTIZVKIZVK\nMXbsWPz000/272PYsGFe2yNkDM5BtGLFCnTu3BkAsGfPHuTl5WH16tVISkrC66+/js2bN8NsNuPP\nP/+0B72JEyfCYrHgueeeQ3l5Of7+97/jwQcftL9nZWUlLBaL/Y+GSCRChw4dXH5+ly5d3CZ/efsc\nVxw/RyKRICYmxv5vi8UCANDpdHjhhRdw5MgRiEQiaLVaTJgwodl76XQ6JCcn24/j4uJQUVFhP46P\nj/fYFiJvHK8/dxx/j8Visf33uLq6GsuXL0dBQQEAwGw2Q6VSoayszP77nJmZieeffx4SiQQajcYe\nXN566y0Ajb3vVatW2T/L8Xfa1+sEgP1at91oWCwWVFdXY+PGjdi2bRsAwGq1or6+HgBQVVXVpNce\nFxfn9ucfOXIk5s6d6/LcvHnzsGTJEkyaNAlRUVF45JFHmt3sO7P9jRCJRBCLxS6/259//tneWxaL\nxdBoNPZzjnQ6XZPvLD4+Pqz+RjA4C0T//v3RtWtX7Nu3Dw0NDdi8eTM++ugjqFQqfPbZZ1i3bh0A\nQCqVYurUqZg6dSp+++033HPPPbjiiivs75OYmAiRSAS9Xg+VSgWr1YoTJ06ge/fuEIlEPrfH2+e0\n1j//+U9IpVKsW7cOcrkcjz76qMvndezYEZWVlfbjysrKZlmjRMGSnJyM7OzsZj1loPlqh6ysLGzY\nsME+F+wLX68TT+3729/+hlmzZjU7FxcXB4PBYD/W6XQtem+bjh074umnn8bTTz+Nbdu24cEHH8TV\nV1/dqvdy9Nhjj2HChAm48847IRKJ3L5nuP+N4JyzQPz222/47bffcOGFF6KiogIpKSlQqVTQ6/X4\nz3/+gz///BMA8Mwzz+Cnn34CAHTv3h0dO3ZsEnTlcjmGDBmCNWvWAGicB546dWqLArOnz5FKpTAY\nDLC2cjOziooKpKenQy6X4+jRoygsLERNTQ2AxhuC6upqAI1DUhs3boRerwcAfPrppy7n7YiC4brr\nrsOXX36J2tpaAI2/n7Zrztk//vEPLF26FD/88IP9sUOHDuGtt97CBRdc4PI1nq4TX2RnZ+Pbb7+1\nB95Nmzbh7bffBgD07dsXGzduBADs378fJ06c8Pl9berr65GXl2efrsrIyIBUKoVYLG5yHbdGRUUF\nevfuDZFIhDVr1qC2ttbt34hVq1bBbDajpqYGX375ZVj9jWDPOYgc57zkcjmee+45XHLJJUhKSsLX\nX3+N4cOHIzU1FQ8//DDuv/9+vPzyy8jNzcUzzzyDF154AVarFdnZ2Rg0aFCTeaG5c+di5syZWLly\nJeLj47FgwQKXn+9qzhkA5s+f7/ZzTp8+jQULFuDqq692mRTmzeTJkzFr1iysXr0a/fr1w6xZs/Dk\nk08iMzMT1157LWbOnIlTp07h9ddfx9SpU/H3v/8dFosFf/nLX/Dss8+2+POI/CEnJwe//PIL/va3\nvwFovIF1N/zbr18/LF68GIsWLcLcuXNhNpsRFxeHvLw83HHHHS5f4+k68UVGRgbuu+8+5OXlwWKx\nICkpCc899xyAxp7po48+ii+//BJ9+vTB4MGD3b6PqzlnoHF04NZbb7WPBojFYjz11FOIjo5uch23\nZt53xowZmDZtGhISEpCbm4s77rgDTz/9NFauXImRI0fikUcewUMPPYS8vDyUlpZizJgxEIlEGDVq\nVFgliIqsre0CERERkV9wWJuIiEhgGJyJiIgEhsGZiIhIYBiciYiIBIbBmYiISGAEs5RKo/G+Li4x\nMQZ6ve9r/QKBbfIN2+QbX9qkVscGqDWtw2u5/bBNvgnVNnm6lkOq5yyVNt/xJdjYJt+wTb4RYpv8\nQYg/J9vkG7bJN21tU0gFZyIiokjA4ExERCQwDM5EREQCw+BMREQkMAzOREREAsPgTEREJDAMzkRE\nRAIjmCIkRP5SWnoCr7++EJWVepjNFlx2WSamTXu4xe/z6quv4PDhQ1i8eCk6dFACAPbv34tnnpmN\nHj0utD9v4MDByMub2F7NJ6IIxOBMgmOsN6PKYES8UgGFrG0L+c1mM5566nE8/PBjyMq6AlarFa+9\n9n94771leOqp2S16rx07tuPddz+yB2abvn0vx4svzm9TO4mIHDE4k2CYLRYUbD6OwmINdOeMUMUp\nkJWuxh3ZF0Eibt0MzJ49u9C9ew9kZV0BABCJRHjggYcgEjW+32effYLvvvsWAHD11UMxbtxEzJ37\nLDp2VOPYsf+irOwsnnnmRezbtxsVFRrMmvUPzJ//GpRKpdvPBIBvvlmHnTu3Q6vV4Lnn5uHTTz/C\nkSNFMJlMuOmmWzB27E04e/YMXnxxDiwWCzp37oInn3y2VT8jEYUfzjmTYBRsPo5Ne0+i4pwRVgAV\n54zYtPckCjYfb/V7njjxOy6+OL3JYwpFFORyOUpLS/Gf/6zDG28swxtvLMPmzRtx6tRJAIDJZMKr\nr/4Lt92Wi/Xrv8Zdd42HSpWEBQte9xqYbcrKzuKNN5YhLi4enTt3xZIly/Hmm8vwzjtvAQDefvtN\n5Ob+HW+++Q46duyIo0f/2+qfk4hazmQ2QVNTAZPZ5PW4Jc9tD+w5kyAY680oLNa4PFdYrMUtQ9Na\nOcQtgsVicXnmv//9LzIyLoNU2ngZXHZZHxw/XgwA6NMnCwCgVnfCkSNFHj/hwIH9mD59qv141KjR\nEIsl+MtfLoVIJIJCocC5c1W4777JkEqlqKzUAwCKi49ixoxHAQAPPDCjFT8bUWQwmU2oMlYjXhEL\nuUTe5BgAzho0MJvFzc65em6VsRpKeQy++nUjDmmKoDdWIkEejxh5DGrra5sd64x6KMQKQNTYDk/P\n1RsrkahIQKY6A/cm5bbpZ2ZwJkGoMhihO2d0eU5fXYcqgxHJiTEtft8LLuiBf//7syaPmUwmnDx5\nAiKRCFar1f54fX29fbhbIjl/I+D4HFdczTl/8806SKUyAEBh4T7s378X//rX25BKpRg+/GoAgFgs\nhsXi+b2JwoUvQdP5nKcg2pKg6fxcuVgOo+X83xu9qRJ6U6Xb45Y8V2fUY+vJbYg5KMOYbte3+vti\ncCZBiFcqoIpToMJFgE6MjUK8UtGq9+3ffyDefHMRtm37AVdddQ0sFguWLFmMmJgYjB9/F/75z0Vo\naGgAABw5UoTx4yfjxx+3tuVHaaaqqhLJyZ0glUqxbdv3MJstqK+vR69el2L//j247roReOedt9Cn\nTxZGj85p188mCqSW9lK9BVhvQbS1Adbx3/6y9+QhDO9yHeQSeatez+BMgqCQSZCVrsamvSebnctK\n79jqrG2xWIyFC/+F+fPn4r33lkEmk6F//4GYNOkedOoUjxtu+BsefHAqLBYrxo69EZ07d2nrj9JM\nv34D8fHHH2D69Km4+uqhGDz4KixY8BKmTLkX8+Y9jzVrVqFTp06YNOmedv9sIn9y1cNtaS/V13Oh\nRlujQ5WxGuqYpFa9XmT1NmYXIL5s0K5Wx/r0vEBim3zjS5vOZ2troa+uQ2JsFLLSO7YpW7utbQo0\nX9rkaYN2IeC13H6C0SZvw88SpQWmamuT3rBzACYgOSYJT/T/h8ees6drmT1nEgyJWIy7ctJxy9C0\ndlvnTESetWZ+1zkYMzA3169bZquHtIEQDc7tWaSChEchk7Qq+YuIXPNlLrglw8/hEowVEgVMZhMS\nFfGIljlmXJ8/1hn1iJIoYAW8PldvrIQqKgGXdcxAXp9boKuoaXXbfArOxcXFeOCBBzBx4kSMGzeu\nybnt27fj1VdfhUQiwTXXXINp06YBAObNm4eDBw9CJBIhPz8fmZmZrW6kjdlswcpNxe1apIKIKFyZ\nLWasPv61T3PBQg64roKoY9CsN5uQ0IIAawugf+05HIb6GpdD+L5mlLs7lojb1nH0Gpxramrwwgsv\nYNCgQS7Pv/jii1i+fDk6deqEcePGYeTIkdDpdPjjjz9QUFCAkpIS5Ofno6CgoE0NBYB31xU1SRiy\nFakAgLty0t29jIgorLkLFJtLf8APp3bYnxfMYOwtwLrqlfoSRIHGeXCzwfd1zrZzABAti7a3US6R\nN0ngcj72dM75uK28Bme5XI5ly5Zh2bJlzc6VlpYiPj4eXbo0ZrgOHToUO3bsgE6nQ05O45KQtLQ0\nVFVVwWAw+FxZyRVjvRk7D59xea5tRSqIiEKTY8/Y1TyxOMBFIBUSBYxmY4t7qYDnXingOYiqlbHQ\n1Fa7POcpwAqZ1+AslUrtFZScaTQaqFQq+7FKpUJpaSn0ej0yMjKaPK7RaDwG58TEGEil7oPrGe2f\n0FTWujynr66DRC6DumMHbz+OXwgxe5Zt8g3bRKHKZDbh02NrsOvsPvtjzvPEFriujtdebL1hVwEY\naFkv1Z+90FAUkIQwX1Zr6fWeJ87N9WaoE6JRrm8eoBNjo2A21QdlGQSXX/gmmG1yt2VkSkpSi9rk\nastImxUr3kNBwcf44ov1bm9mfREOS6mofZjMJpdlKW2JXAfLDzcJxO3Fl/ndRDe9YaBpAI70ANsW\nbQrOycnJ0Gq19uOysjIkJydDJpM1eby8vBxqtbotHwWFTIIre3fB2h9/bXauLUUqSHhcDWm1ViC2\njASATZs2IC4uHnv37saVVw5uU5spcriaH9XVVeH7k9twWHvUZe3m9l5T7Kn3621+F2gajKn9tCk4\nd+vWDQaDASdPnkTnzp2xZcsWLFiwAHq9HosXL0Zubi6KioqQnJzcpvlmm8ljM1BTa3JZpIJCn/P8\nma2A/M0XjWl15mMgtowsKTkOs9mC3Nxx2LRpgz0479mzE0uXvgmxWIycnBG4/fa7XD5G4ctdcpK3\nNcWO2qNqlhhiWGDxaS4Y8H1+l/zHa3A+fPgwXnnlFZw6dQpSqRQbNmxAdnY2unXrhuHDh+PZZ5/F\no4827qwzevRo9OzZEz179kRGRgZyc3MhEokwZ86cdmmsRMIiFeFs9fGvsfXkNvuxrYA8ANyWfkOr\n3tPdlpEA7FtGLlv2IQBg6tQJuPbaxkRG25aRX3yxCuvXf40ZMx7F6tWfY8GC1xET03QN9saN65GT\nMwLDhmXj7bffgNFohFwux8KFr2DJkncRFxeHJ554FDfeeLPLx2ztofDQmt6vcwBub0O6DsR13a/x\naS6YhMFrcO7duzdWrFjh9nz//v1dLpOaOXNm21rmAYtUhB+T2YRDGtdbM/6sLcKNaaNaOcTt3y0j\nrVYrvvvuW/zzn28gLi4eGRmXYefOn5CZ2RdyuRyJiYkAgPnzX4Ner2v2GIU+d/WlHQWqZnSiPKHJ\nTYCtd+w8+sS5YOELyQphFH6qjNXQG133HHR1la0uIO/vLSN//vkgdLoKPPXULACAwVCNTZu+Rd++\nlzfbDpJbRIYHd0PTwa4vPbDzFci95G9ulyJRaGFZLRKEeEUsEhUJLs+pohLsw3Et1b//QJSVncG2\nbT8AgH3LyO++24i//OUvOHz4ZzQ0NKChoQFHjhQhPf2SFr3/xo0bcP/9D+L991fi/fdXYsWKz3Dg\nwH7IZHJYLGZoNOWwWq14/PGHIRZLmj1WXc25u1BhtpjxefFavLBzIZ7bOR9PbpuHrSe3QWfUwwpr\nUIp6iCBCUlQihnW7Cn/vdas9ENvmiRmYQxd7ziQIcokcmeqMJnPONpd1zGj1Hxl/bhnZ0NCAn376\nAXff/bH9sejoaAwefBW2bfsejz46296jzs7OQWxsrMvHKDQ450QEKhirFAk+Vc2i8MItI9uIbfKN\nb1tGNmZr/6wtgq7O/XxZINsUaOGwzjkcr2WDyYB5e15DlfFcu36mu40XVFEJuFTVC7f0GQFrjUxQ\nQ9Wh9v8uWNp6LbPnTIIhEUtwW/oNuDFtlCD+CFHkcp5XLiw/hCpT6wOzr71f5wCsjouFxui6LCWF\nNwZnEhz+EaJAs1XjMjVY25zk5VjU41JVLwxLHQJVVEKrakZT5GJwJqKI5Vz4pjXbKHqrsOWIwZd8\nxeBMRBHJ1cYRLeklJyji0Fed6bXCFlFrMDgTUUSx9ZbbsnFEgjwOT/R/GEp5YxlXBmNqbwzORBRR\nnJdEtUbf5Ex7YCbyBwZnIooIJrMJmhodDmoOt/i1zvPKN180xg8tJDqPwZmIwppj0pdzzWt3WpLk\nReQPDM5EFJZsy5Y2l/6AH07t8Ok1KkUiMtXet1Ek8jcGZyIKK849ZbGPWwgM7XElbrrgrwzGJAgM\nzkQUFtz1lC1wvWUoAIggsg9d39s/F7qKmkA0lcgrBmciCmmt7SmrFAm4v88kdIxu3L3JH/XbiVqL\nwZmIQprz0ihPPWVHmere6Kr0fRcyokBicCaikGUwGVCoOeTTc8UQwworl0NRSGBwJqKQYxvKbslu\nUUO6DsR13a/hcigKCQzORIR58+bh4MGDEIlEyM/PR2Zmpv3cpk2bsGTJEsjlcowZMwbjxo0LYksb\n+VLly1VPmfPKFCoYnIki3O7du/HHH3+goKAAJSUlyM/PR0FBAQDAYrHghRdewJo1a5CQkIB77rkH\nOTk56Ny5c1Da2pIqX+wpUyhjcCaKcDt27EBOTg4AIC0tDVVVVTAYDFAqldDr9YiLi4NKpQIAXHnl\nldi+fTtuvvnmgLaxJVW+bLtFsadMocy3NQdEFLa0Wi0SExPtxyqVChqNxv7vP//8E7///jvq6+ux\na9cuaLXagLfRNoztNTD/b7eo29JvYGCmkMaeMxE1YbVa7f8WiUR4+eWXkZ+fj9jYWHTr1s3r6xMT\nYyCVeg+ManWsT+0xNphQpPuvT88d3OMK9Exp/fIoX9sUSGyTb8KtTQzORBEuOTm5SW+4vLwcarXa\nfjxgwACsXLkSALBw4UKkpKR4fD+93nuVLbU6FhpNtU/t09RUQFujc3vescrXqK4jfH7ftrQpUNgm\n34RqmzwFbw5rE0W4IUOGYMOGDQCAoqIiJCcnQ6k8v1fx3XffjYqKCtTU1GDLli0YNGhQwNpmMptg\nMtcjQRHv8rxKkYD8AQ/jqYGPciibwgp7zkQR7vLLL0dGRgZyc3MhEokwZ84crF69GrGxsRg+fDhu\nv/12TJ48GSKRCFOnTrUnh/mTYwKY3lgJudh1tjWrfFG4YnAmIsycObPJca9evez/HjFiBEaMGBHQ\n9jivYzZajACa77PMKl8UrhiciUhQTGYTDmmKXJ7rII3GzCsesG9WQRSuOOdMRIJSZayG3ljp8pze\nWAWZWM7ATGGPwZmIBCVaqkCcwnUWqyoqAfFuzhGFk7AY1jbWm1FlMCJeqYBCxmxNolDkmARWZXS9\nmcVlHTPYa6aIENLB2WyxoGDzcRQWa6A7Z4QqToGsdDXuyL4IEjEHBYhCiafNLJKiEpkARhHFp+Dc\n0h1rdu3ahRkzZuDiiy8GAKSnp+Ppp59u98YXbD6OTXtP2o8rzhntx3flpLf75xGRf3hKAkuQx+Hx\nfg9CKVe6PE8UjrwG59bsWAM0VhV6/fXX/dZwY70ZhcUal+cKi7W4ZWgah7iJQoSnJLAqUzVqG4wM\nzhRRvI79utuxBkCTHWvEYrF9x5pAqDIYoTtndHlOX12HKoPrc0QkPPGKWCQqElyeYxIYRSKvPWet\nVouMjAz7sW3HGqVS2WTHmpSUFOzatQsDBgxASkoKjh8/jvvuuw9VVVWYPn06hgwZ4vFzWlosPzY+\nGurEaJTra5s9p2NCNNJ6JCFKHpgp9XAruO4vbJNvhNgmf5NL5MhUZ7icc2YSGEWiFkcvX3as6dGj\nB6ZPn47rr78epaWlGD9+PL799lvI5e4vsNYUy89MS2oy5+z4eHVVLQJRBj1UC64HGtvkm7YWyw9l\ntmSvn7VF0NVVsgoYRTSvwbk1O9Z06tQJo0ePBgB0794dHTt2RFlZGVJTU9u18XdkXwSgcY5ZX12H\nxNgoZKV3tD9ORMJnMptQZaxGvCIWt6XfgBvTRtmP2WOmSOU1OA8ZMgSLFy9Gbm6u2x1rXnnlFURH\nR2PLli2YNGkS1q5dC41GgylTpkCj0aCiogKdOnVq98ZLxGLclZOOW4amcZ0zUYhx3twiUZGATHVj\nT1kdkxTs5hEFldfg3Joda7KzszFz5kx89913qK+vx7PPPutxSLutFDIJkhNj/Pb+RNT+nNc164x6\n+/Ft6TcEq1lEguDTnHNLd6xRKpV466232qF5RBSOjA3u1zX/rC3CjWmjOKRNEY1ltIgo4PR1VW7X\nNevqKlFlFFaiHlGgMTgTUcAlRsVzXTORBwzORBRwCmnjumZXuK6ZiMGZiILkxguvRxdLBmCKhtUC\nwBSNLpYM3Hjh9cFuGlHQhfT8933PAAAgAElEQVSuVEQUulZt/Q2/7k0FxF0hkhlhrVfgV4sEq/Ab\nN66hiMeeMxEFXJ2p4fzGNRYJrMYYwNJYo6CwWAtjvTmIrSMKPgZnIgo4/TluXEPkCYMzEQVcYpwC\nqjiF63OxUYhXuj5HFCkYnIko4KLkUmSlq12ey0rvyDK8FPGYEEZEQcGNa4jcY3AmoqDgxjVE7jE4\nE1FQceMaouY450xERCQwDM5EREQCw+BMREQkMGEZnI31ZpTra1hliPzCZDZBU1MBk9nU5N/O54iI\nWiusEsLMFgsKNh9HYbEGunNGqOIUyEpX447siyARh+V9CPmByWxClbEa8YpYyCVy+7FSHoOvft2I\nQ5oi6Ix6KMQKQNT4/AR5PGLkMaitr4XeWIlERQIy1Rn4a8/hMNTX2LdAdPW+tmMiIpuwCs4Fm49j\n096T9uOKc0b7MQvpkycmswm6uip8f3IbDmuPQm+sbBZw5WI5jJbzZSUd/603VUJvqrQf64x6bD25\nDTvO7IHRbPQpkN980ZiA/sxEJFxhE5yN9ebzhfSdFBZrccvQNK6hJDuT2YSzBg1MDdYmvWFHzgHX\nMRj7ymg2Nnutu0AOAA90+nuLP4OIwk/YBOcqg/dC+lxLGTl8GZp21RsOpp+1RTA2cK6aiMIoOMcr\nGwvpV7gI0CykHxnaOjQdbLq6SujrqiBBVLCbQkRBFjbBWSGTICtd3WTO2YaF9MODr4lajtpjaDpQ\nVFEJSIyKx7la4baRiAIjbIIzwEL64ag1vWF/U0gUMJqNiJIoYP1fGxMV8YiWnW+TQiJHnbllbbqs\nYwYUUjkABmeiSBdWwZmF9IXJlnxlNjcuZ/O0nEhIvWGVIqFJwFVFJeCyjr4tj7K1/2dtESrq9B4D\nue19ma1NRDZhFZxtWEg/uFqzLlgIvWGT2QRVVAIuVfXCsNQhUEUluF2PHC2Ltr9WHZNk/7dcIrcf\n35Z+A25MG2V/LcB1zkTkm7AMztR27nq0ro4B+JQJ7Wk5UaDnhhUSBerNJiS66A07B0rHgNtSzq91\nF8iJiBwxOJOdL/O7jsfOvWEhZkJ7GpqWx4lgNohd9oaJiIKJwTmCtMf8rqcebrCDcUuHptXKWGhq\nq4PaZmrOWG9mzghFPAbnCGC2mLH6+Nf24eZgz++2lS+JWu05NE2Bwdr4ROcxOIcxW29xc+kP+OHU\nDvvjobT2ty2JWhRaWBuf6DwG5zBjMptw6txZrDn2LQ5rj0Jn1EMc5J1BGwOsEQoPy4ncLS9ibzgy\nsDY+UVMMziHOl3ljCywBbZNjb9c5+QrwbZ0ze8ORhbXxiZpicA5RzvPI/po3dp7fdezh6oxNi2t4\n6u06Jl95Wk7E3nBkYm18oqZ8Cs7z5s3DwYMHIRKJkJ+fj8zMTPu5TZs2YcmSJZDL5RgzZgzGjRvn\n9TXUeu7mkdsjMLdkftfVOmf2dqm1WBufqCmvwXn37t34448/UFBQgJKSEuTn56OgoAAAYLFY8MIL\nL2DNmjVISEjAPffcg5ycHJw4ccLta4Ih1JdmOK8/bu08shhiWGFtl/ldT8U1iFqDtfGJzvManHfs\n2IGcnBwAQFpaGqqqqmAwGKBUKqHX6xEXFweVSgUAuPLKK7F9+3aUlpa6fU0gherSDH/NIw/pOhDX\ndb+G87skSKyNT3Se1+Cs1WqRkZFhP1apVNBoNFAqlVCpVPjzzz/x+++/IyUlBbt27cKAAQM8viaQ\nQm1phr/mkZOiEu0bK0jE5//YcX6XhIi18YlakRBmtVrt/xaJRHj55ZeRn5+P2NhYdOvWzetr3ElM\njIFU6v0uWa2O9amddaYGHCqpcHnuUEkF7r0lGlHy9smH87VN7hgbTNDXVeGrY99h68lt5x9vRWCO\nlipgbDChY4wKWV0zcP3F1yIpRvW/rQiDq63fkz+wTUQkRF6jU3JyMrRarf24vLwcarXafjxgwACs\nXLkSALBw4UKkpKTAaDR6fI0ren2N18aq1bHQaHwrt1iur4FGX+vynLayFiW/V7TL3XlL2uSoveeR\n3daMNgLnjEYEe4/g1n5P/hSqbWLwJgp/XoPzkCFDsHjxYuTm5qKoqAjJyclNhqfvvvtuvPLKK4iO\njsaWLVswadIkdOnSxeNrAkFoSzMCNY8MsGY0EVGo8xqcL7/8cmRkZCA3NxcikQhz5szB6tWrERsb\ni+HDh+P222/H5MmTIRKJMHXqVKhUKqhUqmavCTShLM0I9DwyERGFPp8mXWfOnNnkuFevXvZ/jxgx\nAiNGjPD6mmAI5tKM9lyP7Gn9MRERhZ+wrhAWjKUZjj3l9pxHdrf+mKg9eCoa9PHHH2Pt2rUQi8Xo\n3bs3nnzyySC2lCgyhHVwtgnE0gx3PeX2mkfm+mPyF0+FhgwGA5YvX45vv/0WUqkUkydPxoEDB9C3\nb98gt5oovEVEcPaX9t4BivPIFAyeCg3JZDLIZDLU1NQgJiYGtbW1iI+PD1jbQr26H1FrMTi3gvPQ\ntSNfe8qcRyah8FQ0SKFQYNq0acjJyYFCocCYMWPQs2dPv7cpVKv7EbUXBudWWH386ybFQnzBeWQK\nFY5FgwwGA5YuXYr169dDqVRiwoQJOHr0aJOkUGftUVBo2Rc/u6zuFxMtxz03XebjT9JyQlxDzjb5\nJtzaxODcAiazCZoaHQ5qDrf4tZxHJqHyVGiopKQEqamp9vr5/fr1w+HDhz0G57YWFDLWm/HTwVMu\nz/108DSuH5DqlyHuUC1KE2hsk2/aWlCIwdkHnoax3XHuKXMemYTKU6GhlJQUlJSUoK6uDlFRUTh8\n+DCGDh3q1/ZUGYzQuSgeBAD66jpUGYysvU1hj8HZB60ZxnbVUyYSIm+FhqZMmYLx48dDIpEgKysL\n/fr182t7hFbdjygYGJw9aM0wNjOuKRR5KjSUm5uL3NzcgLVFKNX9iIKJwdmFlgxjiyBixjVROwtm\ndT8iIWBwdsHXYWyVIgH5wx6EpE7BgEzUjoJR3Y9ISCJuwaCx3oxyfQ2M9WaX5w0mAwo1h3x6r0x1\nb3RP6MrATOQntup+DMwUaSKm5+ytqIFtKLuw/BCqTOfcvo9tGNs2r0xERNTeIiY4F2w+7rKoAQDc\nlZPu01C2SpGA+/tMQsfoJPaWiYjIbyJiWNtYb0Zhscbluf2/nMXv+lM+ZWRnqnujq7ILAzMREflV\nRPScXRc1sECaegx/Jpbh/wq/8vj6BEUc+qozOYxNREQBERHB2VVRA2nqMci6/OH1tQnyODzR/2Eo\n5Up/NpGIfMBdqihSRERwblbUQGyGJLHMp9f2Tc5kYCYKMu5SRZEmIoIzcL6owf5fzqLSWgGxos7t\nc5mRTSQs3hI6icJNxARnwApZ96OIii6CwqiHGGKXey8zI5tIWDwldBYWa3HL0DQOcVPYiZjg7LxU\nylVgBs5nZBORMHCXKopEERGcTWYTDmmKXJ5ztbUjEQkHd6miSBQRwbnKWA29sdLlOQssGH/ReFya\nnIbYqOgAt4yIvOEuVRSJIiI4R0sViFPEosrooiynKRpvf3oGKmUVsz+JBIq7VFGkCevg7Lj1o8vA\nDKC+IhlWi4TZn0QCxl2qKNKEdRfRlgTmck9mUzTqz1yAhtJLmjxcWKx1u2MVEQUXd6miSBG2PWdP\nSWCx0lho9veHtaH5UilmfxIRUbCFbc/ZUxKYocGAhHjXPzqzP4lCh7f92YlCVdj2nOMVsUhUJLgc\n0lZFJeDint2wpeJss3PM/iQSPpbzpHAXlsHZZDahyliN3h174YdTO5qdb1zP3AsSSJn9SRSCWM6T\nwl1YBWfH7Gy9sRIJ8nikKLuitr4WemNlk0IjzP4kCk0s50mRIKyCs3OJTr2pEnpTJa7uOgjXdb8G\n8YrYZvWybdmfRBQaWM6TIkHYTM54ys4+ojvqMjC7wgQTImGzlfN0hQmdFC586jnPmzcPBw8ehEgk\nQn5+PjIzM+3nPv74Y6xduxZisRi9e/fGk08+idWrV2PRokXo3r07AGDw4MG4//77/fMT/I+n7Gxd\nXSWqjNVQxyS5fT0TTIhCA8t5UiTwGpx3796NP/74AwUFBSgpKUF+fj4KCgoAAAaDAcuXL8e3334L\nqVSKyZMn48CBAwCA0aNHY9asWf5tvQNv2dnxiliPr2eCCVHoYDlPCndeg/OOHTuQk5MDAEhLS0NV\nVRUMBgOUSiVkMhlkMhlqamoQExOD2tpaxMfH+73RrsglcmSqM5rMOdtc1jHD45A2E0yIQgsTOinc\neR2v1Wq1SExMtB+rVCpoNI2BTKFQYNq0acjJycG1116LPn36oGfPngAae9xTpkzBhAkTcOTIET81\nv6mbLxqDYd2uQlJUIkQQISkqEcO6XeV1G0hfEkyISHgcy3kyX4TCSYuzta1Wq/3fBoMBS5cuxfr1\n66FUKjFhwgQcPXoUffr0gUqlwrBhw1BYWIhZs2Zh3bp1Ht83MTEGUqn3O1+12vPw9AOd/g5jgwn6\nuiokRsVDIfWeBBYbHw11YjTK9bXNznVMiEZajyREyd1/Vd7aFAxsk2/YptDHfBEKR16Dc3JyMrRa\nrf24vLwcarUaAFBSUoLU1FSoVCoAQL9+/XD48GHceuutSEtLAwBkZWVBp9PBbDZDInEffPX6Gq+N\nVatjodFUN3vcVnTEMSNbgiicqzUC8K3Xm5mW5DLBJDMtCdVVtWj+qZ7bFExsk29CtU0M3k0xX4TC\nkdfbyiFDhmDDhg0AgKKiIiQnJ0OpVAIAUlJSUFJSgrq6OgDA4cOH0aNHDyxbtgxfffUVAKC4uBgq\nlcpjYG4ts8WMz4vX4oWdC/Hczvl4YedCfF68FmZLy4e17si+CDn9uiEpLgpiEZAUF4Wcft2YYEIk\nYN7yRTjETaHKa8/58ssvR0ZGBnJzcyESiTBnzhysXr0asbGxGD58OKZMmYLx48dDIpEgKysL/fr1\nQ7du3fDYY4/h008/RUNDA+bOneuXxjsXHdEZ9fbj29JvaNF7McGEKPSwIAmFK5/mnGfOnNnkuFev\nXvZ/5+bmIjc3t8n5zp07Y8WKFe3QPPc8FR35WVuEG9NG+VR0xBkrhhGFDltBkgoXAZoFSSiUhWy2\nhC9FR4govNkKkrjCgiQUykI2ONuKjrjiS9ERX3BpBpHwucsXuenqnrx+KWSF7MYXbSk64g2XZhCF\nDud8EWWMHF/8+CvmLN/N65dCVsgGZwD24iI/a4ugq2u6JWRbcGkGUeix5Yus3FTM65dCXkgHZ4lY\ngtvSb8CNaaOarXNuLZbyJApdvH4pXITFGI9cIoc6JqnNgRlgKU+iUMbrl8JFSAZnk9kETU0FTGZT\nu78394olCl28filchNSwtq0i2CFNEfTGSiQqEpCpbpxjlojbZ6iKe8UShS5P129mmooFhihkhFRw\nXnHw3+1WEcwT7hVLFLqcr98EpQIdomU4VFKBrYWnmb1NISFkgrPJbMKek4dcnmtLRTBXWMqTKHQ5\nX78b9pRiy/5T9vPM3qZQEDK3jVXGamhrdC7P+asimONesQCLkhCFEoVMgnilAoeOa12e58YYJGQh\n03OOV8SiY4wKmpqKZufaqyKYO56KkhCRcHFjDApVIdNzlkvk6N8t0+W5tlYE88ZWlKTinBFWnB8W\nK9h83G+fSURtx+xtClUhE5wBIK/PLRjW7SokRSVCBBGSohIxrNtVba4I5om3ogZ1pga/fTYRtQ03\nxqBQFTLD2oB/KoJ5421YTH/OGFpfIlGE4eoLCkUhGVdsFcECwdt+sYlxClRX1QakLUTUcu5WXxjr\nzaioquFqDBKkkAzOgeStKEmUXAruHE0kfLbVF2aLBSs3FTdL8Jx+e1awm0hkx+DsAw6LEYUPd7vO\nxUTLcdOQHsFrGJEDBmcfeBoWO6P9E+Z6M4fFiEKApwTPnYfP4PoBqbyWSRAYnFvA5bBYtRGqWJYD\nJAoFnhI8tZW1XPdMgsFI0gpN1j1bue6ZKFR4WvecEKtAtIL9FRIGBucW8rbumeUAiYTL07pn3Tkj\nnn9/D1ZuKobZYglwy4iaYnBuIW7mThTa7si+CDn9uiEpLqrZOY6CkVAwOLeQp2GxuA5yDosRCZwt\nwfOZif2Q6KZ8J0fBKNgYnFvI07BYpcHEYTEKSfPmzcMdd9yB3NxcHDp0fmvWsrIy5OXl2f8bNmwY\n1q1bF8SWtp9aYwMq3Yx0cRSMgo3dvFZwXPdcca6uyTnuFUuhZvfu3fjjjz9QUFCAkpIS5Ofno6Cg\nAADQqVMnrFixAgDQ0NCAvLw8ZGdnB7O57cZb9T9uikHBxJ5zKzgOi7matwI4LEahY8eOHcjJyQEA\npKWloaqqCgaDodnz1qxZg5EjR6JDhw6BbqJfeBoFy0xTocpg5DVMQcOecxvUGhugq65zeY57xVKo\n0Gq1yMjIsB+rVCpoNBoolcomz/v888/x7rvven2/xMQYSKXeC3mo1f7bg91X02/PQky0HDsPn4G2\nshZJ8VGIjZGj6Hc9th44DXVCNK7s3QWTx2ZAIglOX0YI35Mztsk3bWkTg3MbxCsVUCdEo1zffOOL\nBKUCpgYLjKweRiHGarU2e6ywsBAXXnhhs4Dtil5f4/U5anUsNBphVKW/aUgPXD8gFRK5DJ9s+C+2\n7D9lP1eur8XaH39FTa0pKNNUQvqebNgm3/jSJk/Bm8PabaCQSXBl7y4uz9UYGzBn+W48tWwnE8RI\n0JKTk6HVau3H5eXlUKubDvdu3boVgwYNCnTTAkYhkyAxToFDx7Uuz3OaigKNwbmNJo/NsK+ZFIuA\nKHljL7nOZIYVXDdJwjdkyBBs2LABAFBUVITk5ORmPeSff/4ZvXr1CkbzAkZ/zn0NA925Ovx6qooB\nmgKGw9ptJJGc3xRDo6/BolWHUGdqfgEXFmtxy9A0DnGT4Fx++eXIyMhAbm4uRCIR5syZg9WrVyM2\nNhbDhw8HAGg0GiQlBWYP9WBJjHOfvS0SAQs+PWDfXpJ19MnfGJzbiUImgVwm8Vo9jAliJEQzZ85s\ncuzcSw6Xtc2eRMmlbvdut/xvGp5LJSlQfArO8+bNw8GDByESiZCfn4/MzEz7uY8//hhr166FWCxG\n79698eSTT6K+vh6zZ8/G6dOnIZFI8NJLLyE1NdVvP4RQcN0kUWhzrGGgO1cHkeh8YHbEkTDyN6/j\nMo4FCubOnYu5c+fazxkMBixfvhwff/wxPvnkE5SUlODAgQP46quvEBcXh08++QT33XcfFi5c6Ncf\nQii4bpIotNlqGLx4z0DMzO0LF4nrAFhBjPzPa8/ZXYECpVIJmUwGmUyGmpoaxMTEoLa2FvHx8dix\nYwduuukmAMDgwYORn5/v359CQBzvvPXVdUhQKtAhWoZDJRXYWniac1ZEIUAhk+DClHiOhFHQeA3O\nngoUKBQKTJs2DTk5OVAoFBgzZgx69uwJrVYLlUoFABCLxRCJRDCZTJDL5f77SQTCdud9y9A0VBmM\n2LCntMm6Sc5ZEYUG20iYqznoS7onBKFFFElanBDmWKDAYDBg6dKlWL9+PZRKJSZMmICjR496fI07\noVRVyJm7NnU0NaDos4Muzx08rsWNwy5C56QOiJK3f15eKH1PwcQ2kSfOc9CK/y2V3HH4LI6d0HMU\njPzGa1TwVKCgpKQEqamp9l5yv379cPjwYSQnJ0Oj0aBXr16or6+H1Wr12msOtapCNp7aVK6vgcZF\n9TAA0FTW4aEFW/0yzB1q31OwhGqbGLwDx3Ek7KMNx/DT4bP2cxwFI3/yGg08FShISUlBSUkJ6uoa\n60sfPnwYPXr0wJAhQ7B+/XoAwJYtWzBw4EB/tV/QPO39DIBFSohCyNETepePs3oY+YPXnrO3AgVT\npkzB+PHjIZFIkJWVhX79+sFsNmP79u248847IZfL8fLLLwfiZxEcT3NWzvYeLcfYwT0QGxP+8/JE\noabK4L162IUp8VxaRe1GZPVlQjgAfBleDMVhSLPFgoLNx+1zVp6+7ESlAlf0avsQdyh+T8EQqm0S\n+rB2OF7Lxnoznlq202XmtlgEWK3gFFUQhWqbuPFFEDmum3xucn8keRjm1hs4xE0kRJ5qGFisnKKi\n9sfgHCAKmQTdkmPdXuCOOIdFJDx3ZF9k3+RGhMYesyv7j2lwUmPgNUxtwtraAWZbmrH3aDkqDSaX\nz2EdbiLhcczc/vVUFRZ8esDl83TVRsxZvpsFh6hN+BsTYLYL/LnJA5DopsJQglIBU4OFd95EAuRY\nPcwdDnNTWzE4B0lsjBxX9HI9xF1jbMCc5bvx1LKdWLmpGGaLJcCtIyJPPM1BO+M0FbUGh7WDyLkO\nt1wmQZ3JbN8PmkUOiITLuXqYu5UYunN10Ohr0C1Z2Fn2JCzsOQeRYyb3s5P6o0OU63sl3nkTCY+v\nKzGsABatOsRRMGoRBmcBUMgkkMskXoscMEATCY8vKzFso2ArNxajXF/Da5m84rC2QNhKfboqciAS\nAQs+PcDsTyIBsw1z7z+mga7a9Y329wdOc+tY8gl/KwSCRQ6IQpttmPvh2/vAzRJoXsvkMwZnAfG1\nyAHnoImES50Q7XGZlSMWLCF3GJwFxDHBZGZuX7irem7L/iQi4WnJMitbwRIumyRnDM4C5K3IAbM/\niYTN11EwgMPc5BqDs0B5u/vmxUwkXI6jYC/deyWGZqX49DpOWZENg7OA2e6+VbHu5684Z0UkXAqZ\nBMmJMbgr5+ImPWl3uGySbLiUSsBsd9/X9OmKOct3u6xA5KrIPhEJi+OmGRp9DRatOuTTssnpt2cF\nobUkBOw5hwBv2Z+Oc1YrNxbjjPZP3nkTCZC3giXOS63eXVcU2AaSYLDnHAJs88+2OtuefH/gNLYe\nOA1VLIscEAmVc11ukagxMDvbdvAUrsvqitgYeYBbSMHGv9ohwjn70x2LFbBamTBGJGS+L5s04tl3\n93BlRgRicA4RvhbZd8bsTyLh8mVvaL2BN9qRiME5xPhSZN+RvroOVQbXdX6JKPh8LVrClRmRhXPO\nIcrXOau4DnJEK/i/mUjIbNfz3qPlqDSYXD7H1coM5pOEL/6fDVG+FjmoNJjw/PucsyISMtv1/Nzk\nAUhU+rYyg8Pc4Y3BOcS5KnLgjHvJEoWG2Bg5rujl25QVh7nDG8c7w4Ttznvs4B54/v29qDhX1+w5\n3EuWSPicp6zcJHJzmDvMMTiHmVpjA3TVzQMzcH5O2taTBoC7ctID1TQi8oFjNbEGkRjPvr3dZTUx\noOkwN8DrOZzwNivMxCsVUCdE+/RcDosRCZdCJkGPLnE+r8zgssnwwuAcZhQyCa7s3cWn53IvWSLh\n87UAEZdNhhcOa4ehyWMzUFNr8rrMCmg6LGY2WzByQHfEKxVQyCQBbTMRuebrphmJsVGI95DpTaGF\nwTkMSSTnL+YqgxEb9pRiy/5TXl/HhDEi4XIsQOSqzv4l3ROC0CryFwbnMOa4zEoiFnnN/nROGKup\na0DeyEvYiyYSEOdsboW88frccfgsjp3Q88Y6TDA4RwBfh8WcbefFTiQ4jtfzRxuO4afDZ+3nOEUV\nPhicI4i3YTFXeLETCdfRE3qXj3OKKvT5FJznzZuHgwcPQiQSIT8/H5mZmQCAsrIyzJw50/680tJS\nPProo6ivr8eiRYvQvXt3AMDgwYNx//33+6H51Bq+1uV25Hyx33R1Txhq6hmsiYKkymCEzs0IGGsa\nhD6vwXn37t34448/UFBQgJKSEuTn56OgoAAA0KlTJ6xYsQIA0NDQgLy8PGRnZ2PDhg0YPXo0Zs2a\n5d/WU6s4Dov5mjDmfLFvO3QGRpOZd+ZEQRKvVEAVp/Bpimr/MQ2u6dMV6oRo3kyHCK/BeceOHcjJ\nyQEApKWloaqqCgaDAUqlssnz1qxZg5EjR6JDhw7+aSm1O+eEsf3HNNBV+7ZOss7UWOyAyWNEwWHb\natKXKSrHUp+ZaUnI6ZcKVVwUr1cB8xqctVotMjIy7McqlQoajaZZcP7888/x7rvv2o93796NKVOm\noKGhAbNmzcKll17ajs2m9uQpwcRXTB4jCryWTFHZahpsKTyNLYWnkcQpKkFrcUKY1dr8/3xhYSEu\nvPBCe8Du06cPVCoVhg0bhsLCQsyaNQvr1q3z+L6JiTGQSr3/YqjVsS1tst+FU5seG98fSeuKsPPw\nGWj0tRCJAV8Lh9l60XK5FDcNvQiJcQpEyc//ioXT9+RPQmwTCVNrpqhsOEUlbF6Dc3JyMrRarf24\nvLwcanXTWq9bt27FoEGD7MdpaWlIS0sDAGRlZUGn08FsNkMicR989foar41Vq2Oh0VR7fV4ghWOb\nbhrSA9cPSG3xxW6zfsfv+M/235tc7J07xYfd9+QPvrSJwZuctbSmgSPnKSqAyWNC4PX2aMiQIdiw\nYQMAoKioCMnJyc2GtH/++Wf06tXLfrxs2TJ89dVXAIDi4mKoVCqPgZmEx9U+0WIRECX3/v/RYm1a\nFnTlxmKc0f7JovxEfmbrSb94z0A8N7k/kuJaXs6TG2gIg9ee8+WXX46MjAzk5uZCJBJhzpw5WL16\nNWJjYzF8+HAAgEajQVJSkv01Y8eOxWOPPYZPP/0UDQ0NmDt3rv9+AvIr52EzZYwcX/z4a4uSx74/\ncBpbD5yGKpbJKESB0JqaBja6c3X49VQVLkyJ5zUaRCKrq0nkIPBleDFUhyEDLRBtMtabW508BsCe\njBLM+a1Q/X8n9GFtXsvtp61tMlssKNh8HIXFWlScc73PuzOxCLBa4XYOOhy/J39o67XMCmHUKgqZ\nBBNH90J0lLRFxUxsOL9F5H+OI1+6c3XYtO8kDh2vgL66DnKZxD7f7IgFTISBwZlarS2ZojYsjkDk\nfwqZBF2SOiBvxCUwXmtuMkXl7eaa12hwMDhTm7nLFPWlJ+1YHIFrLon8z3a9ArDfXP96qgoLPj3g\n8vnOBUxuH9ELaDDz+vQzBmdqN63tSTtmdjuvuWSwJvIvhUyCC1PiPZYCZQGTwGNwpnbnqiftazKK\n85pLFkgg8r+WlAIFeBGIT88AABTjSURBVH0GAoMz+Y1jTxpSCT7beAyHjlf4XBwBYIGEQHG38xwA\nnDlzBo888gjq6+tx6aWX4vnnnw9iS8lfnEuBsoBJcPEWh/zOtuYyb8QlbSqOALBAgj847jw3d+7c\nZnUJXn75ZUyePBmrVq2CRCLB6dOng9RS8icWMBEWBmcKKMfiCK2hr65DlcG34ifkG3c7zwGAxWLB\nvn37kJ2dDQCYM2cOunbtGrS2kv+15Rrl9dl+GJwpKO7IvqjFZUEBIK6DHNEKzsa0J61Wi8TERPux\nbec5ANDpdOjQoQNeeukl3HnnnVi4cGGwmkkB5niN+ipBqYCpwcLeczvgXzkKCndlQQuLtR4LJFQa\nTHj+/T1MPvEjx6KBVqsVZWVlGD9+PFJSUjB16lRs3boVw4YNc/t67jDXvoLZphl3XoE6UwO0lbVY\n9+Ov2PvfMmgra6GQS1BrbH591prMmPPubqgTonFl7y6YPDYDEklgrtFw+3/H4ExB5WrNpXOwds70\ntiWfmM0WjBzQncs42sjTznOJiYno2rUrunfvDgAYNGgQfvnlF4/BmTvMtR+htEkhAm695kKMHXQB\nJHIZTHUmlzfTtcYGAEC5vhZrf/wVNbWmgCSICeV7ctTW8p3sdpCg2IJ1jEKKu3LS8czEfkhUuk5M\n+f7AaTyxdCeeWrYTKzcVo8ZYj3J9DYfUWsjTznNSqRSpqan4/fff7ed79uwZrKZSkClkEnTp2MF+\nfb54z0A8O6k/OkS57uftPVqO6hpTgFsZHthzJkGrNTag0k2CiXMNYK65bB1vO8/l5+dj9uzZsFqt\nSE9PtyeHESlkEshlEujcFC+pNJjw7Lt7cEUvFixpKQZnErR4pcJj5SJHXHPZejNnzmxy7Lg/+wUX\nXIBPPvkk0E2iEOHtGtUbePPcGvxmSNBslYtag2suifzP12u0zmRuUqp35cZiTkN5wJ4zCZ5z5SJf\nt6bUnauDRl+DbsnCy+IkCie2a3Tv0XJUGnybY/7+wGlsLTzNnrQb/CZI8BwrF71075UYmpXi0+us\nABatOoSVm4phtlj820iiCGa7Rp+bPMBtAqczi7XppjcFm4/7t5EhhsGZQobjhhq+FjDhEBpR4MTG\nyHFFL05DtQcOa1PIcVfAZP8xDXTVrpNSXA2hEVH7c5yG8lRQyJnjNJSx3owqgzGiM7sZnClkORcw\nuaZPV8xZvtvlbjrOy66AxupHRNS+PFX/85QzYgXw2ucH0SFajpq6eujOGSN6PprBmcKGOiHa52VX\n+49p8PuZKkit1oi9MyfyJ3fV/zbsKcWW/adcvkZXbYKu+nxCWSQvi4ysWxEKay1ZdqWrNuKhBVvt\n1cWYMEbkX845I6pY37ek3H9Mg5MaQ0TNSbPnTGGlJcuuHDNFgci7MycKBtuwt6dpKGe6aiPmLN9t\nH+aOhGpjDM4UVpznuzwNoTnaf0yDa/p0hTohOmwvdiIhack0FND0Ztq52tj027P829ggYHCmsOQ4\nhCYRi+w9aXd36c535pGYgEIUSLZpKNvIVUs4l+q1QoRbh14IhUwSNpneDM4U1hx70hp9DRatOuT2\nTt3xzrymrgF5Iy8J6YubSOicl10lKBXoEC2zZ2v7MuQNAN/tLUXhsTKXmd6hOgTO4EwRQSGToFty\nrM936tsPn8WxE3r2osNIaekJvP76QlRW6mE2W3DZZZmYNu1hyOXyFr3Pq6++gsOHD2Hx4qXo0KFx\na839+/fimWdmo0ePC+3PGzhwMPLyJrbnjxB2nKehbAHUWG/2ejPtzF2mt+MQeGZaEnL6pUIVFyX4\nQM3gTBHFOWHM0505k8WCqz2HJ81mM5566nE8/PBjyMq6AlarFa+99n94771luPfeaS16rx07tuPd\ndz+yB2abvn0vx4svzm9TOyOV47Ir23FLbqY9cRwC31J4GlsKTyMpBKavGJwpojjeqTeIxHj27e1e\n78yZLBZYZosFBZuPo7BY026FKPbs2YXu3XsgK6ux8IxIJMIDDzwEkajx/T777BP88MN3qK834+qr\nh2LcuImYO/dZdOyoxrFj/0VZ2Vk888yL2LdvNyoqNJg16x+YP/81KJVKTx+Lb75Zh507t0Or1eC5\n5+bh008/wpEjRTCZTLjpplswduxNOHv2DF58cQ4sFgs6d+6CJ598FhIJf8+A1lcb88Z24202WzBy\nQHdBDnkzOFNEUsgk6Kb27c6cyWKBVbD5eJP/J+0xgnHixO+4+OKmr1UoogAAp0+fwn/+sw5ffLEG\nGk01pk6dgGuvzQEAmEwmvPrqv/DFF6uwfv3XmDHjUaxe/TkWLHgdMTExzT7HlbKys3jrrXdhMpnQ\nuXNXPPjgIzAa63D77Tdh7Nib8PbbbyI39++46qqhePPNRTh69L/IyOjdqp8z3LSmVG9LOJf1FdL8\nNIMzRTTbnbm3i90xWczxbhtAWGSGCoWx3ozCYo3Lc4XFWtwyNK2V37MIFjeFZn755RgyMi6DVCqF\nVCrFZZf1wfHjxQCAPn0al+io1Z1w5EiRx084cGA/pk+faj8eNWo0xGIJ/vKXSyESiaBQKHDuXBXu\nu28ypFIpKiv1AIDi4qOYMeNRAMADD8xoxc8W/lxVG/towzH8dPhsm97Xuayv8xKtYN6IMzhTRHO8\nM/f1Yv/+QOO8VZRcDEDU5GJ2vvN2nDcFGMi9qTIYoXMzzaCvrkOVwdhkbtJXF1zQA//+92dNHjOZ\nTDh58gQAEazW89kH9fX19uFux+Flx+e44mrO+Ztv1kEqlQEACgv3Yf/+vfjXv96GVCrF8OFXAwDE\nYjEsvmxQTnYKmQQTR/dCdJTUZaa3vtrYqiFw5yVawRz2ZnAmQvOL3VOymO3vaJ3pfE/M+c47MVZu\nX9ZRcc7oUyAnIF6pcFuYIjE2yn6T01L9+w/Em28uwrZtP+Cqq66BxWLBkiWLERMTg7/+9Ua8++7b\naGhoQENDA44cKcL48ZPx449b2/jTNFVVVYnk5E6QSqXYtu17mM0W1NfXo1evS7F//x5cd90IvPPO\nW+jTJwv9+w9s188OR4431hK5DGZTfZMbYscNNyrO1bXqMxyHvQOd6c3gTPQ/LVkT7Y7tztt5WYen\nQM5tLM/zVJgiK71jq/8oisViLFz4L8yfPxfvvbcMMpkM/fsPxKRJ90AsFuOGG/6GcePGwWRqwNix\nN6Jz5y5t/VGa6ddvID7++ANMnz4VV189FIMHX4UFC17ClCn3Yt6857FmzSp06tQJkybd0+6fHc4U\nMgnUHTtAo6m2HzsPgevO1WHTvpM4dLzCa1lfR47D3s6Z3v6+uRZZvY3VAJg3bx4OHjwIkUiE/Px8\nZGZmAgDKysowc+ZM+/NKS0vx6KOPYtSoUZg9ezZOnz4NiUSCl156CampqR4/w/bFeqJWx/r0vEBi\nm3wTim1auam4zcs4WiKnXzfMuPMKr9+TWh0boBa1Tluv5fPZ2o3DlYmxUchK7+j3+b9Q/B0NhlBu\nk61X7WtZX0+i5BKP89O+tMnTtey157x792788ccfKCgoQElJCfLz81FQUAAA6NSpE1as+P/27j+m\nyTuPA/gbig8IA6GlJW6nceNkNagxJm4TdPMHw5Pz7gyJBhc05sLmRjC7ZE4bRuJ/myAseGQ3nZOZ\nEJbp/JVNFlSc3IQVNjzngJiY+pcaJi2/BKEF6vf+4OiBFCkt7fPtfL/+e56Hlg+1Hz/P8/1ZAQAY\nHh7G9u3bsW7dOpw/fx4xMTEoKSlBXV0dSkpKUFpa6tEfTCSL6WyiMROu37LBPjjsv18QJCZbmILI\nV+6W9fV2ipa/+6enLM5msxlpaSPTChITE9HT04O+vr4J8/vOnj2LDRs2ICoqCmazGZs3bwYApKSk\nID8/3+dAiQLN2000vNXVa0fXAwf7mv7n8YUpiGbKZFO0fLkRf3xalq+bcUz5/4DNZkNycrLrWKvV\nwmq1TijOX3/9NcrLy12v0Wq1AEb6ekJCQjA4OPjEZfLi4iIRFjb13YaMTXqMyTPBHNMfABgT9YiO\nCkdDSxusXQOICNdgZJDXMMIVDQYcvi2OEB87G3Ex4YhQWJ6JAsFd/7S3N+KPT8uKnK1gc+oCr2Ob\n9v8C7rqor1+/jhdeeGHS1XI86NZGV1f/lD8TzH0dgcSYPONNTJtTF2DjS/MmTI8ae+ftbgH/cGXk\nxnNwyDlpE9rSRB0ilLCg73MmClbumr29Hend0NKGjS/N87qJe8ribDAYYLPZXMft7e3Q6/Xjfqa2\nthYrV64c9xqr1Qqj0YihoSEIIaa9uDyRrB5vbnV35/2kec6PF/KxA56ISH1jm73HjvSeTv+0rXvA\n63n5gAfFOTU1FWVlZcjKykJraysMBsOEJ+Tm5mZkZGSMe011dTVWr16NK1eu4OWXOWePng7uFvD3\ntJATkVzCZ2kwVxeF7ekvwrF24vzpJ/VPx8fO9npePuBBcV6+fDmSk5ORlZWFkJAQ7N+/H2fOnEF0\ndDRef/11AIDVaoVOp3O9JiMjAz/++CO2bdsGRVFw4MABrwMk+r3igKfA8ueWkaMqKr7AiROVOHeu\nGmFhHDvwezLd/ulXFs/16abbo2/P2LnMAGA0Gscdf/vtt+OOR+c2ExF5a9A5iB5HL+aER0PR+NYt\nFogtIwGgpuYCYmLmoKnpJ7zySopPMZPcJpuWNdpN9fe/JKOz86HX789bOyKSivORE2csVfjV2oou\nRzfiwmOxVJ+MzD/+GZpQ755EArFl5O3bFjidj5CVlY2amguu4vzzzw04cuRfCA0NRVpaOrZufcPt\nOQpOk83L12h8WzCH+94RkVTOWKpQe7cOnY4uCAh0OrpQe7cOZyxVXr/nZFtGKori2jKysrISn3xy\nFN9/fwn37o2sDDe6ZeSWLVmorq7CG2/sgFarQ3HxPyeMvbl0qRppaelYs2YdGhrq4XA4IIRASUkh\nDh48hE8/PYampp/gcNjdnqPgNvokPVPjR1iciUgag85B/Gp1vzVjs60Vg85Bt9em5vuWkQ8f9k36\n7kIIXL58EWlpGxATMwfJyUvQ0FCP7u4uKIqCuLg4aDQaFBWVor+/f8K50b2liUaxWZuIpNHj6EWX\no9vttU57N3ocvdBH6txefxJ/bxnZ3HwDnZ0dKCjYBwDo6+tFTc1FLFu2fMJ2kNwikjzBJ2ciksac\n8GjEhce6vaaNiMWccO8WYFmx4mXcv9+GurofAMC1ZeTly5eQlPQiWlqax20ZmZT04rTe/9KlC3jn\nnd04fvxLHD/+JSoqTuKXX/6DWbMUPHrkhNXaDiEE9u79B0JDNRPO9fbKtUAPqY9PzkQkDUWjYKk+\nGbV36yZcWxKf7PWobX9uGTk8PIz6+h+Qk1PpOjd79mykpKxCXd2/8d57JtcT9bp1aYiOjnZ7jmgs\nj7aMDARuGTlzGJNngjUm2Zfv9H3LyJHR2s22VnTau6GNiMWSeN9Ga3siWL8PgcaYPOP3LSOJiAJJ\nE6rBlqS/4m+Jf5qxec5EwYbFmYikpGgUrwZ/Ef0ecEAYERGRZFiciYiIJMPiTEREJBkWZyIiIsmw\nOBMREUmGxZmIiEgyLM5ERESSYXEmIiKSjDTLdxIREdEIPjkTERFJhsWZiIhIMizOREREkmFxJiIi\nkgyLMxERkWRYnImIiCQTFPs5f/jhh7hx4wZCQkKQn5+PpUuXqhbLrVu3kJubi507dyI7OxttbW3Y\nu3cvnE4n9Ho9Dh48CEUJ7MbwRUVFuHbtGoaHh7Fr1y4sWbJE1ZgGBgZgMpnQ0dEBh8OB3NxcGI1G\n1T8nu92OTZs2ITc3FytXrlQ9nsbGRrz77rtYuHAhACApKQk5OTmqx+VvsuQzc3lqsuYyIFc++yWX\nheQaGxvFW2+9JYQQwmKxiK1bt6oWy8OHD0V2drYoKCgQFRUVQgghTCaT+O6774QQQpSUlIjKysqA\nxmQ2m0VOTo4QQojOzk7x2muvqR5TVVWV+Oyzz4QQQty9e1ekp6erHpMQQnz88cciMzNTnD59Wop4\nGhoaxO7du8edkyEuf5Iln5nLnpE1l4WQK5/9kcvSN2ubzWakpaUBABITE9HT04O+vj5VYlEUBUeP\nHoXBYHCda2xsxPr16wEAa9euhdlsDmhMK1aswKFDhwAAMTExGBgYUD2mjIwMvPnmmwCAtrY2JCQk\nqB7T7du3YbFYsGbNGgDq/7tNRta4Zoos+cxc9oyMuQwERz77GpP0xdlmsyEuLs51rNVqYbVaVYkl\nLCwMERER484NDAy4mip0Ol3AY9NoNIiMjAQAnDp1Cq+++qrqMY3KysrCnj17kJ+fr3pMhYWFMJlM\nrmO14xllsVjw9ttvY9u2baivr5cmLn+RJZ+Zy9MjUy4DcubzTOdyUPQ5jyUkXm1Uzdhqampw6tQp\nlJeXIz09XYqYvvrqK9y8eRPvv//+uDgCHdO5c+ewbNkyzJs3z+11tT6jBQsWIC8vDxs3bsSdO3ew\nY8cOOJ1O1eMKJFn/RubyeLLkMiBnPvsjl6UvzgaDATabzXXc3t4OvV6vYkTjRUZGwm63IyIiAvfv\n3x/XTBYoV69exeHDh/H5558jOjpa9ZhaWlqg0+kwd+5cLFq0CE6nE1FRUarFVFtbizt37qC2tha/\n/fYbFEVR/TMCgISEBGRkZAAA5s+fj/j4eDQ3N6selz/JnM8yfCeYy1OTMZ/9kcvSN2unpqbiwoUL\nAIDW1lYYDAY888wzKkf1fykpKa74Ll68iNWrVwf09/f29qKoqAhHjhxBbGysFDE1NTWhvLwcwEgz\nZn9/v6oxlZaW4vTp0zh58iS2bNmC3Nxc1T8jAPjmm29w7NgxAIDVakVHRwcyMzNVj8ufZM5ntb8T\nzGXPyJjP/sjloNiVqri4GE1NTQgJCcH+/fthNBpViaOlpQWFhYW4d+8ewsLCkJCQgOLiYphMJjgc\nDjz77LP46KOPMGvWrIDFdOLECZSVleH55593nTtw4AAKCgpUi8lut+ODDz5AW1sb7HY78vLysHjx\nYuzbt0+1mEaVlZXhueeew6pVq1SPp6+vD3v27MGDBw8wNDSEvLw8LFq0SPW4/E2GfGYue0bmXAbk\nyWd/5HJQFGciIqKnifTN2kRERE8bFmciIiLJsDgTERFJhsWZiIhIMizOREREkmFxJiIikgyLMxER\nkWRYnImIiCTzX4VUjZ9iJsIBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f3167877410>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYxjkSvZY5Tw",
        "colab_type": "code",
        "outputId": "c1f76522-0d75-4a32-d9da-faf4bda42d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "estimator_predictions = basic_estimator.predict(test_suffixes)\n",
        "estimator_leaf_nodes = basic_estimator.apply(test_suffixes)\n",
        "\n",
        "gt = test_labels\n",
        "estimator_leaf_num_samples = get_estimator_confidence_by_leaf_visits(\n",
        "    basic_estimator, test_suffixes,\n",
        "    estimator_predictions, estimator_leaf_nodes, np.array(range(10)))\n",
        "\n",
        "orig_correct = (test_predictions == gt)\n",
        "estimator_correct = (estimator_predictions == gt)\n",
        "estimator_agreement = (estimator_predictions == test_predictions)\n",
        "\n",
        "min_samples = 500\n",
        "estimator_conf = estimator_leaf_num_samples > min_samples\n",
        "\n",
        "print \"ORIGINAL MODEL under BASIC ESTIMATOR (min_samples = %d)\" % min_samples\n",
        "print \"Overall Acc\", 1.0*np.mean(orig_correct)\n",
        "print \"Confident fraction of overall\", 1.0*np.mean(estimator_conf)\n",
        "print \"Confident fraction of correct\", 1.0*np.sum(estimator_conf*orig_correct)/np.sum(orig_correct)\n",
        "print \"Confident Acc\", 1.0*np.sum(orig_correct*estimator_conf)/np.sum(estimator_conf)\n",
        "\n",
        "print \"\"\n",
        "print \"BASIC ESTIMATOR\"\n",
        "print \"Acc\", np.mean(estimator_correct)\n",
        "print \"Agreement\", np.mean(estimator_agreement)\n",
        "print \"Estimator Confident Acc\", 1.0*np.sum(estimator_correct*estimator_conf)/np.sum(estimator_conf)\n",
        "print \"Estimator Confident Agreement\", 1.0*np.sum(estimator_agreement*estimator_conf)/np.sum(estimator_conf)\n",
        "print \"Combined Acc\", 1.0*np.mean(orig_correct*(1-estimator_conf)) + 1.0*np.mean(estimator_correct*estimator_conf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ORIGINAL MODEL under BASIC ESTIMATOR (min_samples = 500)\n",
            "Overall Acc 0.7796\n",
            "Confident fraction of overall 0.301\n",
            "Confident fraction of correct 0.364545920985\n",
            "Confident Acc 0.944186046512\n",
            "\n",
            "BASIC ESTIMATOR\n",
            "Acc 0.6693\n",
            "Agreement 0.7223\n",
            "Estimator Confident Acc 0.939534883721\n",
            "Estimator Confident Agreement 0.97707641196\n",
            "Combined Acc 0.7782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opfvJI1k3XOC",
        "colab_type": "text"
      },
      "source": [
        "##Download and run adversarial examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCgnnqCt3dZ2",
        "colab_type": "code",
        "outputId": "da6f7471-ae5e-4786-c42f-6dad575dbe51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "!mkdir results\n",
        "!curl -sL https://github.com/mzweilin/EvadeML-Zoo/releases/download/v0.1/results_MNIST_100_317f6_carlini.tar.gz | tar xzv -C results\n",
        "!curl -sL https://github.com/mzweilin/EvadeML-Zoo/releases/download/v0.1/results_CIFAR-10_100_de671_densenet.tar.gz | tar xzv -C results\n",
        "!curl -sL https://github.com/mzweilin/EvadeML-Zoo/releases/download/v0.1/results_ImageNet_100_a2749_mobilenet.tar.gz | tar xzv -C results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MNIST_100_317f6_carlini/\n",
            "MNIST_100_317f6_carlini/adv_examples/\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_jsma?targeted=next.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_carlinili?targeted=next&batch_size=1&max_iterations=1000&confidence=10.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_bim?eps=0.3&eps_iter=0.06.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_adaptive_carlini_l2?targeted=next&tf_squeezers=median_filter_2_2,binary_filter_0.5&distance_measure=l1&detector_threshold=0.002915.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_carlinil0?targeted=next&batch_size=1&max_iterations=1000&confidence=10.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_adaptive_carlini_l2?targeted=false&tf_squeezers=median_filter_2_2,binary_filter_0.5&distance_measure=l1&detector_threshold=0.002915.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_fgsm?eps=0.3.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=10.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_jsma?targeted=ll.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=10.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_carlinili?targeted=ll&batch_size=1&max_iterations=1000&confidence=10.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_adaptive_carlini_l2?targeted=ll&tf_squeezers=median_filter_2_2,binary_filter_0.5&distance_measure=l1&detector_threshold=0.002915.pickle\n",
            "MNIST_100_317f6_carlini/adv_examples/MNIST_100_317f6_carlini_carlinil0?targeted=ll&batch_size=1&max_iterations=1000&confidence=10.pickle\n",
            "CIFAR-10_100_de671_densenet/\n",
            "CIFAR-10_100_de671_densenet/adv_examples/\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinili?targeted=next&confidence=5.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=5.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=5.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinili?targeted=ll&confidence=5.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_deepfool?overshoot=10.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil0?targeted=ll&confidence=5.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_jsma?targeted=ll.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_fgsm?eps=0.0156.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_bim?eps=0.008&eps_iter=0.0012.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_jsma?targeted=next.pickle\n",
            "CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil0?targeted=next&confidence=5.pickle\n",
            "ImageNet_100_a2749_mobilenet/\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_bim?eps=0.0040&eps_iter=0.0020.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_fgsm?eps=0.0078.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_carlinil2?max_iterations=1000&batch_size=10&targeted=next&confidence=5.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_carlinil2?max_iterations=1000&batch_size=50&targeted=ll&confidence=5.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_carlinil0?batch_size=1&targeted=next&confidence=5.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_carlinili?batch_size=1&targeted=ll&confidence=5.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_carlinili?batch_size=1&targeted=next&confidence=5.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_deepfool?overshoot=35.pickle\n",
            "ImageNet_100_a2749_mobilenet/adv_examples/ImageNet_100_a2749_mobilenet_carlinil0?batch_size=1&targeted=ll&confidence=5.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMmiuE6i3egJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cPickle\n",
        "\n",
        "#Load adversarial examples\n",
        "targetNext1Pickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinili?targeted=next&confidence=5.pickle\", 'rb'))\n",
        "targetNext1Data = targetNext1Pickle[0]\n",
        "targetll1Pickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinili?targeted=ll&confidence=5.pickle\", 'rb'))\n",
        "targetll1Data = targetll1Pickle[0]\n",
        "targetNext2Pickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=5.pickle\", 'rb'))\n",
        "targetNext2Data = targetNext2Pickle[0]\n",
        "targetll2Pickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=5.pickle\", 'rb'))\n",
        "targetll2Data = targetll2Pickle[0]\n",
        "targetNext0Pickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil0?targeted=next&confidence=5.pickle\", 'rb'))\n",
        "targetNext0Data = targetNext0Pickle[0]\n",
        "targetll0Pickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_carlinil0?targeted=ll&confidence=5.pickle\", 'rb'))\n",
        "targetll0Data = targetll0Pickle[0]\n",
        "targetNextJsmaPickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_jsma?targeted=next.pickle\", 'rb'))\n",
        "targetNextJsmaData = targetNextJsmaPickle[0]\n",
        "targetllJsmaPickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_jsma?targeted=ll.pickle\", 'rb'))\n",
        "targetllJsmaData = targetllJsmaPickle[0]\n",
        "fgsmPickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_fgsm?eps=0.0156.pickle\", 'rb'))\n",
        "fgsmData = fgsmPickle[0]\n",
        "bimPickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_bim?eps=0.008&eps_iter=0.0012.pickle\", 'rb'))\n",
        "bimData = bimPickle[0]\n",
        "deepfoolPickle = cPickle.load(open(\"results/CIFAR-10_100_de671_densenet/adv_examples/CIFAR-10_100_de671_densenet_deepfool?overshoot=10.pickle\", 'rb'))\n",
        "deepfoolData = deepfoolPickle[0]\n",
        "\n",
        "adversarialData = np.concatenate((targetNext1Data, targetll1Data, targetNext2Data, targetll2Data, targetNext0Data, targetll0Data, targetNextJsmaData, targetllJsmaData, fgsmData, bimData), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08Cyx8AGHxzd",
        "colab_type": "code",
        "outputId": "591c1c05-2544-4622-8452-61f9fa7f01da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Test attack success rate. Meant to work with Keras version of get_prediction\n",
        "all_attacks = adversarialData\n",
        "all_attacks_predictions = np.argmax(get_prediction_keras(all_attacks), axis=1)\n",
        "test_images, test_gt = get_test_dataset()\n",
        "all_attacks_gt = np.tile(np.argmax(test_gt, axis=1)[:100], 10)\n",
        "print \"Attack succcess rate\", 1.0*np.sum(all_attacks_gt != all_attacks_predictions)/len(all_attacks)\n",
        "successful_attacks = all_attacks[np.where(all_attacks_predictions != all_attacks_gt)[0]]\n",
        "\n",
        "successful_attacks_suffixes = fingerprint_suffix_keras(successful_attacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 48.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Attack succcess rate 0.561\n",
            "Getting fingerprint for activation_5/Relu:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiUhhwQXK74H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind = np.array(range(10000))\n",
        "np.random.shuffle(ind)\n",
        "benign_suffixes = test_suffixes[ind]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_sQcewdLEiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "benign_be_predictions = basic_estimator.predict(benign_suffixes)\n",
        "benign_be_leaf_nodes = basic_estimator.apply(benign_suffixes)\n",
        "benign_be_conf = get_estimator_confidence_by_leaf_visits(\n",
        "    basic_estimator, benign_suffixes, benign_be_predictions, benign_be_leaf_nodes,\n",
        "    range(10)\n",
        ")\n",
        "\n",
        "benign_fge_predictions = fine_grained_estimator.predict(benign_suffixes)\n",
        "benign_fge_leaf_nodes = fine_grained_estimator.apply(benign_suffixes)\n",
        "benign_fge_conf = get_estimator_confidence_by_leaf_visits(\n",
        "    fine_grained_estimator, benign_suffixes, benign_fge_predictions, benign_fge_leaf_nodes,\n",
        "    11*np.array(range(10))\n",
        ")\n",
        "\n",
        "successful_attacks_be_predictions = basic_estimator.predict(successful_attacks_suffixes)\n",
        "successful_attacks_be_leaf_nodes = basic_estimator.apply(successful_attacks_suffixes)\n",
        "successful_attacks_be_conf = get_estimator_confidence_by_leaf_visits(\n",
        "    basic_estimator, successful_attacks_suffixes, successful_attacks_be_predictions,\n",
        "    successful_attacks_be_leaf_nodes, np.array(range(10)))\n",
        "\n",
        "successful_attacks_fge_predictions = fine_grained_estimator.predict(successful_attacks_suffixes)\n",
        "successful_attacks_fge_leaf_nodes = fine_grained_estimator.apply(successful_attacks_suffixes)\n",
        "successful_attacks_fge_conf = get_estimator_confidence_by_leaf_visits(\n",
        "    fine_grained_estimator, successful_attacks_suffixes, successful_attacks_fge_predictions,\n",
        "    successful_attacks_fge_leaf_nodes, 11*np.array(range(10)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7lIkvbtNVCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "def get_detection_error_rates(\n",
        "    benign_conf, attacks_conf, min_samples):\n",
        "  benign_conf = benign_conf > min_samples\n",
        "  attacks_conf = attacks_conf > min_samples\n",
        "  fpr = np.mean(1.0 - benign_conf)\n",
        "  tpr = np.mean(1.0 - attacks_conf)\n",
        "  return fpr, tpr\n",
        "\n",
        "def get_detection_auc(\n",
        "    benign_conf, attacks_conf):\n",
        "  fprs = []\n",
        "  tprs = []\n",
        "  for min_samples in tqdm(range(1+max(benign_conf.max(), attacks_conf.max()))):\n",
        "    fpr, tpr = get_detection_error_rates(benign_conf, attacks_conf, min_samples)\n",
        "    fprs.append(fpr)\n",
        "    tprs.append(tpr)\n",
        "  return metrics.auc(fprs, tprs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3hdKsCzNhA_",
        "colab_type": "code",
        "outputId": "a4b418ae-f191-4155-8a68-945581e3f346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "min_samples = 10\n",
        "fpr, tpr = get_detection_error_rates(\n",
        "  benign_be_conf, successful_attacks_be_conf, min_samples)\n",
        "print \"Basic Estimator (min_samples = %d)\" % min_samples\n",
        "print \"FPR (<num test non-confident instances>/ <num test instances>\", fpr\n",
        "print \"TPR (<num attack non-confident instances>/ <num attack instances>\", tpr\n",
        "\n",
        "auc = get_detection_auc(\n",
        "  benign_be_conf, successful_attacks_be_conf)\n",
        "print \"AUC\", auc\n",
        "\n",
        "fpr, tpr = get_detection_error_rates(\n",
        "  benign_fge_conf, successful_attacks_fge_conf, min_samples)\n",
        "print \"Fine-grained Estimator (min_samples = %d)\" % min_samples\n",
        "print \"FPR (<num test non-confident instances>/ <num test instances>\", fpr\n",
        "print \"TPR (<num attack non-confident instances>/ <num attack instances>\", tpr\n",
        "\n",
        "auc = get_detection_auc(\n",
        "  benign_fge_conf, successful_attacks_fge_conf)\n",
        "print \"AUC\", auc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 1151/2512 [00:00<00:00, 11508.86it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Basic Estimator (min_samples = 10)\n",
            "FPR (<num test non-confident instances>/ <num test instances> 0.283\n",
            "TPR (<num attack non-confident instances>/ <num attack instances> 0.46167557932263814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2512/2512 [00:00<00:00, 11432.16it/s]\n",
            " 51%|█████     | 1201/2346 [00:00<00:00, 12008.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC 0.6237982174688058\n",
            "Fine-grained Estimator (min_samples = 10)\n",
            "FPR (<num test non-confident instances>/ <num test instances> 0.3202\n",
            "TPR (<num attack non-confident instances>/ <num attack instances> 0.44028520499108736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 2346/2346 [00:00<00:00, 10750.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AUC 0.5891091800356506\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY9Hcw9Bc5sL",
        "colab_type": "code",
        "outputId": "feca7ec2-a768-4db3-c674-c7f37b345f5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "test_images1, test_labels1 = get_test_dataset()\n",
        "all_attacks_gt = np.tile(np.argmax(test_labels1, axis=1)[:100], 10)\n",
        "all_attacks = adversarialData\n",
        "adversarial_images = adversarialData\n",
        "\n",
        "all_attacks_predictions = np.argmax(get_prediction_keras(all_attacks), axis=1)\n",
        "successful_attacks = all_attacks[np.where(all_attacks_predictions != all_attacks_gt)[0]]\n",
        "unsuccessful_attacks = all_attacks[np.where(all_attacks_predictions == all_attacks_gt)[0]]\n",
        "successful_attacks_gt = all_attacks_gt[np.where(all_attacks_predictions != all_attacks_gt)[0]]\n",
        "unsuccessful_attacks_gt = all_attacks_gt[np.where(all_attacks_predictions == all_attacks_gt)[0]]\n",
        "print len(successful_attacks)\n",
        "adversarial_suffixes = fingerprint_suffix_keras(successful_attacks)\n",
        "adversarial_suffixes = (adversarial_suffixes>0).astype('int')\n",
        "\n",
        "be_adversarial_predictions = basic_estimator.predict(adversarial_suffixes)\n",
        "be_adversarial_leaf_nodes = basic_estimator.apply(adversarial_suffixes)\n",
        "fge_adversarial_predictions = fine_grained_estimator.predict(adversarial_suffixes)\n",
        "fge_adversarial_leaf_nodes = fine_grained_estimator.apply(adversarial_suffixes)\n",
        "\n",
        "be_adversarial_conf = get_estimator_confidence_by_leaf_visits(\n",
        "    basic_estimator, adversarial_suffixes, be_adversarial_predictions, be_adversarial_leaf_nodes, np.array(range(10)))\n",
        "print \"Number of confident adversarial examples from basic estimator\", np.sum((be_adversarial_conf)>1000)\n",
        "fge_adversarial_conf = get_estimator_confidence_by_leaf_visits(\n",
        "    fine_grained_estimator, adversarial_suffixes, fge_adversarial_predictions, fge_adversarial_leaf_nodes, 10*np.array(range(10))+np.array(range(10)))\n",
        "print \"Number of confident adversarial examples from fine grained estimator\", np.sum(fge_adversarial_conf)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 52.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "561\n",
            "Getting fingerprint for activation_6/Relu:0\n",
            "Number of confident adversarial examples from basic estimator 129\n",
            "Number of confident adversarial examples from fine grained estimator 272276\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCqyixQXXrmO",
        "colab_type": "code",
        "outputId": "36a434df-9e5f-4095-b270-0715e40c827a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Create and run an inference graph for the given adversarial examples, print num conf instances\n",
        "# This is meant to work with the tensorflow deep cnn stuff\n",
        "# TABLE OUT OF DATE, IGNORE \n",
        "# Fine Grained SS=1 / Fine Grained SS = 10 / Basic SS=10 / Basic SS=5\n",
        "# targetNext1Data       47 / 5  / 24 / 69\n",
        "# targetll1Data         46 / 7  / 28 / 67\n",
        "# targetNext2Data       47 / 6  / 25 / 69\n",
        "# targetll2Data         47 / 6  / 26 / 69\n",
        "# targetNext0Data       43 / 6  / 25 / 67\n",
        "# targetll0Data         43 / 1  / 18 / 65\n",
        "# targetNextJsmaData    50 / 12 / 48 / 75\n",
        "# targetllJsmaData      39 / 9  / 49 / 65\n",
        "# fgsmData              45 / 7  / 27 / 68\n",
        "# bimData               46 / 5  / 25 / 69\n",
        "# Totals                330 / 64 / 295 / 683\n",
        "\n",
        "def preprocess(images):\n",
        "  # 'images' is a list of images of shape <32, 32, 3> and scaled down to 0 to 1.\n",
        "  #\n",
        "  # First scale up to 0 to 255\n",
        "  images = 255*images \n",
        "  centered_data = tf.image.resize_image_with_crop_or_pad(images, 24, 24)\n",
        "  standardized_data = tf.map_fn(lambda image: tf.image.per_image_standardization(image), centered_data)\n",
        "  return sess.run(standardized_data)\n",
        "\n",
        "test_images, test_labels = get_test_dataset()\n",
        "test_labels = np.argmax(test_labels, axis=1)[:100]\n",
        "repeated_labels = np.tile(test_labels, 10)\n",
        "\n",
        "#Select desired data here\n",
        "adversarial_images = preprocess(adversarialData)\n",
        "# adversarial_images is what we must feed to our network\n",
        "adversarial_predictions = np.argmax(get_prediction(sess, t_logits, t_images_pl, adversarial_images), axis=1)\n",
        "successful_images = adversarial_images[np.array([adversarial_predictions[i] != repeated_labels[i] for i in range(len(adversarial_predictions))])]\n",
        "print len(successful_images), \"successful attacks\"\n",
        "unsuccessful_images = adversarial_images[np.invert(np.array([adversarial_predictions[i] != repeated_labels[i] for i in range(len(adversarial_predictions))]))]\n",
        "print len(unsuccessful_images), \"unsuccessful attacks\"\n",
        "adversarial_suffixes = fingerprint_suffix(unsuccessful_images)\n",
        "#adversarial_suffixes = fingerprint_suffix(successful_images)\n",
        "#adversarial_suffixes = fingerprint_suffix(adversarial_images)\n",
        "adversarial_suffixes = (adversarial_suffixes>0).astype('int')\n",
        "\n",
        "fge_adversarial_predictions = fine_grained_estimator.predict(adversarial_suffixes)\n",
        "be_adversarial_predictions = basic_estimator.predict(adversarial_suffixes)\n",
        "\n",
        "#Adjust confidence threshold here\n",
        "minsamps = 10\n",
        "be_adversarial_conf = get_estimator_confidence(basic_estimator, adversarial_suffixes, be_adversarial_predictions, np.array(range(10)), min_samples=minsamps)\n",
        "print \"Number of confident adversarial examples from basic estimator\", np.sum(be_adversarial_conf)\n",
        "fge_adversarial_conf = get_estimator_confidence(fine_grained_estimator, adversarial_suffixes, fge_adversarial_predictions, 10*np.array(range(10))+np.array(range(10)), min_samples=minsamps)\n",
        "print \"Number of confident adversarial examples from fine grained estimator\", np.sum(fge_adversarial_conf)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:00<00:00, 74.05it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 63.14it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "586 successful attacks\n",
            "414 unsuccessful attacks\n",
            "Getting fingerprint for local4/local4:0\n",
            "Number of confident adversarial examples from basic estimator 332\n",
            "Number of confident adversarial examples from fine grained estimator 259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ctr7bM4UCeH",
        "colab_type": "code",
        "outputId": "28325a1f-331e-4dbf-cd67-6f9bd54d1246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "# Examine invariant clusters that contain adversarial images\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "basic_clusters = [get_invariant_generic(basic_estimator, adversarial_images, adversarial_suffixes, i)[0] for i in tqdm(range(len(adversarial_images)))]\n",
        "print \"\\nBasic Estimator:\"\n",
        "\n",
        "basic_images_per_cluster = []\n",
        "basic_considered_clusters = []\n",
        "for cluster in basic_clusters:\n",
        "  ignore = False\n",
        "  for i in basic_considered_clusters:\n",
        "    if np.array_equal(cluster, i):\n",
        "      ignore = True\n",
        "      break\n",
        "  if ignore == True:\n",
        "    continue\n",
        "  else:\n",
        "    basic_considered_clusters.append(cluster)\n",
        "  count = np.sum(np.array([np.array_equal(cluster, i) for i in basic_clusters]).astype(int))\n",
        "  basic_images_per_cluster.append(count)\n",
        "basic_cluster_sizes = [len(cluster) for cluster in basic_considered_clusters]\n",
        "print \"Number of unique clusters:\", len(basic_cluster_sizes)\n",
        "print \"Max cluster size:\", np.amax(basic_cluster_sizes)\n",
        "print \"Clusters larger than 100:\", np.sum(np.array([i > 100 for i in basic_cluster_sizes]).astype(int))\n",
        "print \"# adv images in those clusters:\"\n",
        "print np.array(basic_images_per_cluster)[np.array([i > 100 for i in basic_cluster_sizes])]\n",
        "print \"Clusters larger than 500:\", np.sum(np.array([i > 500 for i in basic_cluster_sizes]).astype(int))\n",
        "print \"# adv images in those clusters:\"\n",
        "print np.array(basic_images_per_cluster)[np.array([i > 500 for i in basic_cluster_sizes])]\n",
        "print \"max adv images per cluster:\", np.amax(basic_images_per_cluster)\n",
        "print \"Adversarial images per cluster:\"\n",
        "print basic_images_per_cluster\n",
        "\n",
        "fg_clusters = [get_invariant_generic(fine_grained_estimator, adversarial_images, adversarial_suffixes, i)[0] for i in tqdm(range(len(adversarial_images)))] \n",
        "print \"\\nFine-Grained Estimator:\"\n",
        "\n",
        "fg_images_per_cluster = []\n",
        "fg_considered_clusters = []\n",
        "for cluster in fg_clusters:\n",
        "  ignore = False\n",
        "  for i in fg_considered_clusters:\n",
        "    if np.array_equal(cluster, i):\n",
        "      ignore = True\n",
        "      break\n",
        "  if ignore == True:\n",
        "    continue\n",
        "  else:\n",
        "    fg_considered_clusters.append(cluster)\n",
        "  count = np.sum(np.array([np.array_equal(cluster, i) for i in fg_clusters]).astype(int))\n",
        "  fg_images_per_cluster.append(count)\n",
        "fg_cluster_sizes = [len(cluster) for cluster in fg_considered_clusters]\n",
        "print \"Number of unique clusters:\", len(fg_cluster_sizes)\n",
        "print \"Max cluster size:\", np.amax(fg_cluster_sizes)\n",
        "print \"Clusters larger than 100:\", np.sum(np.array([i > 100 for i in fg_cluster_sizes]).astype(int))\n",
        "print \"# adv images in those clusters:\"\n",
        "print np.array(fg_images_per_cluster)[np.array([i > 100 for i in fg_cluster_sizes])]\n",
        "print \"Clusters larger than 500:\", np.sum(np.array([i > 500 for i in fg_cluster_sizes]).astype(int))\n",
        "print \"# adv images in those clusters:\"\n",
        "print np.array(fg_images_per_cluster)[np.array([i > 500 for i in fg_cluster_sizes])]\n",
        "print \"max adv images per cluster:\", np.amax(fg_images_per_cluster)\n",
        "print \"Adversarial images per cluster:\"\n",
        "print fg_images_per_cluster"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:20<00:00, 49.23it/s]\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Basic Estimator:\n",
            "Number of unique clusters: 93\n",
            "Max cluster size: 3627\n",
            "Clusters larger than 100: 5\n",
            "# adv images in those clusters:\n",
            "[33  6  5  3  3]\n",
            "Clusters larger than 500: 3\n",
            "# adv images in those clusters:\n",
            "[6 5 3]\n",
            "max adv images per cluster: 122\n",
            "Adversarial images per cluster:\n",
            "[88, 52, 38, 116, 26, 46, 22, 57, 39, 14, 42, 5, 33, 12, 4, 122, 10, 6, 6, 18, 7, 29, 15, 11, 8, 6, 4, 8, 7, 4, 8, 6, 3, 5, 2, 4, 1, 2, 3, 10, 5, 1, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 5, 1, 1, 5, 2, 7, 1, 4, 3, 9, 4, 3, 1, 1, 3, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:23<00:00, 43.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fine-Grained Estimator:\n",
            "Number of unique clusters: 154\n",
            "Max cluster size: 2591\n",
            "Clusters larger than 100: 1\n",
            "# adv images in those clusters:\n",
            "[1]\n",
            "Clusters larger than 500: 1\n",
            "# adv images in those clusters:\n",
            "[1]\n",
            "max adv images per cluster: 81\n",
            "Adversarial images per cluster:\n",
            "[14, 11, 44, 33, 6, 81, 57, 73, 6, 15, 44, 5, 14, 37, 10, 6, 33, 21, 42, 35, 10, 6, 6, 6, 8, 53, 7, 6, 7, 7, 7, 8, 21, 6, 5, 5, 7, 6, 6, 8, 5, 7, 5, 3, 2, 1, 6, 5, 4, 3, 2, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 4, 3, 1, 3, 1, 1, 1, 1, 1, 8, 3, 2, 4, 1, 3, 1, 1, 1, 1, 4, 1, 5, 1, 2, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 3, 9, 2, 3, 1, 4, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 4, 1, 3, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhKV-FYLv-Eb",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing the Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEZ8ZeuMo_9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install graphviz\n",
        "!pip install graphviz\n",
        "import graphviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaOsPspkjuOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dot_data = tree.export_graphviz(basic_estimator, out_file=None) \n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}